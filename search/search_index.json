{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#paul-withers-blog","title":"Paul Withers Blog","text":""},{"location":"#about-me","title":"About Me","text":"<p>I have been a developer since 2000, primarily with the IBM/HCL Domino stack. When IBM introduced XPages, a JSF-based framework, to the stack in 2009 I was one of the early adopters, blogging, training, speaking and eventually co-authoring \"XPages Extension Library\" by IBM Press in 2012 and acting as technical editor for \"Mastering XPages 2nd Edition\" by IBM Press in 2014. As a technical evangelist I was recognised by IBM in their IBM Champion program, which started in 20111 for the IBM Collaboration Solutions brand (which included IBM Domino). I was an ever-present champion since it's inception and in 2018 was made a Lifetime IBM Champion. When HCL bought Domino and other products from IBM in 2019 I became an HCL Master and HCL Grandmaster (now re-titled HCL Lifetime Ambassador).</p> <p>I've been a committed contributor to open source since 2011 and I firmly believe in learning through sharing and empowering others. I always try to understand the \"why\" behind the \"what\", and help verbalise that for others too. By sharing code I've learned a great deal from other contributors and through feature requests. From October 2013 to October 2024 I was a board member of the open source organisation OpenNTF.</p> <p>After working as a Domino user and developer, for both a Domino customer and business partner, in November 2019 I joined the product owner, HCL. I work as a technical architect in the HCL Labs team, covering R&amp;D, innovation and open source.</p> <p>I have strong experience as a technical evangelist. I have always worked closely with end users to build requirements and help them think from all angles. In terms of language, I have a firm preference for Java over JavaScript. I like the strongly-typed nature of Java, which catches more \"simple\" errors and lets me focus on thinking about more complex issues. More recent development experience covered HCL Connections, Node-RED, Vaadin, a little Vert.x and a little Node.js development.</p> <p>Wherever life takes me, this blog is about sharing, learning and growing.</p>"},{"location":"#social","title":"Social","text":"<p>I can be found on Threads as @paul.s.withers.</p>"},{"location":"about/","title":"About Me","text":"<p>I have been a developer since 2000, primarily with the IBM/HCL Domino stack. When IBM introduced XPages, a JSF-based framework, to the stack in 2009 I was one of the early adopters, blogging, training, speaking and eventually co-authoring \"XPages Extension Library\" by IBM Press in 2012 and acting as technical editor for \"Mastering XPages 2nd Edition\" by IBM Press in 2014. As a technical evangelist I was recognised by IBM in their IBM Champion program, which started in 2011 for the IBM Collaboration Solutions brand (which included IBM Domino). I was an ever-present champion since it's inception and in 2018 was made a Lifetime IBM Champion. When HCL bought Domino and other products from IBM in 2019 I became an HCL Master and HCL Grandmaster (now re-titled HCL Lifetime Ambassador).</p> <p>I've been a committed contributor to open source since 2011 and I firmly believe in learning through sharing and empowering others. I always try to understand the \"why\" behind the \"what\", and help verbalise that for others too. By sharing code I've learned a great deal from other contributors and through feature requests. From October 2013 to October 2024 I was a board member of the open source organisation OpenNTF.</p> <p>After working as a Domino user and developer, for both a Domino customer and business partner, in November 2019 I joined the product owner, HCL. I work as a technical architect in the HCL Labs team, covering R&amp;D, innovation and open source.</p> <p>I have strong experience as a technical evangelist. I have always worked closely with end users to build requirements and help them think from all angles. In terms of language, I have a firm preference for Java over JavaScript. I like the strongly-typed nature of Java, which catches more \"simple\" errors and lets me focus on thinking about more complex issues. More recent development experience covered HCL Connections, Node-RED, Vaadin, a little Vert.x and a little Node.js development.</p> <p>Wherever life takes me, this blog is about sharing, learning and growing.</p>"},{"location":"about/#linkedin","title":"LinkedIn","text":"<p>My LinkedIn Profile has this public page.</p>"},{"location":"about/#social","title":"Social","text":"<p>I can be found on Threads as @paul.s.withers.</p>"},{"location":"about/#slideshare","title":"Slideshare","text":"<p>Some of my presentations can be found on Slideshare.</p>"},{"location":"about/#github","title":"GitHub","text":"<p>Some of my open source projects can be found on GitHub. More recent ones for HCL can be found on the HCL Software GitHub.</p>"},{"location":"rss/","title":"RSS Feeds","text":"<p>RSS feeds are generated using the MKDocs RSS Plugin. The following feeds are available:</p>"},{"location":"rss/#xml","title":"XML","text":"<ul> <li>Created posts</li> <li>Updated posts</li> </ul>"},{"location":"rss/#json","title":"JSON","text":"<ul> <li>Created posts</li> <li>Updated posts</li> </ul>"},{"location":"blog/series/","title":"Blog Series","text":""},{"location":"blog/series/#xpagesframework-to-web-app","title":"XPages/Framework to Web App","text":"<ul> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks and the Internet</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> </ul>"},{"location":"blog/series/#documentation-on-domino","title":"Documentation on Domino","text":"<ul> <li>GitHub Pages on Domino - Why</li> <li>GitHub Pages on Domino - What</li> <li>GitHub Pages on Domino - How</li> <li>MKDocs on Domino</li> </ul>"},{"location":"blog/series/#lotusscript-voltscript-classes-deep-dive","title":"LotusScript (VoltScript) Classes Deep Dive","text":"<ul> <li>Part One</li> <li>Part Two</li> <li>Part Three</li> <li>Part Four</li> <li>Part Five - Execute</li> <li>Part Six - Singleton Addendum</li> <li>Delete</li> </ul>"},{"location":"blog/series/#adventures-in-cacheland","title":"Adventures in CacheLand","text":"<ul> <li>Part One</li> <li>Part Two</li> </ul>"},{"location":"blog/series/#statistics-publishing-and-reporting","title":"Statistics Publishing and Reporting","text":"<ul> <li>Domino and Statistics</li> <li>Statistics for Prometheus</li> <li>Using Micrometer</li> <li>Auto-Configuration and Composite Registries</li> </ul>"},{"location":"blog/tags/","title":"Tags","text":""},{"location":"blog/tags/#tag:ai","title":"AI","text":"<ul> <li>            AI Coding - Thoughts About The Future of Development          </li> <li>            AI and Marketing Content          </li> <li>            AI, Tailwind, and The Future of Media          </li> <li>            Adventures in AI          </li> <li>            Effective AI Usage Part One - What is AI?          </li> <li>            Effective AI Usage: AI-fu          </li> <li>            Effective AI Usage: Managing Hallucination          </li> <li>            Effective AI Usage: Understanding Brains          </li> <li>            More AI Lessons          </li> <li>            Using AI          </li> </ul>"},{"location":"blog/tags/#tag:atom","title":"Atom","text":"<ul> <li>            Danger of Mid Code to Pro Code          </li> </ul>"},{"location":"blog/tags/#tag:css","title":"CSS","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Engage 2024          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eighteen: CSP Enhancement          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fifteen - Dialogs          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nine - Services          </li> <li>            XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Seventeen - Lessons Learned          </li> <li>            XPages App to Web App: Part Sixteen: Spots          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:coding","title":"Coding","text":"<ul> <li>            Adventures in Rust          </li> <li>            Danger of Mid Code to Pro Code          </li> <li>            Developing at Speed          </li> <li>            Error Management          </li> <li>            Shu-Ha-Ri          </li> </ul>"},{"location":"blog/tags/#tag:community","title":"Community","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Developing for Research          </li> <li>            Negotiating Enhancements          </li> <li>            Pastures New, New Challenges          </li> </ul>"},{"location":"blog/tags/#tag:conferences","title":"Conferences","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Engage 2024          </li> <li>            IBM Connect 2017: Embrace Fear          </li> <li>            Speaking at Engage 2020          </li> </ul>"},{"location":"blog/tags/#tag:containers","title":"Containers","text":"<ul> <li>            Docker, Java and Processes          </li> <li>            Rancher Desktop, A New Dev Tool          </li> </ul>"},{"location":"blog/tags/#tag:databases","title":"Databases","text":"<ul> <li>            Graph Database News          </li> <li>            Taking Titan to the Next Level          </li> </ul>"},{"location":"blog/tags/#tag:dev-tools","title":"Dev Tools","text":"<ul> <li>            My Development Tools - Part One: Domino and XPages          </li> <li>            My Development Tools - Part Two: Beyond Domino          </li> <li>            Visual Studio Code: Code Spell Checker          </li> </ul>"},{"location":"blog/tags/#tag:docker","title":"Docker","text":"<ul> <li>            Docker, Java and Processes          </li> <li>            Domino on Docker - Some Learning Points          </li> <li>            My Development Tools - Part Two: Beyond Domino          </li> <li>            Rancher Desktop, A New Dev Tool          </li> </ul>"},{"location":"blog/tags/#tag:documentation","title":"Documentation","text":"<ul> <li>            GitHub Pages Sites on Domino 1: Why          </li> <li>            GitHub Pages Sites on Domino 2: What          </li> <li>            GitHub Pages Sites on Domino 3: How          </li> <li>            MKDocs Sites on Domino          </li> </ul>"},{"location":"blog/tags/#tag:domino","title":"Domino","text":"<ul> <li>            Adventures in CacheLand 1          </li> <li>            Adventures in CacheLand 2          </li> <li>            Andre's Directories Challenge          </li> <li>            Application Development Musings          </li> <li>            Bali Unit Testing Framework Videos          </li> <li>            DQL Explorer and Domino          </li> <li>            DQL: What Is It Good For?          </li> <li>            Developing RunJava Addins for Domino          </li> <li>            Domino Timezones          </li> <li>            Domino and JavaScript Development MasterClass Redux          </li> <li>            Domino on Docker - Some Learning Points          </li> <li>            Eclipse Java Debugging          </li> <li>            Engage 2024          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            GitHub Pages Sites on Domino 1: Why          </li> <li>            GitHub Pages Sites on Domino 2: What          </li> <li>            GitHub Pages Sites on Domino 3: How          </li> <li>            Goodbye Nathan          </li> <li>            Introducing Bali Unit Testing Framework          </li> <li>            Java Outside Domino in Eclipse          </li> <li>            LotusScript Classes - Singleton Addendum          </li> <li>            LotusScript Classes Deep Dive Part Five: Execute          </li> <li>            LotusScript Classes Deep Dive Part Four          </li> <li>            LotusScript Classes Deep Dive Part One          </li> <li>            LotusScript Classes Deep Dive Part Three          </li> <li>            LotusScript Classes Deep Dive Part Two          </li> <li>            LotusScript Classes and Delete          </li> <li>            LotusScript Declarations          </li> <li>            LotusScript Profiling          </li> <li>            LotusScript Variants: EMPTY, NULL, Nothing          </li> <li>            MKDocs Sites on Domino          </li> <li>            Pastures New, New Challenges          </li> <li>            Postman: The Crucial Tool for Any Microservice Developer          </li> <li>            Project Jig3dw: Tutorials Re-Imagined          </li> <li>            REST API Gateways          </li> <li>            Statistics Publishing and Reporting Part One          </li> <li>            The Importance of Reproducers          </li> <li>            Thoughts on Domino          </li> <li>            Understanding Parentheses in LotusScript Method Calls          </li> <li>            Understanding Tags and Renderers          </li> <li>            Volt MX LotusScript Toolkit          </li> <li>            XPages App to Web App: Part 20: Custom CSP Settings          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eighteen: CSP Enhancement          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fifteen - Dialogs          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Seventeen - Lessons Learned          </li> <li>            XPages App to Web App: Part Sixteen: Spots          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages Elements Beyond the NSF          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:domino-rest-api","title":"Domino REST API","text":"<ul> <li>            DQL: What Is It Good For?          </li> <li>            Domino REST API Proxy Problems          </li> <li>            Domino REST API, CORS and Regex          </li> <li>            Domino and JavaScript Development MasterClass Redux          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            Postman: The Crucial Tool for Any Microservice Developer          </li> <li>            Speaking at Engage 2020          </li> <li>            Supercharging Input to Domino REST API Agents          </li> <li>            XPages App to Web App: Part 20: Custom CSP Settings          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:eclipse","title":"Eclipse","text":"<ul> <li>            Eclipse Java Debugging          </li> </ul>"},{"location":"blog/tags/#tag:editorial","title":"Editorial","text":"<ul> <li>            AI Coding - Thoughts About The Future of Development          </li> <li>            AI and Marketing Content          </li> <li>            AI, Tailwind, and The Future of Media          </li> <li>            Adventures in AI          </li> <li>            Congratulations to 2020 HCL Masters          </li> <li>            Developing at Speed          </li> <li>            Developing for Performance          </li> <li>            Developing for Research          </li> <li>            Effective AI Usage Part One - What is AI?          </li> <li>            Effective AI Usage: AI-fu          </li> <li>            Effective AI Usage: Managing Hallucination          </li> <li>            Effective AI Usage: Understanding Brains          </li> <li>            Engage 2024          </li> <li>            Error Management          </li> <li>            Introducing My New Blog          </li> <li>            Making Progress (Bars)          </li> <li>            More AI Lessons          </li> <li>            My Development Tools - Part One: Domino and XPages          </li> <li>            My Development Tools - Part Two: Beyond Domino          </li> <li>            Negotiating Enhancements          </li> <li>            Pastures New, New Challenges          </li> <li>            Shu-Ha-Ri          </li> <li>            Thoughts on Domino          </li> <li>            Thoughts on Troubleshooting Support          </li> <li>            Travels In Manila          </li> </ul>"},{"location":"blog/tags/#tag:errors","title":"Errors","text":"<ul> <li>            Domino REST API Proxy Problems          </li> <li>            Error Management          </li> </ul>"},{"location":"blog/tags/#tag:git","title":"Git","text":"<ul> <li>            My Development Tools - Part One: Domino and XPages          </li> </ul>"},{"location":"blog/tags/#tag:github-copilot","title":"GitHub Copilot","text":"<ul> <li>            AI Coding - Thoughts About The Future of Development          </li> <li>            Adventures in AI          </li> <li>            Effective AI Usage Part One - What is AI?          </li> <li>            Effective AI Usage: AI-fu          </li> <li>            Effective AI Usage: Managing Hallucination          </li> <li>            Effective AI Usage: Understanding Brains          </li> <li>            More AI Lessons          </li> <li>            Using AI          </li> </ul>"},{"location":"blog/tags/#tag:graph-databases","title":"Graph Databases","text":"<ul> <li>            Graph Database News          </li> <li>            Taking Titan to the Next Level          </li> </ul>"},{"location":"blog/tags/#tag:graphql","title":"GraphQL","text":"<ul> <li>            Creating a Java API to Access Watson Work Services          </li> <li>            IBM Connect 2017: Embrace Fear          </li> </ul>"},{"location":"blog/tags/#tag:html","title":"HTML","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eighteen: CSP Enhancement          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fifteen - Dialogs          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nine - Services          </li> <li>            XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Seventeen - Lessons Learned          </li> <li>            XPages App to Web App: Part Sixteen: Spots          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:ibm-connect","title":"IBM Connect","text":"<ul> <li>            IBM Connect 2017: Embrace Fear          </li> </ul>"},{"location":"blog/tags/#tag:java","title":"Java","text":"<ul> <li>            Creating a Java API to Access Watson Work Services          </li> <li>            DQL: What Is It Good For?          </li> <li>            Developing RunJava Addins for Domino          </li> <li>            Docker, Java and Processes          </li> <li>            IBM Connect 2017: Embrace Fear          </li> <li>            Java Outside Domino in Eclipse          </li> <li>            Lessons Learned from JUnit and Refactoring          </li> <li>            Mixing web.xml and Annotations          </li> <li>            Pastures New, New Challenges          </li> <li>            The Importance of Reproducers          </li> <li>            Unit Tests and Mocks          </li> <li>            Vert.x and JUnit Testing          </li> </ul>"},{"location":"blog/tags/#tag:javascript","title":"JavaScript","text":"<ul> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eighteen: CSP Enhancement          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fifteen - Dialogs          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nine - Services          </li> <li>            XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Seventeen - Lessons Learned          </li> <li>            XPages App to Web App: Part Sixteen: Spots          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:leap","title":"Leap","text":"<ul> <li>            Application Development Musings          </li> </ul>"},{"location":"blog/tags/#tag:lotusscript","title":"LotusScript","text":"<ul> <li>            Adventures in CacheLand 1          </li> <li>            Adventures in CacheLand 2          </li> <li>            Andre's Directories Challenge          </li> <li>            Bali Unit Testing Framework Videos          </li> <li>            DQL Explorer and Domino          </li> <li>            DQL: What Is It Good For?          </li> <li>            Domino Timezones          </li> <li>            ForAll Loops and Type Mismatches          </li> <li>            Introducing Bali Unit Testing Framework          </li> <li>            LotusScript Classes - Singleton Addendum          </li> <li>            LotusScript Classes Deep Dive Part Five: Execute          </li> <li>            LotusScript Classes Deep Dive Part Four          </li> <li>            LotusScript Classes Deep Dive Part One          </li> <li>            LotusScript Classes Deep Dive Part Three          </li> <li>            LotusScript Classes Deep Dive Part Two          </li> <li>            LotusScript Classes and Delete          </li> <li>            LotusScript Declarations          </li> <li>            LotusScript Profiling          </li> <li>            LotusScript Variants: EMPTY, NULL, Nothing          </li> <li>            Postman: The Crucial Tool for Any Microservice Developer          </li> <li>            REST API Gateways          </li> <li>            Reaping the Benefits of Standard          </li> <li>            Supercharging Input to Domino REST API Agents          </li> <li>            The Importance of Reproducers          </li> <li>            Understanding Parentheses in LotusScript Method Calls          </li> <li>            Volt MX LotusScript Toolkit          </li> </ul>"},{"location":"blog/tags/#tag:markdown","title":"Markdown","text":"<ul> <li>            GitHub Pages Sites on Domino 1: Why          </li> <li>            GitHub Pages Sites on Domino 2: What          </li> <li>            GitHub Pages Sites on Domino 3: How          </li> <li>            MKDocs Sites on Domino          </li> </ul>"},{"location":"blog/tags/#tag:micrometer","title":"Micrometer","text":"<ul> <li>            Statistics Publishing and Reporting Part Four: Auto-Configuration and Composite Registries          </li> <li>            Statistics Publishing and Reporting Part One          </li> <li>            Statistics Publishing and Reporting Part Three: Using Micrometer          </li> <li>            Statistics Publishing and Reporting Part Two: Statistics for Prometheus          </li> </ul>"},{"location":"blog/tags/#tag:nomad","title":"Nomad","text":"<ul> <li>            Project Jig3dw: Tutorials Re-Imagined          </li> </ul>"},{"location":"blog/tags/#tag:openntf","title":"OpenNTF","text":"<ul> <li>            Goodbye Nathan          </li> </ul>"},{"location":"blog/tags/#tag:performance","title":"Performance","text":"<ul> <li>            Developing for Performance          </li> <li>            LotusScript Profiling          </li> </ul>"},{"location":"blog/tags/#tag:prometheus","title":"Prometheus","text":"<ul> <li>            Statistics Publishing and Reporting Part Four: Auto-Configuration and Composite Registries          </li> <li>            Statistics Publishing and Reporting Part One          </li> <li>            Statistics Publishing and Reporting Part Three: Using Micrometer          </li> <li>            Statistics Publishing and Reporting Part Two: Statistics for Prometheus          </li> </ul>"},{"location":"blog/tags/#tag:rest-api","title":"REST API","text":"<ul> <li>            Adventures in Rust          </li> </ul>"},{"location":"blog/tags/#tag:rest-clients","title":"REST Clients","text":"<ul> <li>            Getting the Most out of Postman          </li> <li>            My Development Tools - Part Two: Beyond Domino          </li> <li>            Postman: The Crucial Tool for Any Microservice Developer          </li> </ul>"},{"location":"blog/tags/#tag:react","title":"React","text":"<ul> <li>            DQL Explorer and Domino          </li> <li>            Engage 2024          </li> </ul>"},{"location":"blog/tags/#tag:rust","title":"Rust","text":"<ul> <li>            Adventures in Rust          </li> </ul>"},{"location":"blog/tags/#tag:support","title":"Support","text":"<ul> <li>            Developing for Performance          </li> <li>            Domino REST API Proxy Problems          </li> <li>            Error Management          </li> <li>            Negotiating Enhancements          </li> <li>            The Importance of Reproducers          </li> <li>            Thoughts on Troubleshooting Support          </li> </ul>"},{"location":"blog/tags/#tag:testing","title":"Testing","text":"<ul> <li>            Getting the Most out of Postman          </li> <li>            The Importance of Reproducers          </li> </ul>"},{"location":"blog/tags/#tag:tutorials","title":"Tutorials","text":"<ul> <li>            Project Jig3dw: Tutorials Re-Imagined          </li> </ul>"},{"location":"blog/tags/#tag:vaadin","title":"Vaadin","text":"<ul> <li>            Application Development Musings          </li> <li>            Mixing web.xml and Annotations          </li> <li>            Understanding Tags and Renderers          </li> <li>            Vaadin          </li> </ul>"},{"location":"blog/tags/#tag:vertx","title":"Vert.x","text":"<ul> <li>            Java Outside Domino in Eclipse          </li> <li>            Statistics Publishing and Reporting Part Four: Auto-Configuration and Composite Registries          </li> <li>            Statistics Publishing and Reporting Part One          </li> <li>            Statistics Publishing and Reporting Part Three: Using Micrometer          </li> <li>            Statistics Publishing and Reporting Part Two: Statistics for Prometheus          </li> <li>            The Importance of Reproducers          </li> <li>            Unit Tests and Mocks          </li> <li>            Vert.x and JUnit Testing          </li> </ul>"},{"location":"blog/tags/#tag:volt-mx","title":"Volt MX","text":"<ul> <li>            Application Development Musings          </li> <li>            Domino Timezones          </li> <li>            Volt MX LotusScript Toolkit          </li> </ul>"},{"location":"blog/tags/#tag:volt-mx-go","title":"Volt MX Go","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Domino and JavaScript Development MasterClass Redux          </li> <li>            Engage 2024          </li> <li>            LotusScript Classes - Singleton Addendum          </li> <li>            LotusScript Classes Deep Dive Part Five: Execute          </li> <li>            LotusScript Classes Deep Dive Part Four          </li> <li>            LotusScript Classes Deep Dive Part One          </li> <li>            LotusScript Classes Deep Dive Part Three          </li> <li>            LotusScript Classes Deep Dive Part Two          </li> <li>            Reaping the Benefits of Standard          </li> <li>            VoltScript - A Unique Opportunity (Paul Withers and Jason Roy Gary)          </li> </ul>"},{"location":"blog/tags/#tag:voltscript","title":"VoltScript","text":"<ul> <li>            Adventures in AI          </li> <li>            Andre's Directories Challenge          </li> <li>            Bali Unit Testing Framework Videos          </li> <li>            DQL: What Is It Good For?          </li> <li>            Developing for Research          </li> <li>            ForAll Loops and Type Mismatches          </li> <li>            Introducing Bali Unit Testing Framework          </li> <li>            LotusScript Classes - Singleton Addendum          </li> <li>            LotusScript Classes Deep Dive Part Five: Execute          </li> <li>            LotusScript Classes Deep Dive Part Four          </li> <li>            LotusScript Classes Deep Dive Part One          </li> <li>            LotusScript Classes Deep Dive Part Three          </li> <li>            LotusScript Classes Deep Dive Part Two          </li> <li>            LotusScript Classes and Delete          </li> <li>            LotusScript Variants: EMPTY, NULL, Nothing          </li> <li>            Postman: The Crucial Tool for Any Microservice Developer          </li> <li>            Reaping the Benefits of Standard          </li> <li>            Understanding Parentheses in LotusScript Method Calls          </li> <li>            VoltScript - A Unique Opportunity (Paul Withers and Jason Roy Gary)          </li> </ul>"},{"location":"blog/tags/#tag:voltscript","title":"Voltscript","text":"<ul> <li>            The Importance of Reproducers          </li> </ul>"},{"location":"blog/tags/#tag:watson-work-services","title":"Watson Work Services","text":"<ul> <li>            Creating a Java API to Access Watson Work Services          </li> </ul>"},{"location":"blog/tags/#tag:web","title":"Web","text":"<ul> <li>            2025 05 18 engage 2025          </li> <li>            Adventures in Rust          </li> <li>            Domino REST API, CORS and Regex          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            Lessons Learned from Including Web Components in an Ember.js Application          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eighteen: CSP Enhancement          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fifteen - Dialogs          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nine - Services          </li> <li>            XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Seventeen - Lessons Learned          </li> <li>            XPages App to Web App: Part Sixteen: Spots          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:web-components","title":"Web Components","text":"<ul> <li>            Engage 2024          </li> <li>            Framework App to Web App: Part Five - Home Page          </li> <li>            Framework App to Web App: Part Four - DRAPI          </li> <li>            Framework App to Web App: Part Six - Mocking, DRAPI and CORS          </li> <li>            Framework App to Web App: Part Three - Frameworks and the Internet          </li> <li>            Lessons Learned from Including Web Components in an Ember.js Application          </li> <li>            Understanding Tags and Renderers          </li> <li>            XPages App to Web App: Part Eight - Landing Page Web Component          </li> <li>            XPages App to Web App: Part Eleven - Ship Search and Save          </li> <li>            XPages App to Web App: Part Fourteen - Fields and Save          </li> <li>            XPages App to Web App: Part Nine - Services          </li> <li>            XPages App to Web App: Part Seven - CSS          </li> <li>            XPages App to Web App: Part Ten - Ship Form Actions          </li> <li>            XPages App to Web App: Part Thirteen - HTML Layouts          </li> <li>            XPages App to Web App: Part Twelve - Ship Spot Component          </li> <li>            XPages to Web App Revisited: Part One - Introduction          </li> <li>            XPages to Web App Revisited: Part Two - Dev Tools          </li> </ul>"},{"location":"blog/tags/#tag:websphere-liberty","title":"Websphere Liberty","text":"<ul> <li>            IBM Websphere Liberty          </li> </ul>"},{"location":"blog/tags/#tag:xpages","title":"XPages","text":"<ul> <li>            Avoiding Inline Styles in XPages          </li> <li>            DQL: What Is It Good For?          </li> <li>            Domino REST API, CORS and Regex          </li> <li>            Pastures New, New Challenges          </li> <li>            Understanding Tags and Renderers          </li> <li>            XPages Elements Beyond the NSF          </li> </ul>"},{"location":"blog/2016/07/08/welcome/","title":"Welcome to My Blog","text":""},{"location":"blog/2016/07/08/welcome/#about-me","title":"About Me","text":"<p>I have been a developer since 2000, primarily with the IBM Domino stack. When IBM introduced XPages, a JSF-based framework, to the stack in 2009 I was one of the early adopters, blogging, training, speaking and eventually co-authoring \"XPages Extension Library\" by IBM Press in 2012 and acting as technical editor for \"Mastering XPages 2nd Edition\" by IBM Press in 2014. As a technical evangelist I have been recognised by IBM in their IBM Champion program as an ever-present champion since it's inception for the IBM Collaboration Solutions brand (which includes IBM Domino) in 2011. I've been a committed contributor to open source during that time and I firmly believe in learning through sharing and empowering others. I always try to understand the \"why\" behind the \"what\", and help verbalise that for others too. By sharing code I've learned a great deal from other contributors and through feature requests. Since October 2013 I've been a board member of the open source organisation OpenNTF.</p>"},{"location":"blog/2016/07/08/welcome/#the-future","title":"The Future","text":"<p>I don't know exactly what the future holds for me. I believe I have strong experience as a technical evangelist. I have always worked closely with end users to build requirements and help them think from all angles, which is another aspect I enjoy. In terms of language, I have a firm preference for Java over JavaScript. I like the strongly-typed nature of Java, that catches more \"simple\" errors and lets me focus on thinking about more complex issues. For over a year I've been working with Vaadin. I enjoy the vibrancy of its community, the quality of the documentation, the passion of its evangelists, and the willingness of the company to recommend best practice. In terms of backend, I've helped with development of a Tinkerpop-based graph API and the intersection point of graph and NoSQL appeals to me a lot. A preference for which graph implementation has been harder to define.</p> <p>Either way, this blog is about sharing, learning and growing.</p>"},{"location":"blog/2016/07/12/background-and-purpose/","title":"Background and Purpose","text":"<p>For the last six years, I have blogged heavily (375 posts in 3 years and nine months, more than one post per week) on Intec's blog. So the question naturally arises why I should choose to start a personal blog, and why now.</p> <p>All of that blogging and other community activity has been about the IBM Domino platform (more on that shortly) and, predominantly, IBM's JSF-based XPages development framework. Much of the focus of the last two years has been on delivering that to IBM's Bluemix Platform-as-a-Service, with the on premises version taking a back seat. Apart from some minor server enhancements and upgrades of CKEditor and Dojo (mostly required by browser upgrades), there have been no real officially-supported enhancements. That's not to say there have been no improvements from IBM. A regular stream of updates to the XPages Extension Library - a set of open source components comprising a significant percentage of the XPages components - have been released on OpenNTF. Moreover, community enhancements and fixes have been incorporated into the Extension Library, including a number from me. The open source community have also contributed to expand the power of the platform, with open source projects like POI4XPages, OpenNTF Domino API and an API providing JNA access to IBM's C API methods. There were also other equally significant projects, which it's not my position to discuss.</p> <p>But previous announcements to open source all components, provide encryption and update the version of Java have gone quiet. That's IBM's prerogative. But three years since the last significant set of core on premises enhancements, many people are growing impatient for a reason to commit their time and resources further. A further announcement of \"something\" is expected in the coming weeks, but what's really needed is quality and timely releases, not words.</p> <p>But as it's IBM's prerogative to choose their focus, it's the prerogative of developers to choose theirs. Over the last 18 months I've expanded my knowledge beyond XPages, Domino, Java, and generic web development skills. I've built some experience of Vaadin, Websphere Liberty, graph databases through the Apache Tinkerpop API. Thus far, that's all been in the context of IBM Domino. Over the past weeks I've been extending that further. And because it's not really linked to Intec's core business, it's right for those blog posts to appear on a personal site.</p> <p>So that's the reason for this blog. And the first step will be a sort of \"frame of reference\".</p>"},{"location":"blog/2016/07/13/thoughts-on-domino/","title":"Thoughts on Domino","text":"<p>The key to any relationship is periodically stepping back and appreciating the good points in contrast to the little annoyances that grate, so that you're not distracted by the first pretty young face (or muscular torso, depending on your predilection) that you encounter. When you thinking it might be time to leave the relationship, that is the most crucial (though most difficult) time to evaluate honestly and dispassionately what you have / had. Because if you don't, sooner or later you'll find different annoyances that grate; or you'll find something you took for granted and absolutely needed is missing from your new love; and before you know it that love too will turn sour and you'll be crying into your alcohol bemoaning wasted years and shattered dreams while looking at a bank account that's been wiped out by periodic divorce settlements.</p> <p>So I'm taking a few moments to evaluate what Domino provides, what I use to a greater or lesser extent, particularly what I take for granted, to better identify what's important. Maybe what I provide here will also be of benefit to others. But at the very least, it will prove a useful touchstone and checklist (drawn from a review of documentation and server settings, as well as thoughts about various production applications I've been involved in), for now and for the future.</p> <p>One point worth stressing, for others reading this, is that my Domino administration skills are limited to what, as a developer, I've needed to get to learn specifically for my applications. The vast majority has been off-loaded to specialists. So there are definite aspects in what follows which are not as complete as they could be.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#what-domino-isnt","title":"What Domino ISN'T","text":"<p>First off, let's put the rhetoric aside. Domino isn't:</p> <ul> <li>A mail platform. Sorry technology giants, it's not.</li> <li>An application development platform. Sorry developers, we may love it, but it's not.</li> <li>A NoSQL database.</li> </ul> <p>If it were that simple, there would be no debate, the roadmap would always have a five-year and ten-year plan, the platform would be modern, and iNotes, Verse and all applications would be on XPages and in the process of being made available as XPages on Bluemix for cloud customers. Or conversely, Verse would not need Domino as a backend, related databases like Resource Reservations would be available on Bluemix with e.g. Cloudant backend and A.N.Other framework, migration tools would convert applications as easily as they can email. But none are true.</p> <p>So let's deconstruct why that's the case.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#no-sql-databaseand-more","title":"No SQL Database...And More","text":"<p>NoSQL gave Domino a flexibility over RDBMS in its early days. Then we were told that enterprise applications needed to use RDBMS. But then developers rebelled against DBAs and started using a new breed of NoSQL. Yes, there are issues with things like Domino view indices, but that was in the days when most Domino applications held all the data and all the design in the same NSF. XPages has made more developers challenge that design paradigm, to shard data across more databases and also across more documents. Meanwhile, NoSQL alternatives often focus on ease over security, choosing not to reproduce the kind of security Domino's NoSQL database has had for so long at document and database and server level, and yet data security nightmares regularly hit the headlines. There have been some proprietary extensions to NoSQL databases to simulate this, but personally that seems like paying a hefty divorce settlement to get a new girlfriend who looks like the old one. It's not a sentiment shared by all, but this is about my personal journey.</p> <p>But the people who said big data meant RDBMS have since moved to graph. OpenNTF Domino API has had graph in development for over a year, I've used it on a number of occasions including semi-production applications. And the ability to combine NoSQL data and graph data, using Java interfaces for easy schema management and out-of-the-box componentization of data objects, gives a huge degree of flexibility. And all without requiring any view indices.</p> <p>The NoSQL database also includes data objects that manage both architecture as well as data - the design elements. The fact that they are, effectively, Notes documents is worth considering.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#das","title":"DAS","text":"<p>If you're developing using a JavaScript framework and all you want is very basic CRUD, DAS is available to deliver access to documents and views via simple configuration. DAS is the equivalent to the basic REST service calls for many NoSQL databases like Cloudant. Needless to say from what I covered above and for anyone who's read my blogging for Intec, DAS is not for me.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#domino-api-java-lotusscript-c","title":"Domino API (Java / LotusScript / C)","text":"<p>In terms of \"design\", few databases just use server-side scripting to manage CRUD events. Server-side scripting is used for business logic, workflow routing and database management. This means not just basic REST services like those exposed for mail and calendaring, but a complete API to do more than update values passed from the UI to the database layer. The API can be used to send emails (regardless of whether the user has a Domino mail database), create databases, modify view designs, manipulate folders, update indices, query servers, manage access rights, manage users, manage administration processes on the server and more.</p> <p>That API has been extended in OpenNTF Domino API to allow more flexible interaction with data, more standard code structures, more readable code and some extensions around those API calls. Developers like Karsten Lehmann have gone a step further and exposed lower-level C API functionality to Java.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#directory-management","title":"Directory Management","text":"<p>Domino can manage its own authentication, with out-of-the-box server and client authentication against one or more directories managed within Domino itself. It doesn't and cannot cover all requirements all of the time, but it's very widely used and it just works. Combine with replication (see below) and it solves a problem that has been ignored for years in large, worldwide cloud applications.</p> <p>If the proper name change administration process is used, Domino can then ensure names are updated in Names, Authors and/or Readers fields in selected databases. The administration process for this can also be kicked off programmatically. Name changes, like archiving, may often be ignored in initial development either because of complexity or low impact (for example XPages Help Application on OpenNTF ignores it, so if a name change occurs, bookmarks would be lost). But they occur and they may come back to bite you with inaccurate data or more manual data fixup.</p> <p>Alternatively, security could be handled within an application itself, by authenticating against other Domino data or external sources.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#ldap-service","title":"LDAP Service","text":"<p>Domino also has an LDAP service. I don't fully understand the administrator help about LDAP, so I don't fully understand how having or losing that impacts my application development. But I have seen servers without the LDAP task running where authentication still works. So it seems to be independent of authentication against a Domino directory. The fact is that while I'm developing on Domino, I don't have to care too much. If I'm developing on something else though, I may need to understand more about LDAP, find someone who does, or ignore it and hope it doesn't matter.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#indexers","title":"Indexers","text":"<p>Views and full text indexes get built automatically with Domino. There may need to be some tuning or optimisation, both in terms of application design (for example, if @Now is used), data management (archiving) or in task administration. But in many cases, it just works. A recent fix pack added live view refresh, though it was poorly documented.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#archiving","title":"Archiving","text":"<p>Automatic archiving can be enabled at application level via Database Properties, enabled based on age, lack of modification, expiration date or inclusion in a view. It may not be heavily used, but if relevant it is easily configured.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#replication","title":"Replication","text":"<p>Whether scheduled or manual, complete or partial, clustered or standard, Domino replication is a powerful strength. Domino replication can also run across domains.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#clustering","title":"Clustering","text":"<p>Databases can also be clustered within a domain, whether for scalability or failover.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#security","title":"Security","text":"<p>Domino security is flexible and powerful. It can be server-level, application-level, document-level or field-level. It can be managed via groups, user names, organisational units or roles. There is a level of flexibility that few alternatives, if any, offer (as far as I am currently aware).</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#auditing","title":"Auditing","text":"<p>Many servers log user activity, as Domino does. But out-of-the-box Domino also captures creation date-time and user, last modified date-time and user, as well as last modified date-time of every single field on a document. That information can be viewed in Document Properties (via Seq Num property, cross-referenced with $UpdatedBy) and programmatically (<code>Item.getLastModified()</code>).</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#smtp","title":"SMTP","text":"<p>Domino routes email as well. This is regularly leveraged in applications for workflow and reminder notification.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#scheduling","title":"Scheduling","text":"<p>Domino agents are one of the areas that have not been modernised. LotusScript is outdated and proprietary, Java agents promise a lot for developers who have embraced Java but fail to deliver (memory leaks, inability to easily re-use Java code elsewhere in the application etc). But it is a heavily leveraged area that virtually all existing Domino developers are comfortable with. If considering an alternative, they will need to get familiar with alternatives.</p> <p>Under scheduling also comes scheduled admin tasks, like compacting, fixup, design updates, as well as others.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#dots","title":"DOTS","text":"<p>DOTS (Domino OSGi Tasklet Service) is an alternative for scheduling, but has never been officially supported by IBM for development so has lacked good examples from IBM, and again cannot leverage application logic. It also doesn't have access to a number of OSGi plugins the XPages core has access to. This can make development frustrating, as the only resolution is to copy the plugins across to the osgi-dots folders on the server (taking backups in case upgrades remove them again!).</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#http-server","title":"HTTP Server","text":"<p>In recent times Domino has been leveraged heavily as an HTTP server. There are many alternatives out there, and with XPages on Bluemix IBM are trying to use Bluemix as the HTTP server and a separate Domino server for the data. (The point of paying for another HTTP server runtime when you already need one for the data server is beyond the scope of this post and, if IBM do not deliver enough enhancements to XPages and Domino, moot.)</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#osgi","title":"OSGi","text":"<p>One of the most popular aspects of Domino in the last few years is OSGi. Instead of copying and pasting code between applications, OSGi plugins have been used to load code once for the whole server. With the Update Site database, these extensions are lazy loaded, untouched by upgrades and can be replicated across servers.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#expeditor-web-container","title":"Expeditor Web Container","text":"<p>Although this has not been used much, it's an aspect of the Domino server I've started using and planned to use more. Standard web applications, wrapped in an OSGi plugin, can then be deployed. Again, this area is out-of-date, only using the Servlet 2.5 specification. Most modern web frameworks (e.g. Vaadin) have documentation which demonstrates annotations which require Servlet 3.0 specification or higher.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#xpages-runtime","title":"XPages Runtime","text":"<p>XPages has had a lot of press with Domino, because over the last seven years IBM have made it the future of Domino application development. More recently, on premises application development has been sidelined for XPages on Bluemix. But XPages is just one application development framework that can be used on Domino. There are a number of strengths - rapid development (though not as rapid as some salespeople might have you believe), component-based approach (OneUI has been replaced with Bootstrap which has been updated, all just with renderers), open sourcing that included accepting community code. But lack of understanding of the refresh lifecycle by developers and a reluctance from IBM to recommend better practice (e.g. using Java instead of SSJS) has frustrated. It has also brought into sharp focus out-dated elements (Java 6, the IDE, a draconian Java security policy that chooses to ignore Domino's own security). It has also brought more developers to other web development frameworks and, as outside ICS, there is no consistency on which other frameworks ICS developers prefer.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#summary","title":"Summary","text":"<p>So that's the stack I am currently most familiar with. Different customers may use some or all parts of it. Most applications will only use a subset. Projects will be able to compromise on some and use workarounds for others. The key is knowing what's used for a specific situation, so any future steps are well-planned without nasty surprises around the corner.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#the-present","title":"The Present","text":"<p>Most Domino developers use only the full stack. For some time, some have been using Domino as just a database and web server. Others have been using Domino as middleware, using its runtime but a different backend database. More recently, I've been experimenting with Websphere Liberty as an alternate web server, Vaadin as an alternate application interface framework and Domino just as a database storing data as graph (although the server would also be of use for SMTP / scheduling / auditing etc). Removing Domino for me, someone not extremely familiar with alternatives, leaves a lot of learning, questions, pitfalls, concerns and (I suspect) painful learning. For anyone who uses the whole stack and has not pushed their learning over recent years, it must be terrifying.</p> <p>But I don't have any significant issues with the NSF, whether for storing traditional NoSQL data, less traditional data like Java objects in Notes Items, or graph data objects. I also like the RAD component-based and extensible approach of XPages, although it offers no advice or opinion on best practice. The degree of understanding I've gained over the last seven years means a lot of comfort with stretching the framework, diving into Extension Library code and building my own extensions. I still believe there is untapped potential in XPages and customers are only still starting their journey with it.</p> <p>This is why the recent position of IBM has frustrated me, because it leaves the app dev community in limbo, has put a halt to further empowerment of the community and has undermined a lot of community work and initiatives that were in progress to enhance the developer experience. (Many of these may not be widely known about but would be welcome to developers.)</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/13/thoughts-on-domino/#the-future","title":"The Future","text":"<p>In terms of (any) future for Domino or ICS application development, we will have to see what IBM announce in the coming months, whether developers are engaged by it, what timeframe is stated, whether developers believe they will be met, whether developers are willing to wait, or whether developers reduce or remove their reliance.</p> <p>This blog will cover the learning I gain for my own personal development, even if others do not choose to follow my path. I've long found the benefit of using Google (or books!) to index my learning and remind me of what I've learned but forgotten. So as a starting point, I'll summarise where I am with Websphere Liberty, Vaadin and graph in more detail.</p>","tags":["Editorial","Domino"]},{"location":"blog/2016/07/14/ibm-websphere-liberty/","title":"IBM Websphere Liberty","text":"<p>IBM Websphere Liberty is a widely-used modern lightweight server (less than 70Mb, I have several running on my PC) that allows modern Java EE development with Java 8. The server is free for development and build server, with limited production use (up to 2Gb of JVM heap space across all instances for the organisation). Web applications are deployed as standard web archives, which means it's easy to download and install demos of various frameworks and easy to use Maven. This is something I've fought with when developing OSGi plugins, because Maven is designed to manage and install dependencies be compilation. On the other hand OSGi best practice is to pull them from other plugins, defined in the MANIFEST.MF. At the very least OSGi needs the plugins adding to the classpath in the MANIFEST.MF. Tycho is designed to bridge the gap between what Maven expects and what OSGi expects, but it's not great. So a standard web application, sucking any jar files in via Maven, makes life a lot easier.</p> <p>Setup and installation of your Websphere server(s) is easy, particularly so if you use Eclipse. My preference is to use a separate Eclipse instance and workspace for each Liberty server. This means installing all relevant plugins in separate Eclipse instances, but the Eclipse Marketplace makes this easier. In the Eclipse Marketplace is \"IBM Websphere Application Server Liberty Developer Tools for Neon\" (Neon was released on June 24th 2016, the Liberty tooling already had 1,568 downloads before the end of that month and, to date, has 28.8k downloads). Once installed, it's easy to create and manage a Liberty server from Eclipse, by right-clicking in the Servers view and selecting New &gt; Server. I documented this further on OpenNTF's CrossWorlds wiki.</p> <p>One of the advantages of the server is that features can be deployed to give server-wide functionality. An example of this is CrossWorlds, an application listener and servlet filter to configure and manage Domino thread setup and OpenNTF Domino API factory setup for any configured applications. It means code can be run when the application is initialised and destroyed. An example of where this functionality might be useful is for managing thread pools for access to an underlying database, as seems to be best practice for e.g. OrientDb. Plus it only needs to be installed once on the server, not in each application. Admittedly Maven makes management of duplicate code easier, because it just needs updating in the central Maven repository and can be pulled down into each application. (If a specific version number is defined in the Maven configuration of the application, only that version will be used, but alternatively the latest version will be pulled down.)</p> <p>The server can be configured via XML and other files, or editors from the Eclipse plugin. The main ones are:</p> <ul> <li>bootstrap.properties which holds variables to be loaded at runtime.</li> <li>server.env which holds environment variables for the server to use.</li> <li>server.xml which holds the main server configuration. A screenshot is shown below.</li> </ul> <p></p> <p>I've not gone beyond basic configuration so far. But changing the post the server uses is very straightforward, just defined in the HTTP Endpoint feature. I also set up HTTPS for one server following instructions on a blog post, which was also easy. But I know a real-world deployment would require a lot of additional configuration and questions answering, possible pitfalls and probably optimisation.</p> <p>However, initial setup is very easy, easy enough to complete in just a few minutes and let me focus on doing the development I want. So much so that when developing applications that will subsequently be wrapped up into OSGi plugins for Domino, my preference by far was using Liberty. One of the reasons for that is that, once an application is added to the server, when it's built with any additional changes, it is automatically stopped and restarted with the changes available within seconds. That is very useful. If those few seconds get frustrating, JRebel is the obvious choice.</p> <p>At this point, while I'm focussing on other parts of a stack and predominantly doing development work only, Liberty is useful to stay with. Apache Tomcat is an obvious other server to consider and experiment with, when there is time and need. One area I've not found a quick answer to is how to deploy the kind of server-wide feature extensions used by CrossWorlds. It may be a case of deploying that code within each application itself, I don't know. If that is required, it's probably not a showstopper and certainly not something I have to worry about now.</p>","tags":["Websphere Liberty"]},{"location":"blog/2016/07/26/vaadin/","title":"Vaadin","text":"<p>Before I started working with IBM's XPages framework in 2009, I was starting to use AJAX calls in web applications and starting to dig into Dojo charting options for an application. So not unsurprisingly, when I started with XPages I blogged quite a bit about Dojo charts and understandably chose to write the Dojo-related chapter and a half of \"XPages Extension Library\". I also contributed a Dijit Tooltip custom control and an extension to the Dojo Legend component, to allow more sophisticated formatting of the legend.</p> <p>But after my early projects I was already preferring Java as a server-side language to IBM's proprietary shim on top of Java, Server-Side JavaScript or SSJS as it is commonly known. When it was explained to me that SSJS was compiled as a string and parsed at runtime to call underlying Java methods, it just made much more sense to cut out the SSJS layer, regardless of how daunting Java might seem. The benefits of getting more familiar with Java grew when writing \"XPages Extension Library\", to be able to understand what was happening under the hood, and when using more standard Java packages like Apache POI. I began to find a choice appearing, one I saw appearing for more and more developers, between specialising in Java or JavaScript. My background in logical languages and the editor made my mind up towards Java.</p> <p>Having been a full-stack developer for so long, and a \"developer\" rather than a \"programmer\", my preference is to develop smaller applications with Java access to the back-end database. My preference is also for a Java framework that gives a decent look and feel but lets me focus on functionality. Some will prefer building pretty widgets that give an optimal look and feel, but I am happy to accept a more standard user interface. Many will not agree with me, but the existence of many frameworks leads me to believe that some do. It's a personal preference, one of many in the world of development.</p> <p>There are a number of Java frameworks out there, with Spring MVC, Spring Boot and JSF being very popular in this year's survey by ZeroTurnaround. But Vaadin is in the \"chasing pack\". I was introduced to Vaadin by a colleague about 18 months ago and it soon became a firm favourite. Vaadin has been pretty easy to pick up after XPages, with a nice look and feel and component-based approach, as well as being open source. The class and method names were pretty similar to XPages as well as being very self-explanatory. The theming approach also fitted well with what I was used to.</p> <p>Based on my experience of the last (mumble mumble!) years, documentation and community are also key aspects for me. The documentation is of a high quality with good examples and the certification exam is comprehensive and challenging (I've yet to pass it, I tried about a week after I started and failed - I know there are aspects I'm yet to dive into). The community has been impressively open and vibrant, the Vaadin developers and developer advocates have been very welcoming and actively engaged with the community. I was approached late last year to write a blog post on getting started with Vaadin on Domino and the company (not surprisingly) are committed to the product. They also are not shy about recommending best practice and Vaadin Designer structures the files it creates in order to create the best practice separation between UI and business logic. This all makes it a framework that's enjoyable to work with and minimises floundering with new technology.</p> <p>The main downside is fighting positioning and CSS, which the forums show is a not uncommon challenge. But the posts also help solve the problems. Vaadin Designer makes some of this easier. It is a fairly new (licensed) add-on (version 1.0 came out last October), currently just for Eclipse although an IntelliJ version is in the works. It's a good starting point and useful for small layouts, but there is still room for improvement (e.g. interaction with the valo theme when setting styles). As part of the Vaadin Pro Tools, I am sure there will be further enhancements.</p> <p>Speaking of the forum, it is extremely useful and has typically answered very quickly any questions or challenges I've encountered. The Vaadin developers and technical advocates are very active there, as are others in the community.</p> <p>And speaking of community, there are already a large number of add-ons, some of which I am conscious I need to dive into much more (particularly viritin). Looking at the names of authors, many are familiar as people working for Vaadin, so it's good to see very active contribution from the developers and advocates. Some seem to reference Vaadin 6 (we're currently on Vaadin 7), so I'm not sure how active they all are. But the specific categories in add-on releases to licensing, maturity and versions supported are very useful, as well as links to code samples and other related links. After all, the critical requirements for any open source repository of extensions is to make it easy to identify whether a given extension will work with the specific versions you need, make it easy to get a proof of concept into your development environment and give confidence in future-proofing. It's also of interest to me as an OpenNTF Board Member helping manage an open source repository for projects that have accumulated over a large number of years.</p> <p>A vibrant community demonstrates a passion for a product. And if the company responsible for that product are engaged with their community, the community response drives them to keep enhancing the product. The community will also enhance and evangelise the product, helping it to grow. A symbiotic relationship between the company and the community is key, and if both are engaged, great things can happen. I've seen that over the last few years with XPages, although IBM seems to have disengaged from the development community over recent months. The relationship between Vaadin and the community has impressed me greatly over the last 18 months. With a company whose core business is application development, that should continue providing they are not acquired by a larger company that dilutes their vision. No one can consistently predict the future, but the current vision of Vaadin means it is a development framework I look forward to working with for the foreseeable future.</p>","tags":["Vaadin"]},{"location":"blog/2016/07/30/communication-and-community/","title":"Communication and Community","text":"<p>Communication &amp; transparency are critical to lifeblood of any community, open source or closed. Seeing impacts in more than one community</p>\u2014 Paul Withers (@PaulSWithers) 28 July 2016 <p>A couple of days ago I posted this tweet. It was prompted by experiences in a variety of technical communities over recent months and year, good and bad. Anyone who knows me or has been involved in the IBM Collaboration Solutions community will be aware of how passionately I believe in the power of community and the importance of empowering those within the community. It was a topic I blogged about in depth earlier this year. And it shouldn't come as a surprise, considering I'm a board member on for open source community, OpenNTF.</p> <p>In my last blog post, I talked about my enthusiasm for and enjoyment of Vaadin. It's an open source framework with an engaged community and a committed, open and communicative provider in Vaadin themselves. With regular releases, regular blog posts (by evangelists at Vaadin and beyond), active interaction with the community on forums, it gives someone on the periphery of that community confidence in the future of the platform and its community. That's important for those committing money for their company on training or deployment of a production application on that platform. It's even more important for developers investing their personal time in learning a new framework or providing fixes, enhancements and extensions for the platform.</p> <p>When communication stops, however, FUD starts to set in. This is something I've seen on a number of occasions with a variety of technologies, open source and closed. It's why I've sought to be accessible and responsive in the open source projects I'm involved with. It's also why I've tried to ensure there are others who can take key projects forward, so it can be guaranteed a future regardless of my involvement. That leaves me able to stay involved as long as I wish (both actively developing and empowering others to develop them), free to pursue other projects, and confident in their longevity should any unforeseen circumstance ensue (god forbid!).</p> <p>With open source, a lack of communication leaves the community uncertain whether or not they should step into the breach, cautious either because they may be stepping on the toes of those who have been heavily involved, or because they may be taking the project in a direction contrary to where the core developers expected to take it, or because there may not be anyone who has the authority to create an official release incorporating their work. There may be unspoken reasons that may justify some period of quiet, resulting in some patience from the community. But if that period of quiet continues, concern will increase.</p> <p>In some cases even an admission from those responsible for the project that they have no intention to continue can be helpful. It allows the project to move forward. I was recently involved as an intermediary between a potential committer and a project owner, which resulted in the committer being welcomed as the new project owner, a satisfactory outcome for all involved. Even in the worst case scenario of no one wanting to take over, at least it gives clarity that the project is dead. But if the project is truly open source, it's not the fault of the project owner (providing they're open to upskilling potential new project owners), it's the fault of the community who collectively were not willing to take over ownership. There may be a need for specialist skills, but in most cases skills can be learned.</p> <p>Sometimes even after a sustained period of quiet, there are whispers that those involved do intend to return to open source. When that happens, it's certainly welcome, regardless of the amount of time they intend to devote.</p> <p>In closed source, the outcome can be more destructive because no community has ever been empowered to take the product forward. It results in significant effort to reinvent the wheel. If communication and transparency has been lacking, that effort is required with limited notice and no choice. If the product is a component in a larger stack and not easily replaced, the resulting frustration and disappointment (possibly more extreme - despair and anger) can mean a re-evaluation of the whole stack, particularly if the cost or feasibility of replacing the component is prohibitive. It can significantly impact those who have supported the product in good faith, as well as those who may appear to no longer care. At the very least it results in a loss of good faith which may be difficult to regain.</p> <p>But whether open or closed source, communication can only go so far. Sooner or later there needs to be action to back up the words. Without action, the community will start to tune out. I'm reminded of Aesop's fable of the boy who cried wolf. But the action needs to show two-way communication. There is nothing so dispiriting as an outcome that is not relevant to the community or fails to deliver on the promise of the vision sold.</p> <p>This may seem a negative and cautionary tale, but again I'm going to draw on my school and university days with Latin and Greek. There are a host of woes here. For those committed to a given open source project or closed source product, it's probably analogous to a Pandora's box. But at the bottom of that box, there is always hope. It may not be small and not the hope you were specifically looking for, but seek hard enough and it may be a hope that takes you in a new and unexpected direction. The road out of the darkness is never easy. A herculean effort with a lot of painful stumbling may still ensue. Even as the dawn rises, like Orpheus leaving the underworld, a bitter blow may await. But perseverance and adaptability is an inherent trait of the human psyche.</p>"},{"location":"blog/2016/08/24/graph-databases/","title":"Why Graph?","text":"<p>The bulk of my experience with application development has been building workflow-related rich client and web applications on NoSQL databases, typically IBM Domino. The challenge in the Notes Client was to provide dashboard-style displays and a good way to display documents for action by the current individual. Private views can be used, but impact database performance. So, typically, the approach is to display views that present a  scrollable table of data. Domino's document-level reader security is then used to ensure only the appropriate data is visible. If data is archived appropriately, performance of the database is good enough for many reasonably-sized applications. (Of course, archiving is often omitted from scope of the first phase for the rapidly-developed application, and becomes a case of \"out of sight, out of mind\".) But with the increasing prevalence of web applications replacing Notes Client applications, the ability to display \"my documents\" and use structured searches to display a targeted subset of documents was much easier.</p> <p>NoSQL datastores with a document-driven API are something I'm very familiar with, and equally familiar with the criticism and challenge that they lead to content being duplicated on different document types, which can lead to inconsistency. For all the criticisms, the flexibility of schema has led to a degree of popularity in NoSQL that relational databases could not ignore, resulting in the rise of NewSQL databases.</p> <p>But another approach has gained ground in recent years, particularly for large datasets - graph databases. Over the last couple of years I've been introduced to graph databases via the Apache Tinkerpop API and an open source implementation for IBM Domino by Nathan T. Freeman in OpenNTF Domino API. As a co-developer of the project I was an early adopter and it became an attractive way of building the data architecture.</p> <p>The approach of vertices and edges works nicely for workflow-related applications. Storing approvals on the document type being approved was a typical approach for document-driven databases but an alternative was having separate \"child\" documents for each approval. Separate child documents gives greater flexibility and scalability (what if an additional level is needed or a level removed?). But approvals have to be related both to the object being approved and to the individual approving. Approval then requires access via each approval document (e.g. InvoiceApproval), but overall review requires access via the document type being approved (e.g. Invoice). Graph gives that flexibility, by having the InvoiceApproval as a vertex connected via separate edges to both the Invoice and the Person. Alternatively, RequiresApproval, Approved or Rejected could be different edges between the Person and the Invoice. If the edge can contain properties of its own, a submission can generate a RequiresApproval edge, which is replaced by an Approved or Rejected edge as appropriate, with dates and comments stored as properties of the edge.</p> <p>Similarly, graph databases fit nicely for social data, like comments on a blog post. The graph approach allows the Comment vertex to have edges to the BlogPost, the Author, as well as anyone Mentioned.</p> <p>Although it's not something I've implemented, this multi-directional approach could also add benefits to traditionally hierarchical systems like CRM applications. Person A works for Company X at Location N. The hierarchy of Company-Location-Person worked nicely when everyone worked at a main office. But what if they work from home and don't want your system to have their home address? Do you store them against a dummy \"Home\" Location? Or against an office they sometimes go to? If so, do you add some comment to say that they actually work from home? Then there's the scenario of Person A leaving to work for Company Y, who you also work with. What happens to their interactions with you when they were at Company X? Yes, they're relevant for whoever takes over their position at Company X, but they may also be relevant to the business you want to do with them at Company Y. Do you duplicate the interactions? Graph would allow them to be in both places, all interactions connected to Person A, the interactions prior to their move connected to Company X and the ones afterwards connected to Company Y.</p> <p>Once you understand how to model your data in this vertex-edge structure, it becomes quite easy to construct the architecture. But the strength of the OpenNTF Domino API implementation is two-fold.</p> <p>Firstly, it was built prior to Tinkerpop 3 and so uses framed graphs. This means vertices and edges can be converted to Java objects by just defining a Java interface. The core code manages the getters and setters and other methods. This speeds up development quite a bit and also removes the need to deal directly with vertices and edges, unless you wish to.</p> <p>Secondly, because it's built on Domino, it means the database is multi-model. Because Domino is NoSQL, the content is accessible as Documents. At the same time, because the OpenNTF Domino API makes the Document class extend Map, the content is also accessible by querying the documents as Java Maps. And because it's using Tinkerpop, you can build the databases using a Graph API. Then by using Proxy Vertices, existing Domino documents can be extended into the Graph API by adding a \"wrapper document\", so the graph wrapper document is queried for properties and, if they're not found, the core document is queried. This is particularly relevant where the core document should not be modified, for example when dealing with the Person documents managed by the core Domino server and administration processes. This gives a wealth of flexibility, for creating applications using a NoSQL database design or a graph database design, and even combining the two.</p> <p>As a result, when it comes to re-evaluating what database backend to use going forward, graph is my preferred approach, particularly once based on Tinkerpop, with which I've become familiar. The next steps from there have become a little more challenging though.</p>"},{"location":"blog/2016/09/20/taking-titan-to-the-next-level/","title":"Taking Titan to the Next Level","text":"<p>Since earlier this year when I started trying to get a better handle on the breadth of graph database options available for a developer, Titan has been an option I have kept in regular contact with. It's fair to say there has been a lot of uncertainty about the prospects for Titan. But there have been some interesting developments regarding Titan during the summer. IBM Graph has reached GA on Bluemix, albeit with only REST access, which may not appeal to Java developers, particularly those familiar with Titan and comfortable with natively handling vertices and edges. And more recently there has been a lot of work on integrating Titan with ScyllaDb, which provides a long-term option for using Thrift as a communication mechanism between Titan and the backend database.</p> <p>With Scylla Summit and Cassandra Summit in close proximity (in location as well as time), it was a good opportunity for discussions to be had. Recently Jason Plurad of IBM has led those discussions in the community. For those interested in graph and open source, like myself, it's worth being very aware of an initiative Jason is promoting on the Google Group for Titan to take an incubation proposal for Titan to the Apache Foundation. This is definitely what Titan needs now, a new team to take it forward beyond 1.0 release. Although I've not really played with it yet, the foundation seems solid, with some good documentation, a choice of backends and a choice of index backends.</p> <p>Now is not the right time for me to get involved - there are other initiatives I'm working on (both personally and for OpenNTF) and other discussions I feel are more critical to get involved in for the next few months. But it's a project I will be actively following. As a technical evangelist committed to open source, who fervently believes in the benefit and potential of graph, it's a project that, with the right time, I would like to be involved in. Much of my work over the last few years has been around documentation, speaking and training. These are areas some developers are less interested in working on - understandably we like adding functionality more than detailing what's been added. So I'll be keeping tabs, and we'll see what next year brings.</p> <p>But good luck to Titan and thanks to all those dedicating their time to the worthy cause of open source. Regardless of what open source products you use, anyone involved with open source is always to be applauded.</p>","tags":["Databases","Graph Databases"]},{"location":"blog/2016/09/28/impress-evangelist/","title":"How To Impress A Technical Evangelist","text":"<p>I've been involved in developing training materials, tutorials, videos, online documentation and even books. So I've gained a full appreciation of the effort involved in not only creating good documentation but maintaining it too. Rene Winkelmeyer wrote a good blog post today about developer experience and his points are very valid.</p> <p>Over the last couple of years I've seen a number of approaches as I've dug into a variety of new technologies.</p> <p>One approach has been to deliver a workshop to business partners. That's okay if the business partners chosen will be cascading training down to a wider audience. But if the business partners are primarily focussed on providing services or developing products, that approach is not going to result in widespread adoption.It might result in a few products your salespeople can sell on, after time, if those trained embrace the solution the technology. But the rest of the world will probably ignore it, especially if there are gotchas or tips that are not more widely shared.</p> <p>Online documentation and demos are very powerful, if the quality is high. I first came across this with Vaadin last year and I've already discussed that on the blog post about Vaadin. The challenge is keeping the documentation relevant and up-to-date with regular release cycles, but Vaadin have done a good job at doing that. This requires considerable investment. The strong visibility of the Vaadin developers and evangelists makes it clear there is that commitment from the providers.</p> <p>With flexibility inevitably comes bad practice, but a good provider will recommend best practice approaches. There are a number of Vaadin webinars that do that, even beyond using such standard tools as FindBugs.</p> <p>But if a technology is just part of a stack, there is a bigger challenge. The other parts of the stack may vary and may work differently. A Java application could be deployed as a jar program or a web application, could connect to a variety of backends, which may or may not include persistence. It may be deployed standalone or on a web server. If the provider doesn't provide their own IDE for the developers, then that is also an area of personal choice and integration may need continual effort. If they do, it's guaranteed to be compared to other IDEs and needs to be kept up-to-date. But beyond all of this, there is an additional challenge: if the developer cannot easily integrate the technology into their preferred stack, you've lost them. To do so will require specific skills beyond the technology itself. The Vaadin Bluemix challenge last year worked well to bring together the two parts of the stack - the PaaS and web development framework. Similarly Vaadin webinars have brought together complementary parts of a stack to help empower developers to take advantage of each other's technologies.</p> <p>For a fuller stack, there is a bigger challenge. Not only does the provider have to ensure not only good documentation, tutorials and evangelists to grow and empower. They also have to ensure all elements of the stack are fresh and up-to-date. The longer a technology is around, the bigger the challenge. It's easy to get distracted by building something new and lose focus on keeping what is there up-to-date and regularly enhancing it. But adding something new means something new to keep up-to-date, which means more expense. If there are not enhancements and regular prospect of enhancements, developers may fear that area is being silently deprecated. Moving off a platform is not easy, and they often persist a lot longer than planned. But deciding a platform is not part of a strategic future is much easier, and if the applications on the platform are not enhanced, they increase the perception that the platform itself is legacy, which further reinforces that the platform should not have be part of a strategic future. It becomes a self-fulfilling prophecy and, if it becomes pervasive, the platform and its applications may just atrophy.</p> <p>Sometimes it may be more appropriate to bring in other technologies as an alternative or replacement, and I've seen that in XPages with Bootstrap becoming the preferred UI framework over IBM's OneUI. The technology needs to be scalable to enable that to happen and the decision needs to be attractive both in terms of financial cost and developer effort. XPages is a component framework, the same as Vaadin and much of what I've seen with Salesforce. So refreshing a whole application to a new look and feel is much easier: the framework handles it for the developer, with minimal (if any) coding changes. Change the underlying renderers, applied via a specific theme, and most if not all of the application just changes the look. Similarly for XPages, the component approach meant upgrading from Bootstrap 2 to Bootstrap 3 was done at the framework layer, with different renderers. Abstracting the rendering minimises the effort required and lets developers focus on functionality. It makes the application cheaper to upgrade and keep refreshed for the customer. With the proliferation of browser-based applications, minimising the cost in order the ensure they keep working on modern browsers may be more and more important for enterprise and SMB customers.</p> <p>When it comes to documentation, cloud technologies like Salesforce have a bigger challenge. The cloud requires even more frequent releases and developers have no choice - they can't choose not to upgrade, they have to get the new releases. So the documentation needs to support them, it becomes even more critical that the documentation be kept up-to-date and good quality. As I've started to play with Salesforce, this is an area that has impressed me significantly. Keir Bowden blogged about how Trailhead has vastly enhanced documentation and made it easier for developers to hit the ground running. The quality and accuracy of the early tutorials I've tried has impressed me. The combination of videos and textual instructions, with challenges in each section to reinforce learning via gamification makes it useful both as I'm learning, but I can also see it being beneficial later on. At this point I don't have a specific project, so I'm not in the situation I was with Vaadin of diving in and referring to the documentation as and when I couldn't work something out. That's a valid use case for which I can't comment on their efficacy. But from the amount of tutorials and the pace of cloud releases, it's clear there is a huge investment from Salesforce, an investment that will be required for the long term, and that is very impressive.</p>"},{"location":"blog/2016/10/20/making-progress/","title":"Making Progress (Bars)","text":"<p>Earlier this week I had problems with high CPU utilisation and had to restart my PC. I took the opportunity to bite the bullet and install some Windows Updates. What I saw brought my mind back to UX and coding of applications. For at least five minutes, the progress displayed as \"100% Complete\". It prompted me to issue the following tweets:</p> <p>Developers, progress bar should never show 100% complete. It should be 0% of next process or gone. Windows updates 100% complete for minutes</p>\u2014 Paul Withers (@PaulSWithers) 18 October 2016 <p>As web applications have become more sophisticated, it is commonplace for applications to load at least the basics of a page before running additional processes, or to make callbacks to the server before taking action based on the response (what used to be called AJAX calls, are still asynchronous, but rarely return XML nowadays). There are various mechanisms for managing the user experience of such processes, from the non-existent to the sophisticated - doing absolutely nothing, just disabling a button clicked, adding a mask over the page to prevent access while the process runs, adding some kind of animated image to the mask to help comfort the user that something is still happening, or adding some kind of progress bar that is updated as degrees of progress are reached. They may be newer to web development, but they've existed in computers for as long as I can remember.</p> <p>But being on the user end gives some kind of perspective. A progress bar that updates its progress may seem the best and most sophisticated approach. It's certainly the most sophisticated, but in my opinion not necessarily the best. Take the scenario I had with a status of \"100% Complete\" or a progress bar that just sits at 100% full. What is the user to think? How does the user know something is still happening? After all, as I vented on Twitter, if it's actually doing something either the process is not at 100%, or there is another process which is happening. And as much as the developer might think that process won't last long so doesn't need a progress bar, as in this case, it might not be instantaneous. And when (not \"if\") it's not, the effort put in to create a nice user experience by showing progress for the last process is completely undermined. The developer has took the time to communicate to the user that something is happening and will take some time, they've communicated where in the process it is, but they've then misled the end user by implying that everything is finished.</p> <p>Admittedly, as developers we're all also human beings and the nature of being human is to be lazy. So there will be times when we don't make the effort to produce that best user experience. If the process isn't critical, maybe we can afford to be lazy. But if the process is critical, and connectivity or other circumstances mean a process takes longer than we think, and we don't bother with the progress bar, we can't blame the user if they think it's hung and not doing something when in fact it is. If they abort the process prematurely, it's then our fault, not theirs.</p>","tags":["Editorial"]},{"location":"blog/2016/10/24/mixing-webxml-and-annotations/","title":"Mixing web.xml and Annotations","text":"<p>Over the last few months a lot of what I've had to do has come from combining frameworks. Usually one of those frameworks is Vaadin. But whenever you're combining frameworks of any kind, it usually means some content is pre-configured, which may conflict with settings in another framework. If you're not familiar with the framework and the technologies in use, it's a lot like looking at hieroglyphics without a Rosetta Stone! The result is a lot of learning on the job.</p> <p>Over recent releases Vaadin has moved from using a web.xml file for configuration to annotations. Some of my work still requires using the web.xml - IBM Domino currently runs the Expeditor Web Container, which uses an older servlet container that only fulfils Servlet 2.5 specification. But where possible I look to use more modern approach, and running on Websphere Liberty that's possible.</p> <p>But a recent framework still used a web.xml. When I tried combining them, instead of getting a button displayed, all I got was the text. It didn't seem to be loading the Vaadin theme properly. After moving various classes around, amending various settings, trying newer versions in the pom.xml and still not getting anywhere, I decided to come at the problem from the other end. I created a standard Vaadin Maven project and tried adding the web.xml, copying and pasting the basics across from the project that was not working. Promptly it failed, which enabled me to isolate the responsible attribute of the web-app element, metadata-complete=\"true\". It made sense straightaway, and the tooltip documentation confirms this:</p> <p>Attribute : metadata-complete The metadata-complete attribute defines whether this  deployment descriptor and other related deployment descriptors  for this module (e.g., web service descriptors) are complete, or  whether the class files available to this module and packaged with  this application should be examined for annotations that specify  deployment information. If metadata-complete is set to \"true\",  the deployment tool must ignore any annotations that specify  deployment information, which might be present in the class files  of the application. If metadata-complete is not specified or is set  to \"false\", the deployment tool must examine the class files of the  application for annotations, as specified by the specifications.</p> <p>Data Type : boolean Enumerated Values :     - true     - false</p> <p>There is a trade-off in startup performance of the application - jar files need to be iterated for servlet annotations, web fragments, injected resources etc. But if you prefer annotations over XML, it's worth bearing in mind.</p>","tags":["Vaadin","Java"]},{"location":"blog/2016/11/15/wws-java-api/","title":"Creating a Java API to Access Watson Work Services","text":"<p>A few weeks ago, IBM announced Watson Workspace, the final name for Project Toscana, and its API Watson Work Services. The product itself has similarities to Slack or Microsoft Teams, but this post is not about discussing a comparison of the products. It's about the API backing it.</p> <p>Watson Work Services is a REST API that uses GraphQL, a method of querying and posting via REST that focuses on configurability. Whereas traditional REST services have fixed endpoints that take fixed parameters and return fixed data objects, GraphQL is a sort of \"API-as-a-service\". You call an endpoint, pass a JSON object determining what elements of which data you want to return, include any dynamic variable values. The queries are passed to an engine at a REST service endpoint which parses the JSON passed, replaces any variables with the dynamically passed values, and returns just what the application or user asks for. This may include data from what, in a traditional REST API application, would be from different REST endpoints. For example, to get members and messages from a space, you might need to make a REST call to get the space ID, then another REST call to get its members, and a third to get the messages.</p> <p>Before Watson Work Services I had not heard of GraphQL and mistakenly related it to Graph databases. But since investigating it, I'm a convert. It's the future of REST services, in my opinion.</p> <p>Earlier this year, I had looked at a Java API to create REST service calls to Stash (Slack Java API and slack-api). Thankfully a fellow community member had begun the work of converting resulting JSON into Java objects. But it was the posting and query side of it that interested me most of all.</p> <p>While looking at various Graph database options earlier this year, I saw a number of Java APIs that used REST or JDBC to connect to backend databases, going beyond REST options and similar to what I would need, where the content of the query or stored procedure was more flexible. That left me firmly convinced of what is best practice and what is casual implementation. The strength of Java is that it is a strongly-typed language: where possible errors are caught at compile time rather than deferred to runtime. Some of those APIs had Java methods  to build up the query as a Java object, which the API converted to a string to pass to the underlying database. Others required queries to be stored as strings which were then passed to be parsed at runtime. Maybe others trust themselves to avoid typos and other easy mistakes, but I don't. In mine and in others code, I've often seen issues caused by unidentified typos. That's why I have for some time preferred Java over JavaScript. So when it came to writing an API, I determined early on that creating the queries would be done via Java methods, not storing them as strings. I was under no illusion that this meant a lot more work from me. But I was convinced it was the better approach and one that would be welcomed as generating more readable queries and fewer errors.</p> <p>Going back to those REST service examples for Slack, the second example uses a Java method that takes all parameters required by the REST service call and then adds them as arguments to the REST service call. That's easy in a standard REST service because the parameters are fixed. The second creates an object that then gets parsed to build the parameters for the REST API. That was more the kind of thing I needed.</p> <p>There were a few aborted attempts as I fumbled my way via trial-and-error. But I've got something that works now for queries. Mutations may change what's required, but I have a starting point. For the query itself, I have a <code>DataSender</code> object that has <code>addAttribute()</code> to pass what appears in brackets after the object name in the JSON, <code>addField()</code> to pass scalar fields to return and <code>addChild()</code> to add other objects to return, each of which could have their own attributes, fields, and children. There also seems to be a standard PageInfo object that can be returned, with standard values, so the object also has an <code>addPageInfo()</code> method. This results in some long queries - below is the current full code for a request for spaces, their members and messages. But it's much more readable than the string JSON query that results.</p> <pre><code>    // Basic createdBy ObjectDataBringer - same label for all\n    ObjectDataSender createdBy = new ObjectDataSender(SpaceChildren.UPDATED_BY.getLabel());\n    createdBy.addField(PersonFields.ID.getLabel());\n    createdBy.addField(PersonFields.DISPLAY_NAME.getLabel());\n    createdBy.addField(PersonFields.PHOTO_URL.getLabel());\n    createdBy.addField(PersonFields.EMAIL.getLabel());\n\n    // Basic updatedBy ObjectDataBringer - same label for all\n    ObjectDataSender updatedBy = new ObjectDataSender(SpaceChildren.UPDATED_BY.getLabel());\n    updatedBy.addField(PersonFields.ID.getLabel());\n    updatedBy.addField(PersonFields.DISPLAY_NAME.getLabel());\n    updatedBy.addField(PersonFields.PHOTO_URL.getLabel());\n    updatedBy.addField(PersonFields.EMAIL.getLabel());\n\n    ObjectDataSender spaces = new ObjectDataSender(\"spaces\", true);\n    spaces.addAttribute(BasicPaginationEnum.FIRST.getLabel(), 100);\n    spaces.addPageInfo();\n    spaces.addField(SpaceFields.ID.getLabel());\n    spaces.addField(SpaceFields.TITLE.getLabel());\n    spaces.addField(SpaceFields.DESCRIPTION.getLabel());\n    spaces.addField(SpaceFields.UPDATED.getLabel());\n    spaces.addChild(updatedBy);\n    spaces.addField(SpaceFields.CREATED.getLabel());\n    spaces.addChild(createdBy);\n    ObjectDataSender members = new ObjectDataSender(SpaceChildren.MEMBERS.getLabel(), SpaceChildren.MEMBERS.getEnumClass(), true, false);\n    members.addAttribute(BasicPaginationEnum.FIRST.getLabel(), 100);\n    members.addField(PersonFields.ID.getLabel());\n    members.addField(PersonFields.PHOTO_URL.getLabel());\n    members.addField(PersonFields.EMAIL.getLabel());\n    members.addField(PersonFields.DISPLAY_NAME.getLabel());\n    spaces.addChild(members);\n    ObjectDataSender conversation = new ObjectDataSender(SpaceChildren.CONVERSATION.getLabel());\n    conversation.addField(ConversationFields.ID.getLabel());\n    conversation.addField(ConversationFields.CREATED.getLabel());\n    conversation.addChild(createdBy);\n    conversation.addField(ConversationFields.UPDATED.getLabel());\n    conversation.addChild(updatedBy);\n    ObjectDataSender messages = new ObjectDataSender(ConversationChildren.MESSAGES.getLabel(), true);\n    messages.addAttribute(BasicPaginationEnum.FIRST.getLabel(), 200);\n    messages.addPageInfo();\n    messages.addField(MessageFields.CONTENT_TYPE.getLabel());\n    messages.addField(MessageFields.CONTENT.getLabel());\n    messages.addField(MessageFields.CREATED.getLabel());\n    messages.addField(MessageFields.UPDATED.getLabel());\n    messages.addChild(createdBy);\n    messages.addChild(updatedBy);\n    conversation.addChild(messages);\n    spaces.addChild(conversation);\n    GraphQLRequest req = new GraphQLRequest(spaces, \"getSpaces\");\n    return req.getQuery();\n</code></pre> <p>I'm still some way from completion, but I have working queries  and something I feel is more developer-friendly and error-proof.</p>","tags":["Java","Watson Work Services","GraphQL"]},{"location":"blog/2017/01/13/graph-database-news/","title":"Graph Database News","text":"<p>Last year I blogged about an initiative to take TitanDb to the Apache Foundation as an incubation project. Yesterday, the next step for TitanDb was announced, when the Linux Foundation welcomed JanusGraph, a project which uses TitanDb as its starting point. The current documentation will look familiar to anyone who has investigated TitanDb, but the news will be welcomed by the TitanDb community. So it's worth keeping an eye on the JanusGraph website.</p> <p>Also yesterday, I picked up a tweet that pointed to a new Java Client for IBM Graph. Considering IBM Graph is using TitanDb and Apache Cassandra, the lack of a Java SDK was a significant downside for me. So it's good to see that IBMers have released this client, as well as a sample application.</p>","tags":["Databases","Graph Databases"]},{"location":"blog/2017/02/25/ibm-connect-2017/","title":"IBM Connect 2017: Embrace Fear","text":"","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/02/25/ibm-connect-2017/#this-is-not-a-vendor-pitch","title":"This Is Not a Vendor Pitch","text":"<p>When I began my personal blog last year, I was investigating new technologies beyond the IBM Collaboration Solutions stack. This last week I've been attending and presenting at the conference, including covering one of those new technologies. The experience for me has changed drastically the direction I was heading. But this is not about a vendor's products, this is about the open source technologies I intend to pursue over the next year.</p> <p>Firstly, much of my direction over the last year has been Java. I like Java, the way it's strongly-typed, the way editors pull in what you need, particularly using something like Maven to do dependency management. And my brain is only so big. So keeping Java, JavaScript frameworks and all the proprietary technologies concerned me. But what concerned me most was that with current experiences, I couldn't see a way to regularly be working with JavaScript frameworks in my day-to-day work. And my personal time is personal, I've spent a lot of time evangelising to a certain audience and as a board member developing infrastructure and process management for OpenNTF. So with the speed of change in JavaScript frameworks, working with them for a couple of months, learning things, then not touching them for six or nine months during which the landscape shifted again, that was just pointless and depressing. So the key has always been technologies that works for business, that work for me, that work for OpenNTF, that work with other frameworks that I see having benefit like Darwino, and that give me scope to continue evangelising.</p>","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/02/25/ibm-connect-2017/#turning-an-issue-into-a-feature","title":"Turning an Issue Into A Feature","text":"<p>Or \"It's Okay to Fail\". Monday was spent taking part in a hackathon. The planned outcome we were to have was a whereabouts app - not granular access into calendars, but what are people I'm interested in doing today, tomorrow etc. \"Office\", \"Working from Home\", \"Customer A\", \"Office am, Holiday pm\", that kind of thing. Then with microservice functionality around it. Integration with IFTTT to know you were in the office now because your mobile device connected to a certain wifi, to know you were no longer in the office now because your mobile device disconnected from a certain wifi. Also microservice functionality to integrate with Watson Workspace where you could ask where someone was. It was going to be API driven, REST services whose core methods could also be called directly from Java by the (Java-based) UI controllers. The idea generated a lot of interest from the IBMers visiting.</p> <p>We failed. Miserably! We struggled to even get the basics of the app working, when I had expected to have that part done by lunchtime.</p>","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/02/25/ibm-connect-2017/#graphql","title":"GraphQL","text":"<p>The desire to integrate with Watson Workspace and the confidence about how easy that part would be, that all came from experience since October of the Watson Work APIs. Technology-wise, the interest for me came in GraphQL. Within half an hour of reading the documentation I was sold on the developer benefits of not only consuming it but also providing it. So when I was looking for a topic for an abstract for IBM Connect, GraphQL was a no-brainer and pretty much a certainty in my mind to be accepted. I expect to be delivering sessions on it for quite a bit this year, because it's still so new and it's power is unknown for the attendees of many of the conferences I speak at.</p> <p>What was also key for my week was the start of that hackathon, when I was nursing a hangover, still feeling queasy. Jason Roy Gary gave the introductory speech, talking about Connections Pink, and he mentioned it would include GraphQL access. My interest was piqued.</p>","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/02/25/ibm-connect-2017/#the-turning-point-pink-wednesday","title":"The Turning Point: Pink Wednesday","text":"<p>It takes a confident man to wear a pink suit. It takes someone very special to pull it off.</p> <p>I had spoken to Jason Roy Gary again the night before, had heard about his apparel for the session, and had been encouraged to attend. When I sat the following morning, I had two other sessions on my schedule at the same time, one that was very pertinent to the technologies I was currently working with. What made my mind up was seeing a slide deck that was not significantly different to what had previously been presented. I even passed on a technical session on Connections Pink to go to the more high-level introductory session by Jason Roy Gary and Baan Slaven.</p> <p>I was already aware that Connections Pink was aimed at allowing developers to extend it, and as a developer who has struggled with developing against its predecessor, that interested me. What really interested was this slide:</p> <p></p> <p>Yes, I had sat in the session for the speaker not the screen, so it's not at the best angle. But this is the first time I remember IBM Collaboration Solutions explicitly stating the stack they had chosen. Knowing that Darwino really needed a JavaScript framework to develop with, that Connections Pink would give me a framework, this interested me as tools and technologies I could get regular experience with, my main priority.</p> <p>It scares the hell out of me, because there are technologies that are just words. But I have a community. Those in that community have a community, and one that is renowned beyond the community for being helpful not judgemental. We can be scared together. We can fail. We can learn. And we can adapt.</p> <p>What was also extremely interesting was this slide:</p> <p></p> <p>This is an amazingly refreshing attitude and one which, as a developer, excites me. I've been involved in developing with IBM occasionally. I wrote XPages Extension Library book for IBM Press with an IBMer. I contributed code back to the Extension Library via pull requests. At a time when resources are being squeezed more and more, it's the only approach that makes sense.</p> <p>And if you're looking to work on a project that includes new technologies, and you're looking to document that, it's the \"how\" and documenting that journey which is most important, not what you build. The \"how\" was important to me when I took two weeks off work to change from building XPages with Server-Side JavaScript some years ago to build a small Help application with XPages and Java. I produced and shared the \"what\", but not the \"how\". As an evangelist now, I realising sharing the \"how\" is the key thing I need to do over the next year.</p>","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/02/25/ibm-connect-2017/#the-new-stack","title":"The New Stack","text":"<p>What does my new stack look like? I'll tell you when I know what works for me, could appeal to my customers and could appeal to other developers. As an evangelist and an employed developer, I'm very mindful of others.</p> <p>But IBM Domino will be a key part of that. It's a strong data store with a long heritage, with community work that makes it even more innovative, and more will come around that. Those who think it's old and clunky don't fully understand it's power.</p> <p>So I'm glad that as well as developing a Watson Work Services Java SDK I also developed a starter REST servlet for Domino using OpenNTF Domino API, both focussed on strong documentation. (I know the rest of OpenNTF Domino API is not as strongly documented, just this area, for now.)</p> <p>Documentation will be key for the next year. So I want people to look at the documentation and feed back where it's not clear, needs improvement, or just could be improved. I want those to be templates that allow me and others to smooth a scary path for ourselves and others over the coming months, where we are willing (as Jason Roy Gary's anecdote proved) to taste the pufferfish, to take risks, and reap the benefits.</p>","tags":["Conferences","IBM Connect","Java","GraphQL"]},{"location":"blog/2017/04/26/my-dev-tools/","title":"My Development Tools - Part One: Domino and XPages","text":"<p>I've recently had a new laptop. Since I last had an upgrade of hardware a lot has changed. Back then, I think my development tooling was Domino Designer, a Domino server, and possibly SourceTree. Now the software I needed to install was much more significant. So now is a good time to cover that.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#domino-designer-domino-server","title":"Domino Designer, Domino server","text":"<p>I'm still a Domino developer, so Domino Designer was the first application development software installed.</p> <p>The bulk of that development is around XPages applications now. There are a few Notes Client applications or traditional Domino web (only) applications, but for a number of years now any new applications have been XPages. New aspects of traditional Domino web applications have been done in XPages too, resulting in a number of \"hybrid\" Domino web applications, with older areas using traditional Forms-based access but any newer areas using XPages. And where significant development has been done on Notes Client applications, the focus has been on providing functionality via XPages, so Notes Client access has been predominantly for administrative access only.</p> <p>Moreover, since 2011, every XPages application has included at the very least the Extension Library and often other plugins like Debug Toolbar or XPages OpenLog Logger. Over the last few years, every application I've used XPages in has also used OpenNTF Domino API.</p> <p>This use of plugins means Domino Designer local preview is inadequate as a rendering engine. That means I also have a local Domino server installed.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#plugins-open-source-extensions","title":"Plugins / Open Source Extensions","text":"<p>It's beyond the scope of this blog post to cover my passion for open source and why I think everyone who wants a career in application development should get involved with it. But that means there are a lot of extensions to Domino Designer or my Domino server that I install automatically (and which I cover in my training courses). Here is the list I currently use:  </p> <ul> <li>OpenLog: I believe it's a no-brainer for best practice Domino logging. That belief is backed up by experience of where I've inherited applications where developers have tried to use \"supported\" alternatives, making errors in implementation, incorrectly copy/pasting, not returning adequate information for support etc. With OpenLog I always quote the example I had some years ago where an error started occurring because of a Notes Client bug introduced in a fix pack. Without OpenLog I would have spent a long time trying to work out the cause, because I had no reason to believe the customer had deployed that fix pack version and the end user would probably not have realised. OpenLog immediately identified the version they were using, giving absolute concrete proof that corresponded exactly with an IBM technote. No other Domino logging mechanism that I've encountered, in-built or proprietary, would have provided me with that information.  </li> <li>XPages OpenLog Logger / ODA: OpenLog itself has libraries for use in LotusScript or Java agents / web services. But for XPages the code in XPages OpenLog Logger (subsequently incorporated into ODA) is my de facto integration point between XPages applications and OpenLog. That partly started because an OpenLog implementation in TaskJam was not Apache-licensed, so couldn't be incorporated in Apache-licensed open source solutions. But thanks to knowledge I had gained and improved by feature requests, it has gone way beyond just an implementation of the Java Script Library available with OpenLog itself.  </li> <li>Extension Library: The extension library was a no-brainer for me from when it was first released. Anyone involved since the early days of XPages will have read blog posts about various hacks for using Dojo dialogs within XPages. The issue arose not because of a failure of XPages, but because of an inevitable conflict that will always occur when trying to combine two independently developed frameworks. Extension Library immediately solved the problem. But anyone who stuck only with \"officially supported\" releases has really been handicapping themselves, missing out on things like Bootstrap and enhancements I've contributed back for Value Pickers and Name Pickers. I've always used Extension Library from OpenNTF, never just the core. Feature Pack 8 and the focus on a continuous delivery model now mean if companies keep their servers up-to-date their will be less of a gap, but really there's little point not deploying the Extension Library. If you happen to hit a bug, you'll be waiting longer for a fix in the \"officially supported\" version than if you use the open source version. And if support is the argument against using it, plenty of business partners will provide support.  </li> <li>Debug Toolbar: Mark Leusink's Debug Toolbar is another no-brainer. It provides insight into the current XPage that's not easily available elsewhere. I've not seen many custom implementations since Debug Toolbar was released, but I've never seen a better one. Plus, for those wanting to use it as a learning resource, it's a good demonstration of a tool that started as a custom control and has morphed into an OSGi plugin.  </li> <li>XPages Log Reader: For reading log and configuration files, XPages Log Reader is a great interface, reducing the need for direct access to the server itself.  </li> <li>ODA: OpenNTF Domino API is a no-brainer too for me. Prior to using that, infinite loops were a regular inevitable outcome. Java developers can take advantage of setting up templates (using core Eclipse functionality) to mitigate, but if you're not using ODA, you're probably not aware of how to do that. Since ODA I can remember only once having an infinite loop since, and that was not in Domino-specific code. Recycling is another minefield. If you know how to recycle correctly and efficiently, chances are you've since embraced ODA. And that's without the host of enhancements to Domino functionality that have come in since early versions. The ODA channel on OpenNTF Slack currently has 74 members and that's by no means covering all developers. It's a key library.  </li> <li>POI4XPages: This is another useful library, though not one I've used heavily. My integration with Apache POI has tended to be more customised, so I've just added the relevant jar files to my application.</li> <li>XPages Toolbox: This is always useful for performance tuning.</li> <li>Swiper: Swiper is another no-brainer as far as I'm concerned, if you're using source control. Enhancements for FP8 make it even more useful.</li> </ul>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#source-control","title":"Source Control","text":"<p>Talking of Swiper leads perfectly into covering source control. Source control is still problematic with Notes Client or traditional Domino web applications, because of limitations with DXL round-tripping. Recent work done for Panagenda's ApplicationInsights may address some areas I raised at IBM Connect some years ago, we'll have to see when that comes into a future feature pack. But beyond Notes Client or traditional Domino web, it's a no-brainer.</p> <p>I committed 100% to it after making the mistake of accidentally deploying changes made for a POC change request for a customer. At the time I had no reason not to expect the change request to go ahead, so the changes were made in the template to speed up actual development. But months passed and the approval on the change had still not happened. Some months after the change request was submitted a fix was required. I'd forgotten I had made the POC changes and they accidentally got deployed.</p> <p>Since then, source control has been integral to my development and that kind of POC is done in a feature branch. If the feature doesn't go ahead, it remains a dead branch. But its still there if it is needed in the future. Plus it also is useful as an aide memoire if similar functionality is required for another project, and I've had that occur.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#sourcetree","title":"SourceTree","text":"<p>For source control, the tool generally used by developers I'm aware of is Atlassian SourceTree. It's a nice GUI interface which also provides terminal access as well.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#p4merge","title":"P4Merge","text":"<p>Merge conflicts inevitably occur. My tool of choice for managing them is Perforce P4Merge. It nicely compares original, yours and theirs. It also makes intelligent guesses about which to choose. Once the result of the merge is saved, you still have a .orig file which is the original, although I typically delete that.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#git","title":"Git","text":"<p>I still also install Git. Sometimes it's useful to just use a command line prompt, although SourceTree's terminal would also suffice. An example is if you want to move a file or folder. Physically moving it will produce a deleted and new file. The <code>git mv</code> command moves it allowing comparisons against the previous versions in the previous locations. StackOverflow regularly gives answers on how to do such functionality (changing a tag or branch name is another), but answers typically give the raw git commands. Being able to issue those get around working out how to do it in SourceTree, if SourceTree actually exposes that functionality. After all, it doesn't reproduce all git functionality, and nor should it: that's what the terminal access is for, for those kinds of advanced functions.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/04/26/my-dev-tools/#pageant-ssh","title":"Pageant / ssh","text":"<p>If you're using SourceTree, you'll probably use SSH integration. Setting up the SSL private and public keys can be done from SourceTree itself or by installing ssh. But it requires a SSH authentication agent program running. The SourceTree help mentions Pageant for a Windows environment and I've always used that for PuTTy authentication.</p> <p>That covers software specifically for XPages development and source control. Next I'll cover the host of software used for work around and beyond Domino.</p>","tags":["Dev Tools","Editorial","Git"]},{"location":"blog/2017/05/26/my-dev-tools-2/","title":"My Development Tools - Part Two: Beyond Domino","text":"<p>Following on from my last blog post it's now time to move on beyond the tooling related to Domino and XPages.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#eclipse","title":"Eclipse","text":"<p>My preferred IDE for OSGi plugin development is Eclipse. Although I'm aware IntelliJ has become very popular, for me it makes sense to minimise the number of tools like it also makes sense to minimise the number of frameworks. So I stick to Eclipse. But although I started off using it for OSGi plugin development, it's now used for a lot more:  </p> <ul> <li>OSGi plugins  </li> <li>Vaadin  </li> <li>Darwino  </li> <li>Setting up and managing local development Websphere Liberty servers So it makes sense at this time for me to stick to Eclipse.</li> </ul> <p>That also means a number of plugins installed into Eclipse:</p> <ul> <li>XPages SDK  </li> <li>Eclipse Color Theme (I've installed a version also into Domino Designer)  </li> <li>Vaadin  </li> <li>Various Maven and Tycho plugins that have automatically been added  </li> <li>Darwino studio and the dependent plugins  </li> </ul> <p>I don't have all the plugins installed in the same Eclipse installation. I tend to have multiple installations. I'm still in the process of migrating them all, but I'll have separate Eclipse installations for:</p> <ul> <li>Domino OSGi development  </li> <li>Vanilla Java development  </li> <li>Vanilla Vaadin development</li> <li>CrossWorlds development with Vaadin  </li> <li>Darwino development That allows me to keep things pretty self-contained, and have Websphere Liberty servers where necessary and Tomcat servers for Darwino.</li> </ul>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#maven","title":"Maven","text":"<p>As well as using Maven embedded in Eclipse, I also have Maven installed separately. Sometimes it's nicer and easier to use the command line interface (although I resisted that for some time!).</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#rest-service-development","title":"REST Service Development","text":"<p>Those who follow my posts on Intec's Blog will know I've recently been involved in developing REST service plugins using OpenNTF Domino API's ODA Starter Servlet which I developed and then extended further as a Maven archetype. But developing the REST service is just one part.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#postman","title":"Postman","text":"<p>For testing a REST service, the Chrome plugin Postman is a very useful tool. It doesn't allow custom HTTP methods, but there are other tools for that. It does allow tests to be exported from and imported into it, which is very useful, particularly when setting up a new laptop!</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#swagger","title":"Swagger","text":"<p>For documentation and testing purposes, the standard tools are Swagger Editor and Swagger UI are standard tools. I've blogged about those on Intec's Blog, and they are easy to install.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#nodejs","title":"NodeJS","text":"<p>If installed outside of Docker, Swagger requires NodeJS. And to be honest, if you're doing development, you're likely to run into something that needs NodeJS sooner or later, so it's worth embracing.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#docker","title":"Docker","text":"<p>But on my new laptop I can run Docker, so I've installed the Swagger Editor using the Docker image, which is much easier. Docker itself is easy to set up and once that's running, two lines of code will start the docker image of Swagger Editor.</p> <p>Again, Docker is another piece of software gaining prominence and before long I'm sure most developers will be running - or packaging - at least one docker image.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#web-services","title":"Web Services","text":"<p>Some older web application development requires web services, and for that I use SoapUI. This seems to be the de facto tool.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#my-blog","title":"My Blog","text":"<p>Although I blog heavily still on Intec's Blog, this blog is on GitHub Pages. When I wanted to set one up, this was free and pretty easy to set up. But as a result, there are a number of tools I use. Of course SourceTree is used to push it up to GitHub for publishing, but you need to create and review the content.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#markdownpad-2","title":"MarkdownPad 2","text":"<p>The blog posts etc are edited in markdown. Anyone who's used GitHub or set up a project on OpenNTF will be aware of markdown. It's a lightweight language for formatted text and makes a lot of sense for me as a replacement for \"rich text\" content in XPages. Yes, it's not as intuitive and doesn't have a nice editor for the web unlike something like the CK Editor. But I've come to believe that a web application should minimise contain minimal rich content like that, keeping such content in attachments.</p> <p>But for editing .md files on your PC, I've found MarkdownPad 2 very nice. With Windows 10 there's a bit of a workaround needed to get the Live Preview working, requiring installing Awesomium but that was very straightforward.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#jekyll","title":"Jekyll","text":"<p>It's important to see what a post looks like before submitted. For that Jekyll is the choice recommended to me by Eric McCormick (it's what GitHub pages uses anyway). That means installing Ruby and Ruby Gems. That then lets you run the ruby commands necessary to install jekyll and bundler. Then I can go my blog folder in a command line prompt and run <code>bundle exec jekyll serve</code> and see my blog locally.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#cmder","title":"Cmder","text":"<p>Following advice from Oliver Busse, instead of using the in-built Windows tooling for running from the command line, I installed Cmder. It is a significantly more powerful tool, and well worth using.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/05/26/my-dev-tools-2/#summary","title":"Summary","text":"<p>I'm sure my toolset will only increase as time goes by. But it's significant how much it has changed during the last five years. The tools developers use are as much of a learning and training investment as the language and framework developers use. Developers and companies should focus as much on tooling as a requirement for jobs and as a focus for professional development. Many of these tools may be relatively intuitive for basic use, but some like Postman and Docker will require more than just basic skills to maximise their use.</p>","tags":["Dev Tools","REST Clients","Docker","Editorial"]},{"location":"blog/2017/08/17/developing-for-performance/","title":"Developing for Performance","text":"<p>One of the themes which crops up from time to time in Domino application development and beyond is the theme of \"performance\". It's a topic which makes me grit my teeth because of the basic premise. Most articles start from what, for me, is a narrow terms of reference: performance optimisation being about how quick a specific amount of data can be served to a specific environment. My interpretation of the term \"performance\" is much wider and that is the reason for my scepticism and concern of how people may interpret or use such articles. There are important points raised, but there are also caveats that need to be borne in mind.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#data-volume-analysis","title":"Data Volume Analysis","text":"<p>The articles often focus on sending large numbers of entries. There are cases where that might be required, but a good business analyst should question the necessity and expectation of that. We're talking applications, not \"War and Peace\". I would question whether any user, regardless of what they claim, will actually scroll through a screen that has five thousand rows of data. And even it's an application to allow people to read \"War and Peace\", no end user would want to wait for the whole book to load or scroll down until they find where they left off.</p> <p>Chunking into reasonable amounts to allow the user to quickly and effectively access what they need to will optimise user experience, and so increase the user's performance. In terms of editable forms, a wizard-style approach can be more effective and can even appear to display as a tabbed table.</p> <p>A common technique on websites now is post-processing of the DOM, but it's one that I often find annoying as an end user when it's not done well. Then intention is to allow the end user to get started on whatever they want to do while everything else is loading. But as an end user, I've often identified the link I want to go onto before the page has loaded. The link is either not active or the click deferred until the post-processing has finished, at which point content has been re-ordered and the click activates a different link. Or I start scrolling down to read content but post-processing freezes the page, or the contents jump around, so my ability to read effectively is compromised. Occasionally I come across a site where an image placeholder is initially loaded that is the same size as the final image that will be loaded. This is post-processing done well, with careful forethought for the use cases of what is to be done while post-processing occurs. Unfortunately, this is the exception rather than the rule.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#data-size-analysis","title":"Data Size Analysis","text":"<p>But let's be clear, I'm not saying articles about improving the speed of loading aren't useful. One of the aspects often highlighted is the html and data performance, amount of data per document or per operation. Different frameworks or platforms will handle that differently in their basic use. But any framework or platform can usually be optimised with custom coding or different components in the overall stack. Custom DOM manipulation will typically be possible, but may be overkill for some applications.</p> <p>REST services have become de rigueur for data transfer to JavaScript applications. But a REST service is provider driven. This means unless you're using a custom REST service design specifically for your use case, you may be receiving more data than your application actually needs, data that's not in the format you require so needs additional browser-based manipulation, or the data you need may require multiple REST service calls. This is why GraphQL is gaining in prominence, because it delivers only the data the consumer wants, typically allowing data at multiple hierarchies in the database architecture to be delivered in a single call. It also handles versioning better than a standard REST service, by providing the flexibility for changes independently of the consumer. But it takes longer to set up - it has a negative impact on the time taken to develop the REST services, the coding performance. If performance of calls for data is an issue and you're not using custom REST services, GraphQL will probably bring benefits for data transfer performance. Also, if you're just consuming the data, not providing it, you have little control.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#hardware-software-connectivity-device-performance","title":"Hardware / Software / Connectivity / Device Performance","text":"<p>But who always serves a large amount of data always across a poorly performing network, always to a poorly resourced device?</p> <p>This question alone highlights that there are bigger factors to performance that just the code that's running.</p> <p>If the customer demands an intensive application and some end users are using your application on an under-powered device with lack of available memory, device performance will still be slow, regardless of the effort you put in. The developer and even the business owner may not anticipate such issues. The question of who is responsible is a moot one - the end user needs to use a reasonable device, the business owner's requirements need to take into account the end users, otherwise the developer is stymied. If a small percentage of end users' connectivity is significantly sub-standard compared to other users, is this a factor those users should accept responsibility for themselves?</p> <p>The variety of potential factors is considerable, it cannot - and should not - be included in such discussions on performance. But nor should it be ignored by anyone reading the article. In an enterprise environment with data and application logic in the same location as the client using it, with good hardware and software, and small data loads, the importance of coding for optimal code performance is different.</p> <p>That's not saying you should code it badly. But when there is cost involved - financial or time - if it's a small application with a few users on good hardware communicating across a strong network, is it a good use of those finances for you to eke out every last millisecond when what you're gaining is barely perceptibly performance.</p> <p>But questions need to be raised about an internal enterprise application being used on old browsers across a poorly performing network installed on hardware (desktop and server) that are out-dated. Upgrading one or all of those will have a bigger impact than spending time improving a single application.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#frameworks","title":"Frameworks","text":"<p>Most developers will use frameworks. This is not solely for application performance reasons, even if the chosen framework is better at performance than another framework. A custom framework aimed specifically at the application in question will, with a theoretically optimal experience of all platforms and framework code, get better performance. But it's going to require significantly more experience, take longer to develop, be harder to support, be less likely to cover all devices, require more effort to keep future-proofed and require an equally skilled person to replace you when you want to move on or get run over by a bus. And at the end of the day someone needs to pay for the application (even if it's only you with your time). Using a framework is a no-brainer. It improves coding and maintenance performance.</p> <p>But any investment in a framework is just that - an investment. No matter what the providers say, no framework can be effectively picked up and used in an an hour. You may be able to get reasonable use out of it. But if it's anywhere near comprehensive, you'll need to dig a lot deeper to get the most out of it or answer the edge cases any given application may need. At this point, the quality of documentation and availability of source code, or even support, really comes to the fore. If you hit enough problems, the time lost overcoming them may outweigh the time gained by using the framework. In the case of frameworks that get a complete re-write of the framework, that could have a massive impact on a complex application. And typically you'll need to combine in code that's not based on that framework, and that may result in conflicts that need technical skill to work around. The stability, quality and interoperability of the framework you choose could have a big impact on your developer performance for years to come.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#coding-experience","title":"Coding Experience","text":"<p>Blog posts on performance can often highlight alternative approaches to development or introduce different ways of thinking that might not have been considered.  And this is an important point. I know as I've gained years of experience of development environments, the way I've coded applications naturally has improved the performance. That performance improvement has come from investigation and experience of the platform. If I moved to a.n.other platform, I would not have the same degree of understanding that would enable me to optimise. It may be possible to get better performance, but not possible for me from day one to get better performance for the specific use cases of the application I'm trying to build.</p> <p>The level of experience can mean the application a given developer can create may take longer to develop and have worse performance on platform or framework B, even though platform or framework B is statistically better performing. Application or time-to-market performance may vary from developer to developer. Customers need to be willing to pay for what they want.</p> <p>This does not mean the developer should only ever build on platform or framework A. But it's not advisable to choose a migration of a massive application for the first project on platform or framework B. Similarly, there's little point in doing one small project on platform or framework B if there will be no chance to do another for nine months or a year. Timing, planning, training, learning time are critical to expanding horizons. Companies need to invest for what they need.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#migration-performance","title":"Migration Performance","text":"<p>This brings me onto a common topic, where business choose to migrate between technologies or frameworks. Let's be clear here - performance of end users will suffer. If you migrate the application without the data, users have to spend time re-keying data. If there was integration with external systems, that needs re-developing or can often cease to happen. If you migrate data, end users should clean the data between export and import. If not, you will have problems. And data cleaning is not a quick task, nor is it one that can be fully automated or done without specific business knowledge. If you migrate your developers as well, they need time to learn. The project manager must be willing to ensure the end user is willing to compromise as a result, because of the lack of experience. If you use new developers for a custom application, there is likely to be a loss of business knowledge, which may impact readiness for business of the application. Key questions that address edge cases users have forgotten about or key configuration rarely updated will be missed. Using an off-the-shelf application will need compromises, which may require changes in business processes. And all of the requirements will require business input into the project, time that's rarely budgeted in advance in my experience.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2017/08/17/developing-for-performance/#the-right-performance","title":"The Right Performance","text":"<p>At the end of the day, the effort required to ensure performance is within reasonable bounds for an application will vary from project to project. Consequently the degree to which the application should be optimised for performance and the methods used will vary from situation to situation. The will not be - nor should there be - a \"one size fits all\" model. If there is, the majority of applications will be over-complicated and the infrastructure will be vastly more expensive than it needs to be. The more information available from those requesting the application at an early stage, the better a developer can code for the right performance. If complete and accurate information is not available for all factors covered above - and I would argue it rarely is and typically should change - the best approach is to tune performance.</p> <p>The developer can make a \"best guess\" at coding and architecture based on the best information available at the time. But everyone needs to take some responsibility: those hosting the application need to ensure wherever it's sitting remains fit for purpose; business owners have to accept that further development may be required to maintain performance; they need to be aware that new APIs and developer knowledge will improve, and leveraging this in existing applications should not be expected free of charge; they need to be aware changing requirements may impact - positively or negatively - the complexity of the application and thus its impact on performance; users have to accept that devices used need to be maintained at the appropriate specification; and developers have to keep apprised of new APIs or functionality that might improve performance and some leveraging of this may be covered if maintenance costs charged.</p> <p>Performance can never be a one-off task. It needs to be something that's regularly reviewed and optimised. The worst case scenario is an application or platform where performance is never reviewed or tuned, data sizing never maintained, performance degrades and the decision is made to migrate under the premise that the application or platform cannot cope. The frustration, cost, effort and pain that arises for all parties is unforgivable. And unless the lesson is learned, the cycle of woe and perpetual migration will continue.</p>","tags":["Editorial","Performance","Support"]},{"location":"blog/2019/09/17/java-outside-domino-eclipse/","title":"Java Outside Domino in Eclipse","text":"<p>Recently I've been diving back into running Java outside of the Domino HTTP stack, picking up some work I did quite a few years ago playing with Vert.x and Domino based on Stephan Wissel's blog series on Vert.x and Domino. Quite a few things have happened since I was last working on the project, not least the laptop I had at the time got rebuilt, I have got a new laptop, several version of Eclipse have been released and XPages SDK has been deployed to the Eclipse Marketplace (thanks Jesse Gallagher).</p> <p>One thing I've learned with modern IDEs is that you don't just install them and go. Even with Domino Designer, if you use XPages and Java to any real extent, there are a host of plugins that need installing, there are a variety of Java preferences you set (and hopefully backup). Beyond Domino Designer, there are a host of plugins or extensions that need to be installed, configuration of things like Java JREs, related software, Maven repositories etc. It's not a five minute job to just get up and running. And you will often come across a problem you didn't find last time. The necessity to understand your development environment is crucial.</p> <p>Even when I was developing with Vert.x last time, there were a lot of aspects I documented. But there were a couple of new issues I encountered, with rather vague error messages.</p>","tags":["Domino","Java","Vert.x"]},{"location":"blog/2019/09/17/java-outside-domino-eclipse/#problem-1","title":"Problem 1","text":"<p>The first was one Stephan covers in his blog series on Vert.x and one that's covered a lot elsewhere when trying to run Java code that interacts with Domino but is external to Domino: <code>SEVERE: java.lang.UnsatisfiedLinkError: no lsxbe in java.library.path</code>. Running the Vert.x verticle from Eclipse ran fine, but as soon as I tried to hit the associated web page, it threw this error.</p> <p>This usually means not being able to find the Notes / Domino runtime when running. This was confusing, because I had jars in the Project classpath. I also had old code adding it to my local Maven repository. I tried setting environment variables in and out of Eclipse, trying to add various settings to the Run Configuration, but to no avail. Importing the project into a separate Eclipse that was set up to use the Domino JRE worked fine. So it was clearly something to do with the classpath settings for the Run Configuration.</p> <p>This was also where source control came in very useful, even for a project with just myself as the developer. It became much easier to roll everything back to the starting point - not just once, but twice!</p> <p>It was then I looked at the classpath in detail and found several references, not only to the Domino jars but also the Notes Client Notes jars. I eventually tracked that down to settings in the global JRE, set up for another project to point to Notes in addition to the AdoptOpenJDK JRE. Removing them to ensure there was only a single set of the Domino JARs available at runtime solved that problem.</p>","tags":["Domino","Java","Vert.x"]},{"location":"blog/2019/09/17/java-outside-domino-eclipse/#problem-2","title":"Problem 2","text":"<p>That then left a second problem: <code>Error writing to process file pid.nbf</code>.</p> <p>Again, it's a problem mentioned in various places on the internet and in IBM technotes. In many cases, reading the analysis I was not confident the true cause of the problem had been properly diagnosed and the solution seemed more \"this was tried and it worked\", without properly clarifying why it worked. I also had that niggling feeling I'd hit this problem before, but couldn't remember the cause.</p> <p>I eventually found a technote that seemed to talk about permissions of the running user (on Linux, I believe). This helped me pinpoint the cause. I had installed Domino on Windows in <code>C:\\Program Files</code>. Even though I was running the code from Eclipse, I wondered if it was not necessarily running \"as me\". Sure enough, setting permissions to the Domino folder and all subfolders solved the problem.</p> <p>Problems solved, code running, and a deeper understanding gained of what's required.</p>","tags":["Domino","Java","Vert.x"]},{"location":"blog/2019/10/01/danger-mid-code-pro-code/","title":"Danger of Mid Code to Pro Code","text":"<p>People discussing Domino application development have been using a new term since early this year - \"mid code\". This has become necessary because of the evolution of Domino development since Domino V10.</p> <ul> <li>\"low code\" is a drag-and-drop web IDE being provided by HCL Leap support for HCL Domino V11, aimed at business users. The output will be a web application.</li> <li>\"pro code\" relates to JavaScript and Java developers, developing applications from scratch or taking the output of those low code applications and extending them. Again, this is intended for web applications or server-based processes.</li> <li>\"mid code\" has become a term for development for Notes Client or HCL Nomad, the mobile client. It has also been the term for XPages development. XPages is a web framework based on JSF but developed via XML markup, which a builder converts to Java classes. As with other Java web frameworks, JavaScript can be embedded in it. Server-side coding is via Java classes or \"Server-Side JavaScript\", a pseudo-language which the editor validates, the builder stores as strings, and the runtime parses as calls to Java methods.</li> </ul> <p>Although some individuals may have the ability to progress from one level to another, some will not. So it's important to differentiate the different levels and ensure no assumptions or expectations for people to cover multiple levels. The key to moving from one level up to another is understanding what complexity is being abstracted for you by the frameworks.</p> <p>When you forget what a framework does for you and move to a different level, it's easy to think something should work when it clearly won't when you step back and realise what is required. And that's something I've just found myself guilty of, losing some hours in development as a result.</p> <p>The scenario is posting ATOM to a web API. As an XPages developer for nearly ten years, I've been writing XML for a long time. But for this, it was a case of manually writing XML as a string in JavaScript to post to the web. The XPages IDE is an XML editor and Properties editor. The Properties editor converts what's entered into XML in the XML editor. The XML editor validates at build time, to ensure it's valid XML. And the key here is that the Properties editor conversion and the XML editor validation ensures valid XML. Manually writing XML as a string in JavaScript in VS Code didn't do that. And the receiving web API just ignored whatever was invalid XML in my string. So I was posting HTML, something like \"\\My title\\ is some text\" and defining the XML object's contentType as HTML, the HTML tags were just getting ignored.  The result was not an error, but a partial value posted, just \"is some text\". Finally I realised that ATOM is XML and XML requires escaping invalid characters. Problem finally solved and lesson learned.</p>","tags":["Atom","Coding"]},{"location":"blog/2019/10/10/error-management/","title":"Error Management","text":"<p>Over the years I've done a lot of development on a variety of platforms. Error management is something developers either bake in from the start, add in later, or never get round to! It seems a good time to review my experiences and my philosophy.</p> <p>tl;dr - it varies.</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#why-should-i-manage-errors","title":"Why Should I Manage Errors?","text":"<p>I think this is often a question skipped by developers, but it's fundamental to whether and how you manage errors. And at this point it's important to differentiate between error management and validation. The two can overlap, because insufficient or incorrect validation can result in errors, which need managing. Validation can be minimal or exhaustive, but there's no silver bullet for where, how and to what extent validation is added. But we'll skip that part and assume validation is handled and sufficient.</p> <p>The purpose of error management, in my opinion, is to aid development and support. That means:</p> <ol> <li>Providing quick troubleshooting to ensure valid and correct code during the development process.</li> <li>Avoiding support calls by notifying the user of actions they or others should already have taken.</li> <li>Quickly identifying the location of an error when support calls are necessary.</li> </ol> <p>Why an error occurs may be trickier. It may depend on data or variables beyond the code itself. Further debugging or logging may be required to pinpoint that. But it should take the minimal time possible to identify the location of an error and the route taken through your code to reach there. Using debugging to pinpoint the location of an error during development is, in my opinion, not productive. Stepping through a debugger is always slower than looking at a log. And although you can debug during development, it's harder for you and your users to do that in production. So you're either backing your skill to identify all bugs before deployment or risking problems down the line for support.</p> <p>Of course there are short standalone blocks of code or functions for quick fixup that you have no doubts about or could only fail under extremely unlikely scenarios. I personally have no qualms about skipping error management for very low risk scenarios, especially because I'm usually the one supporting the code I develop.</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#error-burial-ostriching","title":"Error Burial / Ostriching","text":"<p>That prompts a bugbear of mine, something I call \"error burial\" or \"ostriching\". In LotusScript, that could be <code>On Error Resume Next</code> - basically just continue as if nothing had gone wrong. JavaScript often just hides errors away in the browser's Console, again just pretending for the user that nothing had gone wrong. I've supported environments where IT have locked down the browser so it's not possible to see if an error has occurred, which makes it extremely challenging to support. In both scenarios, you're left in blissful ignorance of an error. And if it becomes apparent there's a serious issue, you're often left guessing what went wrong.</p> <p>There's an unmanageable scenario that can also result in error burial - a crash. Crashes may be the result of code or external circumstances. Managing those proactively is typically not possible, and in those circumstances I'm reminded of the sporting maxim \"control the controllables\". If it's not controllable, accept it and deal with it retrospectively.</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#error-logging","title":"Error Logging","text":"<p>Many frameworks log errors at varying criticalities to one or more logs. This can be useful for support purposes, identifying where the error occurred, maybe with additional some finer logging (either always or switched on via configuration) to help identify variables. Logging allows quick support of a problem, both in production and during development. But in itself, it's not managing the error.</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#error-notification","title":"Error Notification","text":"<p>It doesn't tell the user - or support - that an error has occurred. It only allows you to deal with the aftermath when - or if - you're notified. Because logging handles errors as well as \"events\", not everything needs notification. Similarly, if your process \"expects\" or manages failures, you might not be interested in notification. If the process retries shortly afterwards, the \"error\" only becomes a problem if it persists.</p> <p>Similarly, the notification to a user may be different to the notification for a developer or support. A more user-friendly message may be required for users. If it's a failure to perform some broader configuration by the user, there may be no need to notify support. During development time, you might just want something more blunt, to let you just get on with fixing the error. And depending on the circumstances, notifying support may need its own management - e.g. in an application that may be used online or offline.</p> <p>Whatever the process, the error notification will typically be handled - and need to be set up - separately from the error logging.</p> <p>And I won't go down the rabbit hole of error management of the error notification! ;-)</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#what-next","title":"What Next?","text":"<p>This is the true part of error management.</p> <p>Logging or reporting the error is not enough. What should be the next action? Does the code need to abort? Is the code within a loop and needs to continue with the next element of the loop? Are there subsequent actions that need to be prevented? Are there previous transactions that need rolling back? Are there additional data updates that need to be made in order to prevent other independent processes failing?</p> <p>Obviously there is no single answer for all eventualities. And the correct action may require changes to the architecture of your code. Moreover, additional error management may be required elsewhere in your code, to deal with the failure. And in one of the locations you may need to suppress error logging to avoid duplication. Even more complex is working with or developing an API, because two independent error management processes are needed. Typically no one developer will be in control of both.</p> <p>The \"what next\" question is why ostriching can be so bad. If you ignore the error, you are abrogating your responsibility for managing the impacts.</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/10/error-management/#summary","title":"Summary","text":"<p>As you can see, error management is a more complex topic than it may seem. That may be why developers skip addressing it. It's also an area that a lot of tutorials skip, leaping straight into a \"hello world\" application. But that ignores the one constant of working with new technologies - mistakes will be made, and making it easy to resolve them is key to enjoying working with the technology. Quick and easy is only quick and easy if you get it right first time.</p> <p>Good error management is critical to a quality and easy-to-manage product. It gives reassurance to users and provides a core foundation for effective support. Ostriching may make it seem like everything's working fine, but you can't fix what you don't know. And fixing it when something more critical comes along may be a lot harder. If you have a good logging framework available before development starts, it's a strong starting point. If you have an IDE that minimises effort by boilerplating error management, that's also a strong starting point - as long as you make the time to configure it as required. The \"what next\" can be addressed later in development. And getting the logging in from the start can speed up development, giving you the time you need at a later stage to ponder the \"what next\".</p>","tags":["Coding","Errors","Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/","title":"Thoughts on Troubleshooting Support","text":"<p>Over the years I've spent a lot of time supporting applications. I would like to think I'm pretty effective at it. So I thought it was the right time to share my approaches. The key is a systematic, logical approach to identify the cause - or causes.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#information-gathering","title":"Information Gathering","text":"<p>The key to solving any problem, IT or other, is information and successfully sifting the relevant and irrelevant information. They say a picture paints a thousand words, and a good screenshot certainly does that:</p> <ul> <li>It cuts through incorrect or misleading use of terminology.</li> <li>It will typically identify which application and where.</li> <li>It will hopefully identify the document being edited at the time, which may be critical.</li> <li>If there's an error message displayed to the user, it gives you that information.</li> <li>Depending on the application, the screenshot may also give information of the specific user profile and permissions in play at the time. Although not typically relevant, on occasion it can be.</li> </ul> <p>It is a snapshot in time though, a snapshot after the error or problem has occurred. So it doesn't necessarily identify what happened immediately prior to the screenshot. Hopefully with a good knowledge of the application the support technician will be able to work that out, but obviously that's also critical.</p> <p>Where a screenshot is not available, clear and full details of what was done, what was the expected outcome and what was the actual outcome. Understanding the technical ability of the person who reported the issue will have an impact. Assumptions will have to be made, and symptoms - all symptoms - teased out. If you know the person you're dealing with, that will colour what questions are asked, in which way and whether there is going to need to be additional clarification.</p> <p>Some of that clarification may need to be asking for logs. But this comes back to filtering relevant and irrelevant information. You need to be able to answer the following questions, or additional logs just means additional misleading noise:</p> <ul> <li>What should you be looking for in the logs?</li> <li>What is normal processing in the logs?</li> <li>Can you recognise and filter out irrelevant error messages?</li> <li>Is anything relevant even being written to the logs?</li> </ul> <p>Context is also key</p> <ul> <li>Is it working for someone else, on a different PC, a different site?</li> <li>Has it worked before and when did it stop working?</li> <li>Has the problem previously been reported?</li> <li>Was scheduled maintenance occurring at the time?</li> </ul>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#time-travelling","title":"Time Travelling","text":"<p>If it's a workflow system, when it comes to looking at the data, you need to bear in mind what the status of the data was at the time the problem occurred, and how it's changed since.</p> <p>With Domino, the Seq Num property of fields helps identify this.</p> <p>With local replicas or applications on multiple servers, you also need to think about what the status of the data was on each server at the relevant time.</p> <p>Thinking in this \"fourth dimensional\" way takes some lateral thinking, but can sometimes be key and is always crucial to being certain about what happened when.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#simple-solutions","title":"Simple Solutions","text":"<p>From all of this, it may be immediately obvious what the cause and resolution is. If there is an error message with a stack trace, that can sometimes solve the support call. It may be that there is a misalignment between the expected outcome and the actual outcome. By this, I mean that the user is expecting a certain outcome but, because of certain parameters or environment circumstances, the code is resulting in a different outcome, but one which is correct for normal processing.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#reproducing","title":"Reproducing","text":"<p>If it's feasible and if the relevant information has been provided, it may be possible to reproduce the problem. Reproducing the issue may identify the cause, but at the very least will allow more detailed troubleshooting.</p> <p>But it's worth bearing in mind what is different between the attempt to reproduce and the original issue. There may be environmental or data differences, or you may not have enough detail to reproduce exactly. There is always going to be one particular difference, and that is timing. Trying to reproduce it at another time may not make a difference, but it's worth bearing in mind.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#whats-changed","title":"What's Changed?","text":"<p>But, if not, those questions will have fed into a key part of problem solving: what has changed? Because for a problem to have occurred, something has changed.</p> <ul> <li>You may be installing something new</li> <li>It may be the application's code that has changed and this changed code has introduced an issue.</li> <li>There could have been configuration changes in the user permissions.</li> <li>A different environment, platform or hardware can be the relevant \"what's changed\".</li> <li>The most common difference is data-related or specific parameters that were passed.</li> </ul>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#forming-hypotheses","title":"Forming Hypotheses","text":"<p>If it's not a simple solution, then based upon the information available, an analysis of the data, identifying what's changed, you need to come up with hypotheses. Note, I'm specifically using the plural here, because you should always come up with multiple hypotheses. One is rarely enough if you've had to get this far.</p> <p>There is a process I always take here, almost subconsciously. For each hypothesis:</p> <ul> <li>How can I prove the hypothesis?</li> <li>How can I disprove the hypothesis? Ruling certain hypotheses out is often as useful as proving it.</li> <li>If a hypothesis is right, what other symptoms would I expect to see?</li> <li>And what symptoms would you not expect to see?</li> </ul> <p>If you're thinking about what other symptoms you should see and should not see, you're identifying definitive ways to prove and disprove the hypothesis. If you can't come up with other expected symptoms, you're not in a position to 100% confirm the hypothesis. If you can anticipate symptoms, you're also showing you understand what should or shouldn't happen. If you're just saying \"Maybe it's X, maybe it's Y, maybe it's Z\", throwing possible causes around without a methodology for proving or disproving them, you're basically hoping you get lucky.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#prioritising-hypotheses","title":"Prioritising Hypotheses","text":"<p>So now you have a set of hypotheses, but it's not yet time to dive into them. You need to think about two thing:</p> <ul> <li>The probability of each.</li> <li>The ease of proving or disproving.</li> </ul> <p>This is key to the order you attack them. There may be some that have low probability but which you can quickly disprove. It may be worth taking a few minutes to try that. Because if you fail to disprove it, even though it's less probable, you may have found the cause quicker. The most probable may be hard to prove or disprove, and you may need a lot more information to do so. And if you wait for that information and the hypothesis is disproved, things may have happened to the data that make it harder to prove or disprove other hypotheses.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#identifying-the-right-causeor-causes","title":"Identifying the Right Cause...Or Causes!","text":"<p>This is why gathering enough information and thinking about symptoms is crucial. If there were symptoms you expect that were not reported, it may be worth querying to see if those symptoms occurred but were not identified or not reported.</p> <p>On the other hand, a hypothesis may not answer all the symptoms reported. It's possible that the other symptoms reported were incorrectly identified as relevant by the user. But it may be that you haven't identified the right cause or all the causes. I can be somewhat relentless if my hypothesis doesn't fit all symptoms, but typically I've missed something, either in my understanding or in identifying the true cause.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#fixing-the-problem","title":"Fixing The Problem","text":"<p>Similarly, fixing the problem also needs some careful thinking. There could be previously unreported or incorrectly reported instances of the same problem. A certain combination of clean data and clearer reporting may explain a problem that you've failed to fully diagnose previously.</p> <p>But there may also be knock-on impacts of the problem, impacts you need to address. And there may be specific actions that need to be taken to ensure no additional knock-on issues arise.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/28/troubleshooting-support/#summary","title":"Summary","text":"<p>A nice resolution to a support issue is one that has precise reporting of symptoms, symptoms that neatly fit a hypothesis or hypotheses, that give a definitive cause or causes. The resolution should clean up the data, ensure no knock-on issues, and prevent any additional instances of the problem.</p>","tags":["Support","Editorial"]},{"location":"blog/2019/10/29/pastures-new-new-challenges/","title":"Pastures New, New Challenges","text":"<p>After 14 and a half years at Intec, I'm moving on to pastures new. I've learned a lot and developed in ways that were never envisaged by anyone when I started at Intec. I'm very fortunate to have had a company that backed me, and I think we've both benefited greatly. But I've always sought to embrace opportunities and accept responsibilities throughout my career. And now is no different.</p> <p>It was always going to take something momentous for me to leave. After all, I was at my previous company - the only other company where I've had a permanent job - for over five years. I'm not one to leap from one job to another. My career is like my development, taking the time to properly understand what I'm doing, committing to it beyond just the short-term, which hopefully enables me to do it well. I've always been willing to step outside my comfort zone, not straying too far, so I can leverage both my existing knowledge and all the skills I've gained in all aspects of life. So it's not a huge surprise that I'm not moving away from the platform that has been the core of my career - Domino.</p> <p>I'm moving to HCL, working with Michael Alexander, Stephan Wissel and co on innovation around the HCL Digital Solutions products.</p> <p>It's a significant move, a little scary, and one which will challenge me in new and different ways. But it will allow me to be involved in the future of our products in a way that was not possible at a Business Partner or Customer. Now is an exciting but important time for the products that I love, products I started my career with and products I'd like to end my career with too. Whether vendors, business partners, customers, technicians or users - it's a key time to modernise, revitalise and grow all the products.</p> <p>I'm conscious that the IT world has evolved significantly over the last decade. The demands for software, both in terms of architecture and user interface, have changed massively. But I believe it's very true that those who are not mindful of the past are doomed to repeat its mistakes. Equally, I believe that staying a slave to \"the way we've always done it\" is also a recipe for disaster. To attract new audiences, I think it's important to be aware of how they work. But there's also a need to bring existing people on that journey. It's a challenge, requiring a need to look at the current and future landscapes from a wide variety of aspects, not only in broad strokes but also in detail, all at the same time. And across all of this, any attempt to please everyone is doomed. The key is a vision and an approach that will grow the platforms and keep them vibrant for the next 20+ years.</p> <p>There is an elephant in the room here: XPages. I'm not moving to HCL to lead an XPages team. I'm not moving with a specific vision to re-energise XPages. I've never sought to hide that there are no easy answers for XPages. I've also frequently blogged that I believe Domino applications are why the platform is still used, not XPages.</p> <p>I doubt I was employed for my XPages knowledge, but for the knowledge I gained by going through and beyond XPages - Java, servlet knowledge, deep understanding of Domino objects via ODA, CrossWorlds, OsgiWorlds, OpenLiberty, SpringBoot, Vert.x, Node.js, React, JAX-RS, OpenAPI, GraphQL, Node-RED, Docker and more. These are the skills I'm expecting to be drawing on, as well as diving into a host of additional technologies, integrating them with the deep knowledge of Domino I've gained over the years, not only the limitations but more importantly the strengths. I'm grateful to many in the community who have been patient and helped me learn. I wouldn't be in a position to take this step otherwise.</p> <p>Although I'll still be part of the OpenNTF board and will still be blogging and speaking regularly, this does mean there will be a gap in the community. It's not for me to comment on the size of that gap. Like Stephan Wissel, I expect to still answer some queries on StackOverflow and in Slack channels. But there is a level of XPages understanding that will, over time, be removed from my brain. The good news is that the vast majority of my knowledge, if not everything I know, has already been put out into the public domain. The key parts are understanding the XPages lifecycle, understanding that everything in XPages is Java - all covered in sessions like \"The Eureka Moment\" and \"Marty, You're Not Thinking Fourth Dimensionally\". The remainder comes from wanting to understand, from diving into the source code and debugging - ODA, Extension Library and more. Thankfully there are many who are blogging and speaking already.</p> <p>At various times I've made requests for people to get involved in the open source projects I've worked on. The success has been limited at best. But in the future, I expect my professional and personal inclinations will lead me to involvement in other open source technologies. As I'm less in a position to output code, I expect to adopt a broader remit than development - governance, documentation etc.</p> <p>It's the right time for me to start stepping away and time for others to take up the baton. If there are queries about setting up environments or how things work then, as ever, I will be willing to advise. I'm not going away any time soon.</p> <p>But the longer it takes for people to step forward, the greater the likelihood that related technologies will evolve so that the old answers no longer work. I came across that recently, when trying to add Font Awesome to XPages - the answers I gave on a Stack Overflow question some years ago were now out of date. For this particular topic, however, the standard approach worked - at least when considering an XPages application as a standard web application project rather than an NSF. But as the old answers no longer work, it will require the community to find out for themselves the kinds of things previously learned by the likes of Jesse Gallagher, Nathan Freeman, Tim Tripcony and others - something I do not envisage being an easy task.</p> <p>So it's a time of change, but also a time of opportunity for all. Embracing  opportunity and responsibility has brought me to this point. Hopefully it will take others on a similarly exciting journey.</p>","tags":["Editorial","Community","XPages","Java","Domino"]},{"location":"blog/2019/11/22/project-jigd3w/","title":"Project Jig3dw: Tutorials Re-Imagined","text":"<p>The world of HCL Digital Solutions is evolving rapidly. This brings a lot of excitement but also a lot of challenges. The last certification exams for Domino were in the era of Domino 8.5. Training materials have also languished a lot - official ones from the vendor and from elsewhere, as the ecosystem has contracted. At the same time, the product has diversified and expanded.</p> <p>Would a single \"upgrade exam\" work for a product that covers XPages, Nomad, App Dev Pack, OSGi plugins and more? It's debatable, but probably unlikely. And that's without considering HCL Leap on Domino.</p> <p>Meanwhile, there's a drive to introduce Domino to newer markets. Developers who have no background on Domino will obviously have different requirements. Can a traditional tutorial approach provide enough information for those unfamiliar with the product, while also fitting those with decades of experience?</p> <p>And that also fails to cater for different personalities. There are those who can pick up new technologies and simple 1-2-3 tutorials without an issue. There are others who are adept at finding new and interesting ways to get it wrong, even though they're following the steps faithfully. There are those who just want to speed through the tutorial \"developing by numbers\". There are others who want (maybe \"need\") to understand what they're doing. Traditional tutorial approaches tend to take a \"one size fits all\" approach - you get what you get, and you need to be the one to adapt.</p> <p>And that's before we get into validation / certification to confirm you've followed the tutorial and understood what's done.</p> <p>Project Jig3dw is designed to evolve a proof-of-concept framework for modular tutorials to address all these areas. What there is now is the start of a tutorial on HCL Nomad development, following through building the sample I put together following the 2019 Collabsphere Beauty Contest. It was profoundly disappointing that more in the community didn't get involved - you don't need an iPad to develop for HCL Nomad, as the tutorial will show. The key concepts were outlined previously by Theo Heselmans in his Wine app on OpenNTF and by me in my modernisation session at Engage. And it doesn't take long, as you'll see with the tutorial which will build the app in the related GitHub repo. The screenshots for that were included in a tweet some weeks ago.</p> <p>The tutorial itself is built using GitHub Pages, integrating Bootstrap, thanks to bootstrap-4-github-pages. The theme used is the Bootswatch Cosmo theme. This means it's markdown pages, integrated with Jekyll and Liquid into HTML files. I hasten to mention that this technology is in use for proof-of-concept. I expect it to change, though it's also possible this might be a workable option going forward. Whatever is used needs to be relatively simple with a low barrier of entry, because the framework needs to work for not only developers but administrators, power users and more. You can see some examples of elements in use on the samples page. The key elements specifically introduced are the \"Why?\" and \"Troubleshoot?\" blocks. These are initially hidden, but allow more detailed exposition for those who wish it and troubleshooting to allow followers to jump back and validate they haven't made a mistake on specific areas. This is about richness without overwhelming and ease of use without the burden of support. This is very much the \"3D\" element.</p> <p>This is the tutorial part, but I also have specific ideas about how validation / certification could work.</p> <p>And it's designed for a modular approach. One tutorial could link in to other pre-requisite tutorials (e.g. \"Installing a Domino Server\", \"Administering HCL Nomad\") and link off to other tutorials (e.g. \"Advanced Nomad Development\"). This is envisaged as an approach to minimise the burden on the community while maximising the expertise of HCL Masters and more, and building an expansive \"jigsaw\" of professional development.</p> <p>Whether this is adopted or abandoned, I don't know. Whether it evolves into something bigger, we'll have to see. At this point I'm only throwing it out there to see if it's got legs and can run. It's just one of those ideas that have been forming in my mind for a while, and why I've taken the opportunity to get involved at HCL. But it's something that shouldn't be the responsibility only of the vendor - the community has a wealth of expertise and best practices, but they need to be leveraged in a way that doesn't overwhelm. Because, at the moment, I don't think the proactive people in the community can support the number of consumers without being burned out. And the community can't afford for that to happen.</p>","tags":["Tutorials","Domino","Nomad"]},{"location":"blog/2019/12/04/domino-on-docker/","title":"Domino on Docker - Some Learning Points","text":"<p>About a year ago I did a blog post on Domino on Docker, with the intention to follow it up on developing against that Domino Docker server via Notes Client and integrating with it from other Docker containers or outside of Docker. Unfortunately other things got in the way, and it then got put on hold pending the work Thomas Hampel and Daniel Nashed were doing on the IBM repository for Domino on Docker and Roberto Boccadoro's Domino on Docker guide posted on OpenNTF's wiki guides site.</p> <p>Particularly the IBM repository means that the steps involved in setting up Domino on Docker are very different. A goal for that repository was to auto-manage a lot of the configuration of Domino and some of Daniel's excellent scripts have covered aspects of that. The effort to make a short, brief installation of a Domino server is undermined if developers and administrators - particularly non-Domino technicians - have to then connect with Domino Administrator and configure the server before it's ready for development or learning. Thomas and Daniel have lots of ideas for that and I wanted to get involved, but unfortunately I was not able to find the time.</p> <p>The good news is that Domino on Docker is not really different to any other Domino server. In my video I changed the ports from the standard ones, because I already had a Domino server on my PC and didn't want to conflict. If that's not the case, you can just use the standard ports 80/443 and 1352.</p> <p>However, one thing to bear in mind is that Domino generates certificates that the Notes Client stores and uses to verify the server. It also creates a connection document for the relevant IP address and Domino server name. This means that if you have multiple Domino on Docker servers, all using the same ports, the connection documents will all be pointing to the same place. But two Docker server registered as MyServer1/MyOrg and MyServer2/MyOrg will have different certificates associated. As a result, Notes may generate a warning. That's not a problem for development or training purposes, you just have to realise that's why the warning is being triggered and it's not anything you have to do anything about. Naming them both MyServer1/MyOrg won't help, because even though the Notes name is the same, the certificates will be different. And it's this which is checked to verify authenticity.</p> <p>One very good use case for Domino on Docker is testing multiple versions. If you're storing the data directory locally and re-using it for the different Domino on Docker servers, that's going to be fine as long as you don't upgrade the names.nsf and other core databases. If you do, you may have problems when using a previous Domino Docker version. Alternatively, if you use a Docker volume for the data directory, you can clone it using this useful utility. That means you can have an exact copy of the data directory and notes.ini etc, ready to test and verify functionality. You can be certain that the only difference is the Domino version. This is also a good use case for test servers, deleting and recreating the data volume from a backup clone, so you always have your starting data and databases.</p> <p>As I mentioned, you can alternatively store your data outside of Docker. However, there are differences between different operating systems for the command needed to run.</p> <ul> <li>For Linux and MacOS, <code>-v ${pwd}:/data</code> would map the current working directory the command is running in, mapping it to the \"/data\" folder within the container that gets created. (So for Domino, you want \"/notesdata\". For Node-RED \"/data\").</li> <li>For Windows cmd prompt, you use <code>-v %cd%:/data</code>.</li> <li>For Windows Powershell, you use <code>-v ${PWD}:/data</code>. This took me quite a while and searching a few times to find the right syntax for Windows. On Windows you will need to do the other steps in the Docker documentation to provide Docker with your Windows credentials to interact with your filesystem.</li> </ul> <p>Another bit of advice from personal experience, if you start using Docker a lot you will soon have a number of images, volumes and containers (as I did by the time I was presenting at Engage). I'd recommend keeping documentation of what the images, volumes and containers are, versions in use and what you use them for. It's very easy to get lose track which makes housekeeping more challenging.</p> <p>It's also worth bearing in mind that, if you build images, if an image is deleted the containers using that image lose their name and get a hex string as the name instead. Similarly, when you're building an image, it will build intermediate containers during the process and those intermediate containers are also created with a hex string as the name. So it's very easy to get confused between the two - again a benefit of regular housekeeping and documentation.</p>","tags":["Domino","Docker"]},{"location":"blog/2019/12/17/congrats-hcl-masters/","title":"Congratulations to 2020 HCL Masters","text":"<p>This week is going to be a big week for the HCL Digital Solutions brands and it started late last night with the announcement of the 2020 crop of HCL Mastershttps://www.cwpcollaboration.com/class_of_2020.html. Congratulations to all who were selected, it continues HCL's commitment to the community and those who go above and beyond to bring benefits to all who use the products.</p> <p>As always, the award is a recognition of efforts done throughout 2019, so enjoy the glory but continue the planning for bigger and better things next year.</p> <p>For those who were nominated but were unsuccessful, congratulations on your nominations. Never should there be a situation in any process where all who were nominated were selected. That is evidence of a moribund ecosystem. Our community should ever be expecting, ever causing a headache for those tasked with choosing the brightest and best by giving a larger list of ever more deserving nominees.  You may think you're more deserving than Person A on the list, but you didn't see Person A's nomination(s), in all likelihood you don't work close enough with them to know everything they did, and you weren't involved in the discussions of the nominees. You may not agree with the list and that's your prerogative. But even if unbiased, your opinion cannot be based on the same information and so is not directly comparable or relevant to this list. Instead, pay close attention to the criteria, review whether your peer nomination or self-nomination highlighted specific, measurable efforts above and beyond your day job giving useful benefits to the wider community. Seek advice from other experienced individuals about how your efforts and nomination could be better next time. Because a bad nomination may make it harder to justify an individual's inclusion, and the nominations are key criteria. Then plan for the year ahead according to HCL Master nomination criteria. Achieving your goals requires planning or a whole lot of luck, and planning is the easier route! With a little thought and an awareness of your strengths, there are no shortage of opportunities within the community.</p> <p>Well done to those on the list, and good luck to those aiming to be on the list next year.</p>","tags":["Editorial"]},{"location":"blog/2019/12/31/vertx-junit-tests/","title":"Vert.x and JUnit Testing","text":"<p>My experience with unit testing with either JUnit or TestNG is limited. With Domino development, business logic is often tightly coupled to accessing the underlying database. That makes automated testing a challenge. Similarly, the development of OpenNTF Domino API has been typically focused on code that interacts with the Domino server and databases, again making automated testing challenging. Where data persistence and the database-layer API is the main focus of the development, creating mock classes doesn't really bring much benefit. So it was Watson Workspace Java SDK which was my main experience of JUnit testing. Unit tests make a lot of sense where the business logic is about managing REST service access, both inbound and outbound.</p> <p>The other reason unit testing was a good fit for Watson Workspace Java SDK was that it was added from the start of the development. The main benefit I can envisage for any test suite is to quickly validate refactored code. It's often said that good developers write less code, great developers remove code. If you've got a reproducible test suite, you can remove code and be certain you haven't broken anything.</p> <p>Since I was last working with JUnit in depth, JUnit 5 has been released. JUnit 5 is the first major release since 2006, comprising JUnit Jupiter, JUnit Platform and JUnit Vintage. Because I'm working from scratch this time, JUnit Vintage isn't relevant. So far my testing hasn't worked much with JUnit Platform, for launching testing frameworks on the JVM. JUnit Jupiter is the core for any new tests but it also includes an extension model. Combining frameworks brings challenges and so an extension model makes that much easier. It's this extension model that Vert.x leverages to allow testing of its asynchronous verticles. Because otherwise JUnit would complete before awaiting responses. The Vert.x documentation identifies the extension to use and gives examples with the key annotation <code>@ExtendWith(VertxExtension.class)</code> and <code>VertxTestContext</code> class. The documentation also gives details of using JUnit 5 with Vert.x and there are also samples on GitHub. The key methods are <code>VertxTestContext.succeeding()</code> which ensures JUnit runs asynchronously, <code>VertxTestContext.verify()</code> which tests are wrapped in, and <code>VertxTestContext.completeNow()</code> which is used to end the tests.</p> <p>But as ever simplest is best. Depending on how the code is structured, it may be possible to test many methods without requiring asynchronous calls, separate from the Vert.x verticles that would typically be the entry point to them. This can be the case with classes for configuration objects, for example.</p> <p>As will be seen from the Vert.x documentation for unit testing, the method described to make asynchronous calls to verticles is to use the <code>io.vertx.ext.web.client.WebClient</code> class to make calls to a verticle that extends or is exposed via a Vert.x HTTP server. Typically this requires using <code>DeploymentOptions</code> to ensure the HTTP server is exposed where the <code>WebClient</code> is connecting. In many cases that may make sense to be done using an <code>@BeforeEach</code> method. But there is another, newer way to run tests, a new module <code>vertx-junit5-web-client</code>. This provides <code>io.vertx.junit5.web.TestRequest</code> class and methods for making a request to a specific endpoint passing query parameters and header parameters and expecting certain outcomes. There is a blog post just a couple of months old, \"Send web requests and assert results with vertx-junit5-web-client\", the source as well as tests on GitHub. Obviously this is quite new, so not all documentation is available yet, but it is very useful for matching the response. However, as one would expect from a helper method, it doesn't give as much control as manually using a WebClient. It doesn't seem to allow parsing of the response, just comparing status codes or direct matching of the response, so it's best used when the exact response is known based on the request.</p> <p>Of course it's also worth noting that the Vert.x classes I've mentioned for JUnit testing so far - <code>WebClient</code>, <code>TestContext</code>, <code>TestRequest</code> - are using standard Vert.x classes. The documentation does also cover using <code>VertxExtension.class</code> with the Vert.x wrappers for RxJava, both RxJava 1 and RxJava 2. In my case, because most modern makes sense, it will be RxJava 2. As with everything else, I expect more challenges and learning points when the need arises for writing JUnit tests for Vert.x verticles that use RxJava. But a hunger for learning and sharing is at the heart of my psyche.</p>","tags":["Vert.x","Java"]},{"location":"blog/2020/01/14/travels-in-manila/","title":"Travels In Manila","text":"<p>As I sit in the airport awaiting my flights back to the UK, I'm reflecting on a busy and eventful 10 days in Manila. For those not aware, in addition to some global members of the team, the bulk of the HCL Labs technical team are based in Manila in the Philippines. This was my first chance to meet the team in person, as well as my first chance since joining HCL to sit down with Stephan Wissel in person. Those who were at Collabsphere got a sneak peek at what the team has been working on. But with the start of 2020, it's time to ramp up planning for the user groups and factory tours this year, starting with Engage.</p> <p>You'll have to wait a little while to see how things have progressed and where we're going. But it's great to be working with a diverse and engaged team of individuals. The agile development approach is something new to me. As Stephan said, Domino development is often by accident agile, with developers working closely with customers to build an application in small phases of development with regular reviews. But a structured agile approach is more important when we're looking at longer (though not necessarily \"long\") projects, with team members working on various aspects from back-end to front-end to automation. Obviously as we're pushing the boundaries as well, we're more inevitably going to hit \"blockers\". With a variety of backgrounds and skill levels, it's more important we're all aware of what's happening and able to pitch in. And throughout it all, it's important to keep in mind the bigger picture and potential target audiences.</p> <p>So it was extremely useful to work in person with the team, with a variety of planning steps.</p> <p>I also got the chance for some downtime in Manila. As Mat Newman will attest, a weekend with Stephan Wissel is rather a baptism of fire. We went hiking up Mount Maculot to the Rockies. To give you an idea of what's involved, here's a photo from the bottom.</p> <p></p> <p>For one not accustomed to hiking up about 400m, often with quite steep terrain, it was a challenge. I think this photo from where I stopped highlights the effort involved.</p> <p></p> <p>But just as the climb was breath-taking, so was the view of Lake Taal.</p> <p> </p> <p>If the name sounds familiar, that's because Lake Taal is home to the Taal Volcano. The same Taal Volcano that erupted, starting with emitting steam at 1pm, as we were descending, then ash just two hours after we had left, progressing to a magmatic eruption overnight Sunday into Monday at 02:49am. If you zoom in on the last photo, there is a plume of smoke to the left of Mount Maculot. I'm not sure if that's the volcano. To give you an idea how close we were, Mount Maculot is within the red dotted 14km exclusion zone, just north of Cuenca at the southern end of Lake Taal.</p> <p>Over 450,000 people are estimated to be residing within the 14 km danger zone of the Taal Volcano. On 12 January, alert level 4 was raised and surrounding towns were evacuated. As of 6 a.m. today, @NDRRMC_OpCen reports that over 7,700 people are in 38 evacuation centres. pic.twitter.com/td0Yj0S6Cj</p>\u2014 OCHA Philippines (@OCHAPhilippines) January 13, 2020 <p>Thankfully, the \"small but dangerous volcano\" has not become so bad that my flight Tuesday evening was delayed. However, it did mean the team were working from home Monday because of the conditions, with just minimal ashfall over the south of Manila. It's certainly been eventful, thankfully not too eventful.</p>","tags":["Editorial"]},{"location":"blog/2020/01/20/engage/","title":"Speaking at Engage 2020","text":"<p>It's just over six weeks until the first user group conference of the year, the always excellent Engage. And I will be speaking again this year, with three sessions.</p> <p></p> <p>The first session on Tuesday at 11:30 in Kilimandjaro Lodge, with Stephan Wissel, will introduce Domino developers to event-driven programming and highlight how and why it's used in Project Keep.</p> <p></p> <p>The second session on Tuesday at 16:00 in Knoefzaal, also with Stephan Wissel, will give more background on Project Keep and what it will offer for Domino developers and administrators.</p> <p></p> <p>The third session on Tuesday at 17:00 in Conopy Lodge will take an innovative approach to analysing problems and identifying solutions. I'll take the scenic route but introduce you to the key to innovation.</p> <p>It will also be my first conference as a member of HCL Labs. It's been a busy couple of months with lots achieved and still more to do. Engage has always been a key event in Europe for the HCL products. Without doubt it will be a key milestone for us and other members of the team also have sessions.</p> <p>I will also be representing OpenNTF as well alongside other members of the team. We have our usual round table on Wednesday at 16:00 and we look forward to hearing from you.</p>","tags":["Conferences","Domino REST API"]},{"location":"blog/2020/01/21/dql-what-is-it-good-for/","title":"DQL: What Is It Good For?","text":"<p>tl;dr - anything you're doing on Domino, but the message doesn't seem to have reached everyone.</p> <p>DQL has been at the forefront of my radar since Domino V10 over a year ago. If I remember correctly, documentation wasn't immediately available in Domino Designer's Help, but was soon published online. It's been at the heart of sessions and advances ever since. It's often been discussed alongside the app dev pack, which allows Node.js applications to interact with Domino via the proton task. And judging from a couple of discussions in different fora over the last week, it appears the connection between DQL and the app dev pack seems a little too close. It seems to have led some to ignore DQL assuming it's only for Node.js development.</p> <p>NEWSFLASH: DQL is not just for Node.js development.</p> <p>Thanks to the excellent John Curtis, DQL is coded at the C++ layer, exposed to:</p> <ul> <li>LotsScript</li> <li>Java</li> <li>Therefore also XPages</li> <li>Therefore also ODA</li> <li>Karsten Lehmann's Domino JNA project</li> <li>Node.js endpoints</li> <li>Command line utility domquery.exe</li> <li>and soon more...</li> </ul> <p>Info</p> <p>Update: DQL is also available in Domino REST API and VoltScript now.</p> <p>I'm sure there have been plenty of slides highlighting that, but I'm not sure the message has got through as loud and clear as it should.</p> <p>Yes, it requires at least Domino V10. But I'm aware of customers that have had V10 running in production for a while. And with various releases, more and more features are being added to DQL and fixes are included.</p> <p>Some have bemoaned the lack of a demo NSF that they can use.</p> <p>NEWSFLASH: It's called DQL Explorer.</p> <p>DQL Explorer may be known as a Node.js application and I'm sure some assume it's using the proton task. The UI is indeed Node.js, packaged into an NSF - yes, you can do that. But it's not using proton, it's not dependent on the app dev pack. If you take the time to download it and have a look, the Node.js UI is calling LotusScript agents. There are 13 calls to LotusScript agents in <code>App.js</code>, 1 call to a LotusScript agent (\"getDatabasesFromServer\") in <code>DatabaseList.js</code> and 2 calls to LotusScript agents in <code>index.js</code>. These agents are all called via the ?openagent URL command which has been available for as long as I've been working with Domino. The agents are all available for investigation using the skills you've honed over your whole Domino career in the NSF.</p> <p>And, yes, it's LotusScript. But if you've got skills with Java it shouldn't be rocket science to work out how to use the samples in Java.</p> <p>So now you know.</p> <p>And if you're on V10 or V11, DQL should be one of the tools in your toolkit. If you're on V9, hopefully you're already planning upgrades to a more recent version.</p> <p>There are several sessions on DQL at Engage. If you're in Europe and doing Domino, it is one of the premier conferences in the region and well worth the money, without a doubt, every single time. I would recommend anyone attend. For manager, the ROI is significant and well worth an annual training budget. (And if you don't have a training budget, you don't deserve staff retention.)</p>","tags":["Domino","XPages","Java","LotusScript","VoltScript","Domino REST API"]},{"location":"blog/2020/02/03/unit-tests-mocks/","title":"Unit Tests and Mocks","text":"<p>In the pursuit of optimal code coverage with JUnit tests, there will inevitably be code that interacts with environments that are not available at the time of compiling the code. The solution here is mocking.</p> <p>Sometimes this can just be done by creating a separate class that extends an existing class, overriding specific methods. Stephan Wissel blogged about this recently in his blog post about Unit Tests and Singletons. But more often you need to intercept and change the behaviour of specific code within a method your unit test is calling. This is where Mockito comes in. This allows you to instantiate a call to mock an object and intercept specific methods of that object.</p> <p>Creating the object is done with the static method <code>Mockito.mock()</code>, so:</p> <pre><code>final Session s = Mockito.mock(Session.class);\nfinal NotesDatabase db = Mockito.mock(NotesDatabase.class);\nfinal NotesNote fakeNote = Mockito.mock(NotesNote.class);\nfinal NotesItem fakeItem = Mockito.mock(NotesItem.class);\n</code></pre> <p>The static method <code>Mockito.when()</code> allows you then to intercept the call. For methods that take no parameters or ones where you are looking for a specific input, that's straightforward.</p> <pre><code>when(fakeNote.getUnid()).thenReturn(\"ABCDEF1234567890ABCDEF12345678\");\nwhen(fakeNote.getFirstItem(\"Form\")).thenReturn(fakeItem);\nwhen(fakeNote.getItemValueString(\"Form\").thenReturn(\"Customer\"));\n</code></pre> <p>But it's more likely that you're calling a method that requires parameters and you don't want to specify them. This is where the static methods of <code>ArgumentMatchers</code> come in - e.g. <code>ArgumentMatchers.anyString()</code>, <code>ArgumentMatchers.anyInt()</code>, and most useful of all <code>ArgumentMatchers.any()</code> to match an object of any class.</p> <pre><code>final NotesDbQueryResult result = mock(NotesDbQueryResult.class);\nwhen(db.query(anyString(), any(), anyInt(), anyInt())).thenReturn(result);\nwhen(db.openNoteById(any())).thenReturn(fakeNote);\n</code></pre> <p>But how do you then handle a Document containing fields and values? One option here is to redirect the calls to return the corresponding value in a JSON object. Instead of <code>thenReturn()</code> we use <code>then()</code>. Then takes as its parameter an <code>Answer</code> which says what to do to find what it should return. Because Java 8 is available, it makes sense to use a lambda. So the code is like this:</p> <pre><code>JsonObject doc = new JsonObject();\n// Load your JsonObject from a resource\nfinal Document mockDoc = mock(Document.class);\nwhen(mockDoc.getItemValueString(anyString())).then(invocation -&gt; doc.getString(invocation.getArgument(0)));\n</code></pre> <p>So what's happening here is that <code>getItemValueString()</code> will be redirected to <code>JsonObject.getString()</code>. But <code>getItemValueString()</code> takes a parameter, which we need to get the corresponding value from the JsonObject. This array of parameters or arguments are what's passed into the <code>then</code> part. So we can retrieve it with <code>invocation.getArgument(0)</code>. If the method we were mocking had more arguments, we could access them as <code>invocation.getArgument(1)</code>, <code>invocation.getArgument(2)</code> etc.</p> <p>Things get more complicated though when the code you're mocking is iterating over a collection. You could use a single JsonObject, but you may want to return different JsonObjects each time. So you need a way to get the relevant JsonObject depending on which loop you're in. But if you try to just use an Integer and increment it, you'll find it doesn't compile. There's a solution here - <code>AtomicInteger</code>. This allows you to increment it from one mock method and access it from another. You can now use code like this:</p> <pre><code>JsonArray docs = new JsonArray();\n// Load your JsonArray or JsonObjects from a resource\nAtomicInteger ordinal = new AtomicInteger(0);\nfinal Document mockDoc = mock(Document.class);\n// If nextDoc() is called before processing this document, use ordinal.intValue() - 1, because ordinal will already be the NEXT document\nwhen(mockDoc.getItemValueString(anyString())).then(invocation -&gt; docs.getJsonObject(ordinal.intValue()).getString(invocation.getArgument(0)));\n\nfinal DocumentCollection dc = mock(DocumentCollection.class);\nwhen(dc.getFirstDocument()).thenReturn(mockDoc);\nwhen(dc.getNextDocument(any())).then(invocation -&gt; {\n    ordinal.getAndIncrement();\n    if (ordinal.intValue == docs.size()){\n        return null;\n    } else {\n        return mockDoc;\n    }\n})\n</code></pre> <p>The <code>mockDoc</code> mock object is used regardless of which document we're getting from the loop. We just use the AtomicInteger <code>ordinal</code> to make sure we're pointing to the relevant element of the JsonArray. Because for the first document we want the first element of the array, in <code>dc.getFirstDocument()</code> we just return <code>mockDoc</code>. For <code>getNextDocument()</code>, we don't care about the parameter passed in so we don't need to use <code>invocation.getArgument(0)</code>. But we do need to specify the method that should run, so we need the lambda. We need to increment <code>ordinal</code> each time and then either return a document or null, depending on whether or not we've reached the end of the JsonArray.</p> <p>So we can now perform unit tests on more methods and get more code coverage.</p>","tags":["Vert.x","Java"]},{"location":"blog/2020/02/04/dql-explorer-domino/","title":"DQL Explorer and Domino","text":"<p>A couple of weeks ago I explained that, even though the UI of DQL Explorer is a React app, the use of DQL is in agents. The two key agents, found within the NSF itself are runDQLExplain and runDQLQuery - the purpose of each should be apparent from the name. But the purpose of this blog post is to outline the interaction points between DQL Explorer and Domino.</p>","tags":["React","Domino","LotusScript"]},{"location":"blog/2020/02/04/dql-explorer-domino/#architecture","title":"Architecture","text":"<p>The UI, as mentioned, is a React app. As outlined in the README, the app is packaged up (via <code>npm run build</code> command) and the contents of the resulting build folder copied and pasted into the NSF in the Resources &gt; Files area. Obviously this is just one deployment option, leveraging Domino's in-built HTTP server, which makes a lot of sense because it depends on Domino's HTTP server to interact with the server.</p> <p>But that's not the only deployment option, as seen during development of it. As covered in the README, for developing the application or running it in development mode, you first need to install the relevant node modules using <code>npm install</code> and then start the application using the in-built NodeJS server using <code>npm start</code>. This then makes the application available, running outside of Domino, using http://localhost:3000. However, it then needs some mechanism to know which Domino server to connect to for the agents and other interactions. This is defined in the proxy and homepage variables in the package.json file.</p> <p>Now that we know the HTTP server the application is running on and how it knows the Domino server to connect to, let's dive deeper into the interaction points between the DQL Explorer app and Domino. It's beyond the scope of this blog post to detail the design of the application. All I will say is that <code>src\\index.js</code> provides the entry point into the application and is built as index.html. That loads in <code>src\\App.js</code>, which is the shell for the application design and incorporates other components in <code>src\\components</code> as appropriate. In XPages terminology, these are like Custom Controls or like Subforms in Notes Client terminology.</p>","tags":["React","Domino","LotusScript"]},{"location":"blog/2020/02/04/dql-explorer-domino/#das","title":"DAS","text":"<p>First off, it's worth noting that LotusScript agents are not the only integration point with Domino for retrieving data. As the README notes, you need to enable DAS (Domino Access Services) on the server. This is used for CRUD access to DQL Explorer itself, for supporting saved searches.</p> <p>In <code>src\\App.js</code> line 347 an AJAX GET call is made via axios to retrieve all saved queries in the \"collections\" view. And on line 363 another AJAX GET call is made to retrieve the selected saved query.</p> <p>In <code>src\\components\\querybuilder\\index.js</code>, the create, update and delete parts are run. Line 131 does a POST to create a new saved query. Line 162 again does a POST to update an existing saved query. And line 193 does a DELETE to remove the saved query.</p>","tags":["React","Domino","LotusScript"]},{"location":"blog/2020/02/04/dql-explorer-domino/#agents","title":"Agents","text":"<p>As mentioned, the DQL-specific agents used are \"runDQLQuery\" and \"runDQLExplain\", in line 81 and line 110 respectively of <code>src\\components\\querybuilder\\index.js</code>. These perform the actual DQL processes. The other agents are used for retrieving options to help build the DQL query.</p> <p>The first of these is in <code>src\\components\\querybuilder\\DatabasesList.js</code> line 143, getting the list of databases exposed for DQL Explorer. These are based on the documents in the Lookups view, \"Directories to search\" giving the folder names from which to retrieve databases and \"File names to include\" giving explicit NSFs to retrieve. It's worth noting here that the HTTP task doesn't work for NTFs, so you can only use DQL Explorer to query NSFs.</p> <p>The others are in <code>src\\App.js</code>. On line 312 the form names for a database are retrieved for the selected database.</p> <p>Lines 442-453 are a utility function when selecting a saved query. It varies the agents used depending on whether it's looking for \"forms\", \"views\" or \"folders\". Depending on what's selected, it gets the list of those elements and field names for forms or column names for views or folders.</p> <p>Lines 924-932 are a utility function to retrieve a list of forms, views or folders depending on what's selected.</p> <p>Lines 982-990 are the corresponding utility function to retrieve field name or column names, depending on whether forms, views or folders are selected.</p> <p>All the agents return JSON and could be refactored with Domino 11 to use the NotesJSONNavigator class for JSON generation. Currently the JSON in manually generated via print statements.</p>","tags":["React","Domino","LotusScript"]},{"location":"blog/2020/03/01/runjava/","title":"Developing RunJava Addins for Domino","text":"<p>Most Domino developers use Windows because that's the only platform Domino Designer runs on. For most of my application development life, my main device has been a Dell laptop of some variety for this reason. For almost a decade now I've also been running a Windows Domino server because Domino Designer local preview is not an effective test environment for a Domino web application. If you're using source control you are also usually testing locally unless you're developing cloud functions. So for development, you typically want a Domino server, and if you're using Domino Designer, the easiest server install to develop against is a Windows Domino server. If you want Linux on Windows and you're using Windows Professional, Docker is a sensible approach, if you take some time to understand port access from Docker.</p> <p>The downside of developing against a local Domino server comes when running Java applications that need NSF access. The challenge here is that you can run your Domino server, or you can run your Java application which talks to NSFs either directly from your IDE or from a JAR file - but you can't run both at the same time. That's because only one process on Windows can talk to the NSFs. You can access a Domino session by starting a NotesThread, you can get the username it's running as, but as soon as you try to access any NSF, it will return null.</p> <p>The solution to this is to run the Java application from within the Domino runtime. One method for doing that is as an OSGi plugin, as I blogged about on the Vaadin blog and in my XPages to Web App series some years ago. But if your web application framework is more recent, it either expects a more up-to-date version of the servlet framework or includes its own server. For the former, Jesse Gallagher's work to run Websphere Liberty Profile from Domino's HTTP stack is the best approach. If your web framework packages its own server, for example with Spring Boot or Vert.x, it makes little sense to load a new HTTP server from within the Domino HTTP stack. For this, the best option is the RunJava task in Domino.</p> <p>The RunJava task is, from what I've read, unsupported. However, it's pretty widely used, most notably with the ISpy task. If you're building and deploying the application yourself, it's probably also best to look at Andy Brunner's Domino-JAddin project. If you're not, you'll need your own RunJava addin, coded from scratch. The best resource for this is Julian Robichaux's example. After all, if you can't trust code from a Lifetime IBM Champion and HCL Grandmaster, then whose code can you trust and why do you expect your users to trust your code?</p> <p>There are a few peculiarities / anachronisms to be aware of when using it though. Firstly, when you issue the command <code>tell runjava Foo</code>, it will look for a Java class in the default package called <code>Foo.java</code> or for a Java class <code>lotus.notes.addins.foo.Foo.java</code>. Typically, this means creating a Java class called <code>Foo.java</code> which your (modern) IDE will tell you is discouraged (polite term, aka bad practice). This dates back to the pre-Java 5 days, before Java code was put into packages to avoid name collisions as Java became more widely adopted. So you will want to make sure you have a specific name in use. And it is via <code>tell runjava...</code> that you start the task. It's important to note that the RunJava addin name passed with <code>tell runjava...</code> is case sensitive. So <code>tell runjava foo</code> or <code>tell runjava FOO</code> will fail, only <code>tell runjava Foo</code> will work.</p> <p>To check if it's running, you will look at the Domino server tasks via <code>sh ta</code>. The name of the task, set in the constructor via the <code>setName()</code> method and defined in a variable called <code>progName</code> in Julian's example, is what it will appear as if you issue the Domino server console command <code>sh ta</code>.</p> <p>However, to interact with it, you interact with the Domino message queue. The message queue name passed when you call <code>mq.create()</code> is what is important here. It will typically be a unique name added to <code>MSG_Q_PREFIX</code> (which is \"MQ$\"), is the <code>qname</code> variable in Julian's example, and it's this unique name which you use to interact with the task from the Domino console. So if you set it as <code>MSG_Q_PREFIX + \"FOO\"</code>, you will use <code>tell foo...</code> to interact with it. So to end the task, you'll issue the Domino console command <code>tell foo quit</code>. The name used to interact with the MessageQueue is should be lower case. So you use <code>tell foo...</code>, not <code>tell FOO</code>. The critical part here is to ensure you call <code>mq.open()</code>, otherwise the MessageQueue will not start listening for it.</p> <p>When the addin is running there's a loop that picks up messages from the MessageQueue every 500 milliseconds and, if it's not an error message, passes it to a Java method to process the message. It's this method that the <code>tell foo...</code> Domino console command routes to. \"quit\" and \"exit\" are automatically handled, so don't need addressing here. But you can add methods for every other console command you want to expose, as well as what to return if the admin issues <code>tell foo</code> or <code>tell foo help</code>.</p> <p>As soon as you issue <code>tell foo quit</code>, the message queue message will become <code>ERR_MQ_QUITTING</code> and the loop will break. Once out of the loop, it should be coded to run your <code>doCleanup()</code> method to perform whatever cleanup is required. If you are starting Domino threads, it's critical this closes them down. Also, if using something like Vert.x which requires all verticles to be closed before shutting down, this is where you need to run Vert.x's <code>undeploy()</code> method to shut down the verticles. If that's not done, you will not be able to restart the RunJava addin again before quitting the Domino server. Also the Domino server will not terminate gracefully and will need to be killed.</p> <p>Also, it appears that the RunJava task doesn't wait long before quitting the RunJava addin itself. I took a convoluted development route to successfully ending my RunJava addin and I don't know what the RunJava integration is actually doing, but I don't think promise-based quitting would work. If your doCleanup method doesn't shut down everything cleanly before it finishes, you've also got the same issue that the Domino server will not terminate gracefully.</p> <p>Of course RunJava only works on a Windows Domino server, not on a Windows Notes Client. For that, as far as I'm aware, there's no way to run your Java application and the Notes Client at the same time. On non-Windows platforms, however, this isn't an issue. Linux can run a Java application and the Domino server at the same time. And the same is true for the Mac Client.</p> <p>One big caveat for using RunJava is that it is running a Java application. As such, if notes.ini is set with the Java debug variables, when you call <code>load runjava Foo</code>, it will start debug listening on the Java debug port. If Domino HTTP is running already or a Java agent is used, you will have a conflict because it's already listening on that port and Domino will crash. As far as I can tell, there's no way round that.</p>","tags":["Domino","Java"]},{"location":"blog/2020/03/24/junit-caution/","title":"Lessons Learned from JUnit and Refactoring","text":"<p>JUnit testing just makes sense. But writing tests is certainly a skill and your tests can have a big impact on how you structure your code. Sometimes a sensible bit of refactoring can have a large impact, particularly if the code or unit tests were not written in the best way.</p> <p>It is inevitable that some code will need to interact with a database, and that database will not be available when the unit tests run. There are multiple approaches for handling code that cannot run in a test.</p>","tags":["Java"]},{"location":"blog/2020/03/24/junit-caution/#mocking-methods","title":"Mocking Methods","text":"<p>If the code is embedded within normal class methods or in utility classes, you can create create mocks, where you will effectively create a placeholder for the class from your database's API and \"hard-code\" the output for specific methods. You're effectively skipping the code and specifying what you want to return. The code coverage looks good, but those lines have not actually been tested, you're only really testing the code around them. You need to run integration tests to ensure the real code works, running against the actual database.</p>","tags":["Java"]},{"location":"blog/2020/03/24/junit-caution/#mocking-classes","title":"Mocking Classes","text":"<p>When writing your code, it's best to try to keep all code that interacts with a database self-contained. In the past, I've even used a database-specific package for that code, to isolate it even more and you may also have a specific class as an entry point. When it comes to testing, this means you can ignore a package from code coverage tests. But for writing the tests, it means you are likely to need to mock one of your own classes. The key thing to bear in mind here is that once you mock that class, any code in that class will not run. If it's a large class, this means a lot of code that cannot easily be tested.</p>","tags":["Java"]},{"location":"blog/2020/03/24/junit-caution/#extending-classes-for-tests","title":"Extending Classes for Tests","text":"<p>If it's a small class with only a little database integration, you can extend it specifically within your test suite, which will allow you to test some methods. But this may require rewriting chunks of that class specifically so it can work for tests. You create an extended class and pass in the database-specific classes as arguments to the constructor or the method. Then in your tests, instead of passing in the actual objects, you pass in mocks. This allows you then to process some of that class and therefore get higher code coverage.</p>","tags":["Java"]},{"location":"blog/2020/03/24/junit-caution/#utility-class","title":"Utility Class","text":"<p>The other option is to move the code to a utility class as a static method. The caveat here is that you cannot mock a static method. So you need to mock all the code within it. That can result in a lot of copying and pasting around your test classes.</p>","tags":["Java"]},{"location":"blog/2020/03/24/junit-caution/#refactoring","title":"Refactoring","text":"<p>As you write you're code, you will inevitably have some code blocks or methods that are repeated or repeated with small differences. These are ripe candidates for refactoring. But it's important to think about your tests. Let's assume one of the key arguments passed is your database-integrating class. At this point, it's worth considering whether there's any point in unit testing the code you're refactoring.</p> <p>If it is just making database calls, unit tests will all be false anyway. What you're wanting to test is the outcome of the method, which will affect the calling code:</p> <ul> <li>if you move it to a mocked class, you can mock the method as well, including success / failure. But you need to also need to refactor your tests. Do this before testing or before you've used the code block many times, and the refactoring of your tests will be minimal. Do it later, and you may have to make more significant changes. But the refactoring of your tests may result in cleaner code.</li> <li>if you move it to a class extended for testing or a utility class, you can pass in mocks and still have some control over outputting success / failure. In this scenario, you're less likely to need to make changes to your tests, but you may still have a lot of duplication in your tests.</li> </ul> <p>If the code is doing more than just database calls and you want to test the actual code, then again you need to understand the impacts of mocks:</p> <ul> <li>if you move it to a mocked class, you can only mock the result of the method. You cannot process it.</li> <li>if you move it to a class extended for testing or a utility class, you can still run through it, ensuring it works as expected.</li> </ul> <p>The other key factor for tests is where your mock classes are used outside individual tests. Anything used in <code>@BeforeAll</code> methods has to be static, anything used in <code>@BeforeEach</code> doesn't have to be. If your mock is static, the same instance is used across all tests. That's fine if you're always returning the same result for that mock's method calls, so if <code>MyMock.doFoo()</code> and all other <code>MyMock</code> methods should return the same. But if you want to change the mocked result for a method for an individual test, to simulate success / failure, then that mock will be applied also for subsequent tests that run. Because the tests are not run in a linear fashion, from top to bottom, you need to update the mock in all classes. In this case, it's best to avoid using the mock in <code>@BeforeAll</code> method and ensure it's not static. Then you can set your default behaviour in the <code>@BeforeEach</code> method and override it only where needed.</p> <p>As ever, the best time to refactor is as early as possible. It will have the least impact and greatest affect.</p>","tags":["Java"]},{"location":"blog/2020/03/26/statistics-reporting/","title":"Statistics Publishing and Reporting Part One","text":"<p>Part One: Domino and Statistics Part Two: Prometheus Part Three: Micrometer and Prometheus Part Four: Micrometer and Composite Registries</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/26/statistics-reporting/#domino-v10-and-statistics-publishing","title":"Domino V10 and Statistics Publishing","text":"<p>One of the big additions in Domino V10 was statistics publishing, initially focused on New Relic. But as Daniel Nashed showed this can easily be re-routed to other locations, for example a Domino database. When I worked at Intec I tried the New Relic reporting on Domino early on and was very impressed at what was provided. My response wasn't focused on what statistics were delivered - what is outputted is not the important factor, it can easily be change. My opinion came from how easy it was to set up. New Relic itself is straightforward, but what needed to be done on the Domino side was even easier - a few Notes.ini settings, restarting and the statistics flowed. Since the days of Embedded Experiences I have been convinced that ease of implementation is critical for adoption, and adoption is key to value for effort.</p> <p>Getting the statistics is just one part and I'm not covering which tool is best in this blog post and subsequent. For that sort of topic, there is a webinar this afternoon on the new partner for Domino statistics, Panopta. This blog post is about producing statistics, and the learning points from my experience.</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/26/statistics-reporting/#prometheus","title":"Prometheus","text":"<p>The code I picked up was not publishing to though was not New Relic, but Prometheus. Prometheus seems to be developing quickly from an article I read this morning, with v2.17.0 released barely a month after v2.16.0. As I did the development, a few things became clear following my experience with Domino and New Relic:</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/26/statistics-reporting/#pull-vs-push","title":"Pull vs Push","text":"<p>New Relic expects statistics to be pushed to it. But Prometheus expects to poll an endpoint and receive the statistics. This is a significant difference in how the system generating the statistics needs to be architected. It doesn't necessarily preclude both occurring, but it is a key difference.</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/26/statistics-reporting/#naming-conventions","title":"Naming Conventions","text":"<p>The second difference is more profound - naming conventions for labels. If you use the sample database provided in Daniel's blog post you'll see the statistics outputted from Domino have a dot notation format, e.g. <code>Domino.myserver.Domino.Requests.Per1Day.Peak</code> and <code>Domino.myserver.Domino.Requests.Per1Day.Total</code>.</p> <p>However, Prometheus has a very specific naming convention. Instead of dots the separator is an underscore. There is a strict convention on units, which differs from what is outputted from the Domino statistics, and the suffix needs to reference this.</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/26/statistics-reporting/#tags-and-buckets","title":"Tags and Buckets","text":"<p>Comparing an existing output for Prometheus, labels had tags in brackets afterwards. The tags were key-value pairs. Statistics for durations were also aggregated into buckets based on length of time spent. So for duration of an HTTP request, for example, buckets might be for 0.1 seconds, 0.3 seconds, 0.5 seconds etc. So if a request took 0.5 seconds, only the 0.5 seconds bucket got incremented; if it took 0.2 seconds, the buckets for 0.3 and 0.5 got incremented; if it took 0.1 seconds, all buckets got incremented.</p> <p>In the next blog post, I'll cover my initial coding approach.</p>","tags":["Domino","Vert.x","Prometheus","Micrometer"]},{"location":"blog/2020/03/30/statistics-for-prometheus/","title":"Statistics Publishing and Reporting Part Two: Statistics for Prometheus","text":"<p>Part One: Domino and Statistics Part Two: Prometheus Part Three: Micrometer and Prometheus Part Four: Micrometer and Composite Registries</p> <p>In the last blog post I covered the differences between different statistical reporting tools. I can't speak for Panopta covered in the recent HCL webinar. I suspect they're using \"pull\" (they install an agent on the Domino server) and I suspect they're using one of the traditional reporting databases (their monitoring covers time periods, so there must be a database; and their offering is based around pre-built dashboards for various products, so no necessity for their USP to build their own reporting repository).</p> <p>But my comparators were the Domino statistics and the out-of-the-box statistics for Prometheus from Vert.x. This highlighted some significant differences in what was built:</p> <ol> <li>Prometheus naming convention used underscores, Domino's stats were provided with dot notation.</li> <li>Vert.x stats for Prometheus were dimensional, Domino stats were not. By this I mean the Prometheus stats had a name followed by tags in curly brackets. For example, <code>vertx_http_server_responseTime_seconds_count{code=\"200\",method=\"POST\",} 1.0</code>, where the tags are the HTTP  (\"POST\") and HTTP response code (\"200\"). The tags then allow slicing and dicing the data in different ways. In contrast, the Domino stats didn't include tags, just a descriptor string (e.g. <code>Domino.myserver.Database.DbCache.CurrentEntries 7</code>).</li> <li>Prometheus has a standard set of units always used - seconds and bytes. However, Domino's stats output a variety of different units (<code>Domino.myserver.Mem.Local.Max.AllocFromOS_KB</code>, <code>Domino.myserver.Mem.MaxSharedMemory_MB</code>, <code>Domino.myserver.Server.ElapsedTimeDays</code>, <code>Domino.myserver.Server.ElapsedTimeMinutes</code>).</li> <li>Prometheus stats included buckets for histogram reporting. This is the \"le\" tag in <code>vertx_http_server_responseTime_seconds_bucket{code=\"200\",method=\"POST\",le=\"0.001\",} 0.0</code>. So this bucket is for HTTP 200 responses for a POST request completing in less than 0.001 seconds.</li> </ol> <p>Pull or push is actually not an issue with Vert.x. Creating another HTTP server endpoint mapping to a method is easy. And Vert.x can also handle creating an HTTP client, to perform a push. But for Vert.x, it was a pull.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/03/30/statistics-for-prometheus/#collector","title":"Collector","text":"<p>The stats needed to be incremented as anything was happening. But we needed somewhere to store them, so a singleton enum. Within that singleton, we needed maps to hold the various statistics. When it comes to maps in a singleton that will be updated, concurrent access is a key requirement. And for that my go-to options are ConcurrentHashMap and AtomicInteger or in this case AtomicDouble for when a single value is needed.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/03/30/statistics-for-prometheus/#seeding-the-maps","title":"Seeding the Maps","text":"<p>The first step was to add a method to load the relevant key into appropriate maps. In Vert.x, we create handlers to respond to HTTP requests and Verticles to perform database-facing code. So when setting up each of these, I initialised the map with a 0.0 value (e.g. <code>databaseHandlerCounts.put(classNameForPrometheus, 0.0);</code>). I converted the dot notation of the class name to underscores with single string substitution - <code>StringUtils.replace(className, \".\", \"_\")</code>.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/03/30/statistics-for-prometheus/#incrementing-the-maps","title":"Incrementing the Maps","text":"<p>The second step was to increment the value each time there was a call. With Java 8 the code for that becomes more succinct:</p> <pre><code>databaseHandlerCounts.merge(classNameForPrometheus, 1.0, (a,b) -&gt; a+b);\n</code></pre> <p>The <code>merge()</code> method replaces the need for separate <code>get()</code> and <code>put()</code> calls. The first parameter (<code>classNameForPrometheus</code>) is the key to look for. The second parameter (<code>1.0</code>) is what is being merged in, and so becomes the value of <code>b</code> in what follows. And the third parameter is a lambda function for how to merge the values. <code>a</code> is the old value, <code>b</code> is the value being merged in. So we just need to add them.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/03/30/statistics-for-prometheus/#buckets","title":"Buckets","text":"<p>Handling buckets for a histogram got more complicated. That's because you need to increment the lowest bucket the duration fits in, and all higher buckets. For that I used an <code>if</code> statement where I compared the duration to a bucket's maximum and returned an integer - bucket 1 as 1, bucket 2 as 2 etc. I then used a switch statement without a break to increment each bucket, so that it incremented not only the matching bucket, but all subsequent. E.g.:</p> <pre><code>if (duration.compareTo(0.1) &lt;= 0) {\n    le = 1;\n} else if (duration.compareTo(0.3) &lt;= 0) {\n    le = 2;\n} else if (duration.compareTo(0.5) &lt;= 0) {\n    le = 3;\n}\n\nswitch (le) {\n    case 1:\n        databaseHandlerDurationBuckets.merge(0.1, 1.0, (a,b) -&gt; a+b);\n    case 2:\n        databaseHandlerDurationBuckets.merge(0.3, 1.0, (a,b) -&gt; a+b);\n    case 3:\n        databaseHandlerDurationBuckets.merge(0.5, 1.0, (a,b) -&gt; a+b);\n}\n</code></pre>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/03/30/statistics-for-prometheus/#outputting-the-values","title":"Outputting the values","text":"<p>I then just needed a method to output the statistics when called by Prometheus. Again by comparing the output from Vert.x, it was clear that each unique label was preceded by <code>HELP</code> and <code>TYPE</code> comments, presumably for the relevant UI. So for a given map, this resulted in the following code being added to a StringBuilder:</p> <pre><code>sb.append(\"\\n# HELP keep_database_handler_total Number of messages processed per database handler\");\nsb.append(\"\\n# TYPE keep_database_handler_total counter\");\ndatabaseHandlerCounts.forEach((k, v) -&gt; {\n    sb.append(\"\\nkeep_database_handler_total{class=\" + k + \",} \" + v);\n});\n</code></pre> <p>This worked well. But in part three, the last part, I'll show how I took a much better-practice approach.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/01/statistics-using-micrometer/","title":"Statistics Publishing and Reporting Part Three: Using Micrometer","text":"<p>Part One: Domino and Statistics Part Two: Prometheus Part Three: Micrometer and Prometheus Part Four: Micrometer and Composite Registries</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/01/statistics-using-micrometer/#the-easy-way","title":"The Easy Way","text":"<p>In the previous two parts I gave some background on metrics and then described my first pass at coding metrics, quite frankly the hard way. The elephant in the room should be apparent: the metrics output was manually coded for a specific reporting tool.</p> <p>The solution to that problem, in Java, is Micrometer, a vendor-neutral metrics facade. This is what Vert.x uses to manage the metrics and how it outputs out-of-the-box for Prometheus, InfluxDB and JMX. But of course because they're just hooking Vert.x into standard Micrometer, metrics can also be provided for any other backend that Micrometer supports, e.g. Graphite, Elasticsearch, New Relic and many others.</p> <p>The benefits of this should be obvious. You're offloading all the hard work of integrating with multiple metrics backends to someone else. And Micrometer is open source, so you can see the code for yourself and, if you should identify a problem, work out the fix and submit a pull request. It also means additional metrics backends can be coded, if required, and added to the registry. By using a CompositeMeterRegistry your Vert.x can output to multiple.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/01/statistics-using-micrometer/#why-recode-now","title":"Why Recode Now?","text":"<p>My initial plan was to revisit the metrics in a later phase and recode them. But after some more investigation it became apparent that Micrometer handles buckets for histograms automatically, handles creating summaries for them etc. So the output would undoubtedly be different to what I had already coded.</p> <p>Obviously this would invalidate any effort on producing reporting graphs.</p> <p>The right approach was to recode now.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/01/statistics-using-micrometer/#meters-and-registration","title":"Meters and Registration","text":"<p>When coding manually I had created ConcurrentHashMaps. With Micrometer the approach is to create Meters and add them to the registry. <code>BackendRegistries.getDefaultNow()</code> provides a simple method to get the registry that should have previously been added when your application starts.</p> <p>There are different types of Meters. The ones relevant to what I was doing were:</p> <ul> <li>Timer to record how long something took.</li> <li>Counter for number of times something is called. Note, this is used for something that can only increment.</li> <li>Gauge for a numerical result that can go up or down, used to get the current number.</li> </ul> <p>The Metrics are created with a name and one or more tags. The tags are key-value pairs, so let's compare that with the code I was outputting:</p> <pre><code>databaseHandlerCounts.forEach((k, v) -&gt; {\n    sb.append(\"\\nkeep_database_handler_total{class=\" + k + \",} \" + v);\n});\n</code></pre> <p>Here the name of the metric is \"keep_database_handler_total\". The tag was what was in the curly braces - the key was \"class\" and the value the Java class name. The benefit over the approach I had becomes immediately apparent, it's much easier to use multiple tags, e.g. class the metric is triggered from, database being interrogated etc.</p> <p>There seem to be multiple approaches available for creating metrics.</p> <ul> <li>The Meter base class has a static <code>builder()</code> method to create an instance of the Meter. Here you register the meter to a registry.</li> <li>There is a Metrics class with static methods to create different meters. This is what I first came across in the Micrometer documentation. <code>Metrics.addRegistry()</code> seems to be key here for enabling the counter for output.</li> <li>There is a method on the Meter Registry to create an instance of a Meter.</li> </ul> <p>The important point here is that a meter can only be registered with a given name and tags once. An error will be thrown if you try to register a duplicate meter. This also means you want to have all the tags available before you first register it. The key then is registering it once and then retrieving the meter for subsequent calls to use.</p> <p>When it comes to outputting the results, the beauty of this approach is the simplicity: <code>PrometheusScrapingHandler.create()</code> creates a Vert.x route handler that writes out all metrics using the default metrics registry we've been creating metrics on. The method for enabling outputting for other registries will differ, but is beyond the scope of this blog post.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/","title":"Statistics Publishing and Reporting Part Four: Auto-Configuration and Composite Registries","text":"<p>Part One: Domino and Statistics Part Two: Prometheus Part Three: Micrometer and Prometheus Part Four: Micrometer and Composite Registries</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/#easier-endpoints-for-single-backends","title":"Easier Endpoints for Single Backends","text":"<p>In the last part I covered outputting metrics. If you have an existing HTTP server or wish to create a standalone server, it's easy to output the metrics, as shown on the Vert.x samples:</p> <pre><code>Router router = Router.router(vertx);\nrouter.route(\"/metrics\").handler(PrometheusScrapingHandler.create());\nvertx.createHttpServer().requestHandler(router).listen(8080);\n</code></pre> <p>But even easier is the use of an embedded server. The caveat here is that the embedded server can only be used if you're outputting to a single backend - Prometheus, InfluxDB or JMX. If you only have a single backend, you can set the configuration options and it will automatically start a single registry for the relevant backend. So for Prometheus, the example in the documentation is:</p> <pre><code>Vertx vertx = Vertx.vertx(new VertxOptions().setMetricsOptions(\n  new MicrometerMetricsOptions()\n    .setPrometheusOptions(new VertxPrometheusOptions().setEnabled(true))\n    .setEnabled(true)));\n</code></pre> <p>There is a similar <code>set....Option()</code> method for InfluxDB and JMX. The actual options for the embedded server can be set via JSON configuration or Java methods of the VertxPrometheusOptions class. If you want to use a JSON configuration, the Prometheus-specific options are built using a JSON object like this:</p> <pre><code>{\n  \"embeddedServerEndpoint\" : \"/metrics\",\n  \"enabled\" : true,\n  \"publishQuantiles\" : true,\n  \"startEmbeddedServer\" : true\n}\n</code></pre> <p>This defines an endpoint for the embedded server, which is automatically started because <code>startEmbeddedServer</code> is set to <code>true</code>. <code>publishQuantiles</code> tells it to output histogram data. There is a constructor <code>public VertxPrometheusOptions(JsonObject json)</code> which will take that JSON object and set the properties accordingly. But you also need to define the embedded server's settings - whether to use SSL, which HTTP port etc. This is done by calling <code>setEmbeddedServerOptions()</code>, passing in an <code>HttpServerOptions</code> object, as you would for any other Vert.x HTTP server.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/#composite-registries","title":"Composite Registries","text":"<p>However, the embedded server and VertxPrometheusOptions will only work if you only want to publish stats for Prometheus. As I mentioned, this auto-creates a registry. There is a method <code>MicrometerMetricsOptions.setMicrometerRegistry()</code> to pass in a registry, which you need to do if you want a composite registry for multiple backends. But as the Javadoc confirms, you can either add options for Prometheus / InfluxDB / JMX or you can set a registry - you cannot use <code>VertxPrometheusOptions</code> if you want a composite registry.</p> <p>As I mentioned in the last part, there are two strengths for Micrometer:</p> <ol> <li>It makes it easier to get the correct format for a specific backend.</li> <li>The <code>CompositeMeterRegistry</code> means you can output the same metrics for multiple backends.</li> </ol> <p>If you've got a very specialised implementation and only want to support a single backend, the embedded server makes that very easy. If people want to use other backends, you place the onus on third-party manipulations. That's presumably what Panopta's Domino agent does, if I understood the webinar at the end of last month correctly. Similarly, if you would want to report on Domino's statistics on Prometheus, for example, you would also need to write custom code to convert the statistics yourself - because they're hierarchical, not dimensional; because Prometheus pulls, doesn't receive a push.</p> <p>But offering freedom of choice and making it easier for consumers is always good, and Micrometer allows you to do that. There are (at least) two key changes.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/#create-and-seed-compositemeterregistry","title":"Create And Seed CompositeMeterRegistry","text":"<p>The first is you need to create a <code>CompositeMeterRegistry</code> and use <code>MicrometerMetricsOptions.setMicrometerRegistry()</code> to load it. You can then add the various registries you wish to support with <code>CompositeMeterRegistry.add()</code>. The Micrometer documentation will give you the relevant Maven settings and code needed to add the registry, e.g.</p> <pre><code>PrometheusMeterRegistry prometheusRegistry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);\n</code></pre> <p>In the last part, when creating meters I used <code>BackendRegistries.getDefaultNow()</code> to get the registry. The same method will then get the <code>CompositeMeterRegistry</code> you created. The good news is that if you just add Counters and Timers to the composite registry, then when you scrape the statistics later, you still get them.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/#outputting-statistics","title":"Outputting Statistics","text":"<p>The second change you need is to write the code for the server endpoint (if it's pull) or client (if it's push). The <code>PrometheusScrapingHandler()</code> won't work here, because the registry in use is a CompositeMeterRegistry, not a PrometheusMeterRegistry. Instead, you'll need to scrape from the relevant registry:</p> <pre><code>final HttpServerResponse response = routingContext.response();\nBackendRegistries.getDefaultNow().getRegistries().forEach(registry -&gt; {\n    if (registry instanceof PrometheusMeterRegistry){\n        PrometheusMeterRegistry promRegistry = (PrometheusMeterRegistry) registry;\n        response.putHeader(HttpHeaders.CONTENT_TYPE, TextFormat.CONTENT_TYPE_004)\n        .end(promRegistry.scrape());\n    }\n});\n</code></pre>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/04/18/micrometer-composite-registries/#histograms","title":"Histograms","text":"<p>As I mentioned, adding Counters and Timers to the composite registry means those statistics get outputted. However, histogram data doesn't. There isn't an obvious setting on <code>PrometheusConfig</code> to enable it, though the <code>HistogramFlavor</code> seems to be relevant. There isn't a specific option when scraping. The <code>PrometheusDistributionSummary</code> seems to be key. It may be necessary to create Timers on the PrometheusMeterRegistry rather than the CompositeMeterRegistry, I'm not sure. More trial-and-error investigation would be necessary to confirm.</p>","tags":["Vert.x","Micrometer","Prometheus"]},{"location":"blog/2020/06/08/app-dev-musings/","title":"Application Development Musings","text":"<p>Since joining HCL Labs my focus for Domino development has been Domino APIs, with Project Keep. Obviously there is little point using a REST API against a Domino database in an XPages application or LotusScript agent. Consequently, application development has been almost exclusively outside of XPages. This has reinforced key differences between Domino development and other application development frameworks. Now was a good time to cover this, particularly following Chris Toohey's excellent blog post \"Was XPages a waste of time?\". I've always been vocal, that my experience of XPages was not a waste of time. But I'm very much one of those who took a huge step further, and took a lot of time to understand very deeply how XPages worked and how other frameworks were similar or different.</p> <p>One difference is the use of asynchronous code with callbacks. I've blogged about the virtual absence of asynchronous code in Domino before. Another difference is the clear split between back-end code and front-end code. XPages has a split between front-end and back-end code, but on a number of occasions StackOverflow questions have shown that developers have not grasped this separation, perhaps because of the use of the term Server-Side JavaScript. Both are fundamental architectural differences to web development, which would require inelegant \"hacks\" for any automatic conversion of Notes Client or XPages applications to another web framework.</p> <p>But what I want to focus on something different here, something that became apparent when I first used Vaadin after a few years of XPages, and something that has been reinforced following exposure to other development frameworks and platforms. It's also a significant architectural difference, and one that is worth bearing in mind when looking beyond Notes Client or XPages development. The difference is how the properties of things and their current state are set.</p>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/06/08/app-dev-musings/#notes-and-simple-xpages","title":"Notes and Simple XPages","text":"<p>Consider these techniques in Notes Client development:</p> <ul> <li>Computing the value for a hide-when.  </li> <li>Using Computed Text and refreshing the page.  </li> <li>Computing the options for a Dialog List and refreshing to update the options.</li> </ul> <p>We do not interact with the UI or the components on it, mainly because the Notes Client doesn't really allow it. We define how fields should behave on the field itself, change a state, recalculate the page and let the code on the field decide how the UI should act.</p> <p>The same was the typical approach with XPages. Indeed one of the big strengths of XPages was that almost everything could be computed. Properties of data objects and components are all computed on the relevant components. Sometimes the property value is computed directly. Consider the following code, for example:</p> <pre><code>&lt;xp:button id=\"button1\"&gt;\n    &lt;xp:this.value&gt;&lt;![CDATA[#{javascript:if (document1.isEditable()) {\n        return \"Cancel\";\n    } else {\n        return \"Close\";\n    }}]]&gt;\n&lt;/xp:button&gt;\n</code></pre> <p>This is a common approach in XPages with manipulating properties of components: set a scoped variable and compute a property based on the scoped variable.</p>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/06/08/app-dev-musings/#xpages-alternative","title":"XPages Alternative","text":"<p>But sometimes in XPages a property value will be bound to a scoped variable - requestScope, viewScope, sessionScope, applicationScope - and then using that property in a button.</p> <pre><code>&lt;xp:panel rendered=\"#{viewScope.showMe}\"&gt;\nSome content\n&lt;/xp:panel&gt;\n&lt;xp:button id=\"button1\" value=\"Show Panel\"&gt;\n    &lt;xp:eventHandler event=\"onclick\" submit=\"true\" refreshMode=\"partial\"&gt;\n        &lt;xp:this.action&gt;&lt;![CDATA[#{javascript:viewScope.put(\"showMe\",true);}]]&gt;&lt;/xp:action&gt;\n    &lt;/xp:eventHandler&gt;\n&lt;/xp:panel&gt;\n</code></pre> <p>Something like that is typically only used when the value needs to be re-used across the XPage, to avoid typing the same SSJS in multiple places.</p> <p>Over the last few years my XPages development changed though, subtly, but significantly. I typically used page controllers or some variant. Properties were then bound to properties of the page controller, and those properties had getters and setters. So the Cancel button XPages example would be something like:</p> <pre><code>&lt;xp:button id=\"button1\" value=\"#{pageController.cancelButtonLabel}&gt;\n&lt;/xp:button&gt;\n</code></pre> <p>In the pageController's I would then have a String property <code>cancelButtonLabel</code> with getters and setters, and in the beforePageLoad I would have code like this:</p> <pre><code>if (document1.isEditable()) {\n    setCancelButtonLabel(\"Cancel\");\n} else {\n    setCancelButtonLabel(\"Close\");\n}\n</code></pre>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/06/08/app-dev-musings/#so-whats-the-difference","title":"So What's The Difference?","text":"<p>The difference really becomes most apparent when it comes to troubleshooting.</p> <p>In the first set of scenarios, the component properties are declarative - they say what their property values should be. When your page doesn't behave how you expect, you look on the relevant field or component, look at the code, and try to work out the current values the code is using. Your starting point is the particular component. But your business logic is spread throughout the Form or XPages (and Subforms or Custom Controls), and you need to look at other fields or components to work out what the current values being used are.</p> <p>In the second set of scenarios, your buttons are more imperative - it changes a variable's value, from which the component property retrieves its value. When your page doesn't behave how you expect, you try to work out what the page's last state was, what action has just been performed and how that has changed settings throughout the page. Your business logic is more self-contained. However, what your business logic needs to do depends on the variety of previous states the page might be in.</p>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/06/08/app-dev-musings/#so-why-is-this-important","title":"So Why Is This Important?","text":"<p>Well, it's important because trying to convert one style of programming to another is extremely challenging, if even possible. Have a look at one of your XPages or Notes Forms, of moderate complexity. Now think about how to convert the computations on each component / field to putting all that page manipulation on the click event that triggers the update. You start to see the challenge.</p> <p>It becomes more important because when we move beyond Domino, the latter is much more common than the former.</p> <p>Think of basic web development. XPages sends the HTML for a whole portion of the page to the browser and overwrites that part of the DOM. Client-side JavaScript doesn't do that. It imperatively updates individual elements in the DOM. I have only done a little React, but from what I understand React works the same, changing the state of components and only updating those parts of the DOM that change.</p> <p>On the front-end, that's also what Vaadin does. When it comes to backend coding, again, everything is Java objects and buttons define how components should be changed.</p> <p>After playing a little with Volt, I saw some similarities with this kind of approach with Workflow Stages, where the state of the page - whether fields are visible or not etc - is defined on a particular Workflow Stage, not on the Form. The Workflow Stage seems to effectively copy the Form and allow you to change what the properties should be on the Workflow Stage, not on the original UI Form.</p> <p>Moving a little further beyond Domino, I encountered a similar approach with Temenos' Quantum Visualizer, where again rather than setting properties on each component on the page and recalculating the whole page, actions trigger JavaScript in controllers, which updating component mappings and explicitly set component properties.</p>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/06/08/app-dev-musings/#conclusion","title":"Conclusion","text":"<p>I'm not saying one approach is better or worse than the other, that kind of comparison is not the focus of this blog post.</p> <p>What I am saying is that the imperative approach - actions changing component properties explicitly - seems more pervasive than actions running and components implicitly changing their own properties. It requires a different mindset for architecting the solution and a different approach to troubleshooting the solution.</p> <p>And any difference means trying to convert an application - automatically or manually - is not straightforward, especially if the original code is not well documented.</p>","tags":["Domino","Vaadin","Leap","Volt MX"]},{"location":"blog/2020/07/05/postman/","title":"Getting the Most out of Postman","text":"","tags":["Testing","REST Clients"]},{"location":"blog/2020/07/05/postman/#postman-tests","title":"Postman Tests","text":"<p>If you're developing an API, the best tool to test with is Postman. When I initially used Postman, I only used it for basic individual tests. If you followed my blog at Intec, that also led me to recommend and to use Node-RED for creating a test flow. However, over the last few months I've learned about significantly more functionality of Postman, which starts to become extremely relevant when using Postman collections for other purposes.</p>","tags":["Testing","REST Clients"]},{"location":"blog/2020/07/05/postman/#collections","title":"Collections","text":"<p>Creating collections of requests is standard, but being able to group requests in folders is useful. If you're developing an API, you should be building from an OpenAPI specification. And if your OpenAPI spec has a number of requests, they will have tags to group them. A useful way to order your collections is to builds folders that correspond to the tags in the OpenAPI spec.</p> <p>Having a collection on one device is good. But Postman also syncs your collections onto all devices. This is because your collections are shared to a personal workspace. You can also create team workspaces, but the free option is limited to 25 requests. If your API has a reasonable number of endpoints, the free team workspace probably won't be enough.</p> <p>But a workspace can also be exported. The export is a JSON file, which then allows it to be stored in your source control repository, as a single version of the truth. I've not come across any functionality to automatically sync to a Git repo, which means that your team must be disciplined, ensuring that they do not corrupt the collection. This is easy to do, if you change a request for a specific path or query parameter. But there are ways round that.</p>","tags":["Testing","REST Clients"]},{"location":"blog/2020/07/05/postman/#variables","title":"Variables","text":"<p>Variables allow you to parametrise requests and are key-value pairs. You can recognise them in a collection by the curly braces. The variables can be in a URI, in header parameters or in the body of a request. For example <code>{{HOST}}/document/{{UNID_0}}/default?db=demo</code>.</p> <p>This request URI has two variables:</p> <ul> <li>HOST which is the first part of the URI.</li> <li>UNID_0 which is a document UNID.</li> </ul> <p>The main kinds of variables you will typically use are:</p> <ul> <li>Global - used for all collections and requests.</li> <li>Collection - used for the current collection only.</li> <li>Environment - corresponding to a Postman environment.</li> <li>Data - corresponding to a file passed in.</li> </ul> <p>With this you can have a much more effective set of requests, for example:</p> <ol> <li>A GET request retrieving a collection of documents, for which you store the first document's ID as <code>{{UNID_0}}</code>.</li> <li>A GET request against a single document, using <code>{{UNID_0}}</code>.</li> <li>A PUT request to update that single document <code>{{UNID_0}}</code>.</li> <li>A DELETE request to remove that document.</li> </ol> <p>By using variables, you don't have to regularly change the request and you can have a collection that is run to test for regression bugs.</p>","tags":["Testing","REST Clients"]},{"location":"blog/2020/07/05/postman/#tests","title":"Tests","text":"<p>But the question that remains is how to store those variables. That can be done in Tests. The main purpose of tests is to perform assertion checks against the response of a given request. But as part of that post-processing, you can also parse the response and store them to variables. Consider the following test script:</p> <pre><code>pm.test(\"Status code is 200\", function () {\n    pm.response.to.have.status(200);\n});\npm.test(\"Content-Type is present\", function () {\n    pm.response.to.have.header(\"Content-Type\");\n});\npm.test(\"User worked\", function () {\n    var jsonData = pm.response.json();\n    pm.environment.set(\"UNID_0\", jsonData[0][\"@unid\"]);\n    pm.expect(jsonData.length == 10);\n});\n</code></pre> <p>You can find more details about how to write postman tests in their documentation. But it's fairly straightforward. It doesn't take a great deal of learning to realise that this has three tests:</p> <ul> <li>That the response code is 200.</li> <li>That there is a Content-Type header.</li> <li>That the JSON response contains 10 results.</li> </ul> <p>But most useful is the middle line of the last test. This sets an \"environment\" variable with the <code>@unid</code> property of the first JSON object returned. <code>pm.collectionVariables.set()</code> will set a variable for a collection, <code>pm.globals.set()</code> will set a global variable.</p> <p>The tests then show up in the response are, to easily see whether tests passed or failed, as you can see from the image below. Of course tests are also useful when running a whole collection, as we'll see later.</p> <p></p>","tags":["Testing","REST Clients"]},{"location":"blog/2020/07/05/postman/#environments","title":"Environments","text":"<p>Environment variables are used for environments. Environments are particularly useful when testing a collection against different servers or running the same tests for development, testing, staging and production environments. This means you can have different base URLs, different app IDs and app secrets, different database paths etc. By switching environment, you can run the same collection's requests without changes.</p> <p>The environments are available in the top right of Postman, where you:</p> <ul> <li>See the current environment and switch between the environments using the drop-down lists.</li> <li>View the current settings using the \"eye\" button.</li> <li>Manage the environments using the \"cog\" button.</li> </ul> <p></p> <p>This covers maximising the usage of Postman for running individual tests. But once you have a collection, there are other things you can do with the Postman collection that give you greater benefits.</p>","tags":["Testing","REST Clients"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/","title":"Volt MX LotusScript Toolkit","text":"<p>Earlier this week Jason Roy Gary announced the Volt MX LotusScript Toolkit. It's important to put some background to manage expectations. There will be an OpenNTF webinar on December 17th where we will explain more about our aims for the project and provide a call-to-arms to the community to join us driving this forward. I encourage everyone to attend if you're interested in using Agents outside the Notes Client or a Form's WebQueryOpen and WebQuerySave methods. But in advance, let's cover some questions I expect people to have.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#is-it-ready-to-use","title":"Is It Ready To Use?","text":"<p>At this point the script library is a functioning preview. It was designed (in less than one two-week sprint) to provide a proof of concept against a number of goals, goals which it achieved more successfully than I expected. It is certainly some way short of being fully production-ready. For example, there is nothing yet to convert date fields - in or out. But it covered enough functionality to be used for the Volt MX demos at the start of Digital Week, even though that was not the reason it was built. And more importantly, it is structured deliberately to be used beyond just Volt MX.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#but-its-called-volt-mx-lotusscript-toolkit","title":"But It's Called Volt MX LotusScript Toolkit","text":"<p>Yes, but that's because there's a class, VoltMXHttpHelper, which automatically extracts a specific payload.</p> <ul> <li>unid: the UNID of the document to act upon, which you can retrieve with <code>.getActionDocUnid()</code> and <code>getActionDoc()</code>.</li> <li>dbPath: the database to find that document in.</li> <li>payload: an array of fields and values to update, which is done via <code>updateActionDoc()</code>.</li> <li>returnFields: an array of fields the class can automatically return via <code>addReturnFieldsToResponseBody()</code>.</li> </ul> <p>You can pass that payload from anywhere - Volt MX Foundry, Node-RED, Postman, or any web app. You don't need to pass all of that payload, and can pass additional content as well. The payload is designed for common functions that you might want to do, that's all.</p> <p>If you don't need to pass any of that payload, you may be better using the parent class, <code>NotesHttpJsonRequestHelper</code>, which just expects JSON in and JSON out.</p> <p>If you need something other than JSON in and out, that class's parent class <code>NotesHttpRequestHelper</code> is the one you want.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#when-are-you-going-to-finish-it","title":"When Are You Going To Finish It?","text":"<p>This is not intended as an HCL product or a set of classes built into Domino and supported by HCL. This is more LotusScript than I've written in the last year at HCL. Your apps are the ones where it needs to work, you are the ones who are best positioned to drive scope and probably better skilled to do the development.</p> <p>Yes it has some innovative functionality with fluent methods and some quite advanced use of classes. But it's by no means beyond the understanding of most LotusScript developers and I would recommend a 19-year-old article on object oriented features of LotusScript, which was a very useful resource for building classes. UPDATE the article in question was on IBM developerWorks and unfortunately is no longer available.</p> <p>This is intended as a joint HCL and community effort. The vision is that this will be a new kind of OpenNTF project, with an approach and structure more akin to OpenNTF Domino API. We need people to develop it. We need a project team to decide on what should be in and out of scope. We need people to do documentation and write samples. We need people to test and pick up issues on GitHub, because you'll have the environments and apps to better validate bugs and test.</p> <p>And if there is interest in porting the libraries back to Domino 10, where NotesJsonNavigator is read-only, the community should be able to do that.</p> <p>If there's really a need, the community can port it back before any of the NotesJson classes, so it would work as far back as Domino 5!</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#but-i-have-to-copy-and-paste-libraries-in","title":"But I Have To Copy And Paste Libraries In","text":"<p>Yes. Same as you do for OpenLog. But because it's in Git you can also see what changes are made, and decide if you want to pull in updates. And Script Libraries can inherit from a different template, to make that easier.</p> <p>Plus, you may not have noticed Jesse Gallagher picked up the Import and Export for Domino Designer project on OpenNTF, which allows you to import design elements into Domino Designer from a local filesystem or OpenNTF project. The project was designed for XPages Custom Controls, but it works for any design element. All it would need in the project is an extension.xml file defining the design elements to pull in. Then you could pull the code into your NSFs without leaving Domino Designer, if you wish.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#how-do-i-get-involved","title":"How Do I Get Involved?","text":"<p>Every good project needs a channel for discussion. Thankfully OpenNTF already has a Slack chat, and there's now a channel there.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#what-about-xpages","title":"What About XPages?","text":"<p>Converting LotusScript to Java is not difficult. Creating Java classes - or SSJS Script Libraries - for XAgent XPages would certainly be feasible, if there is a desire to do that. To be honest, my personal recommendation would be:</p> <ul> <li>if you want to re-use LotusScript code, call agents.</li> <li>if you want to re-use Java / SSJS business logic, use XAgents with a small framework that extracts a payload along the lines of the one for the VoltMXHttpHelper into viewScope variables.</li> </ul> <p>But that too should be open sourced.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2020/11/13/volt-mx-ls-toolkit/#want-to-know-more","title":"Want To Know More?","text":"<p>As mentioned, come along to the OpenNTF webinar on December 17th. We'll explain more about the decisions behind the framework and the approach, dive into the code, and hear from the legend Rocky Oliver - the only other person to date who has consumed the library.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2021/01/02/domino-timezones/","title":"Domino Timezones","text":"<p>There are a number of challenges when it comes to two-way REST and Domino. But one of the biggest challenges for manipulation between NotesDateTime objects and JSON is timezone handling. There is an Product Ideas request to provide serialization / deserialization between Domino objects and JSON strings, which surprisingly only has 31 votes, but it's not there yet. So for Volt MX LotusScript Toolkit, this needs handling within the toolkit itself.</p> <p>The first aspect to bear in mind is that time only or date only do not have timezones. 1st January 2021 is 1st January in Australia, 1st January in UK and 1st January in east coast USA. Similarly, 12 noon is 12 noon in Australia, UK and USA - whether daylight savings is in operation or not. If you want to store a single value for 12 noon in the current timezone on any given day, a NotesDateTime is not (strictly speaking) the best storage format, unless the value is to be used only within a Notes or Domino context.</p> <p>So I'm focusing here only on full date-times, which include a timezone. Interestingly, this isn't the first time I've had to approach the problem. Project Keep uses Domino JNX - a redevelopment learning from Domino JNA and OpenNTF Domino API, going via C++ API. That's actually very easy, because the DateTime objects are actually stored as UTC timestamps - and incidentally that avoids all the challenges of writing values. I also addressed conversion in Java in OpenNTF Domino API and blogged about. For ODA and Java 8, I was able to use a Temporal and a DateTimeFormatter, in combination with the internationalisation formats, although we had the same problems for writing that Volt MX LotusScript Toolkit will have.</p> <p>With LotusScript and Volt MX LotusScript Toolkit, I drew upon my experience of ODA as well as some JavaScript code that Jason Roy Gary developed. For the timezone format to write to, there is a standard defined, RFC 3339, section 5.6. This uses UTC offsets, so the only challenge is working out the UTC offset for Domino timezones. Unfortunately, that's not so straightforward.</p> <p>The NotesDateTime object has a <code>.timezone</code> method which might seem to be the go-to method. But this only returns an integer, as the help text says:</p> <p>\"An integer representing the time zone of a date-time. In many cases, but not all, this integer indicates the number of hours which must be added to the time to get Greenwich Mean Time. May be positive or negative.\"</p> <p>An obvious example of where this is insufficient is for times in India, were the offset is three and a half hours. But with <code>.localTime</code> the timezone is appended, after a space. However, the Domino timezones don't neatly correspond to timezones as defined beyond Domino. Jason's code was a useful starting point, because it identified all the possible timezone strings that Domino could output. But it became apparent that it didn't handle the crucial element of daylight savings time. With some further digging I realised that the list - and the offsets Jason had used - came from an LDD article, thankfully reproduced online here. For someone who lives in the UK, I can tell you categorically that GDT does not have a 0:00 offset from GMT. It is one hour ahead of GMT. Similarly, other daylight savings time abbreviations - for example EDT, CEDT - do not have the same offsets as the standard time label. \"ZW\" and \"ZE\" are just offsets, west and east of the Greenwich Meridian. \"YW\" is more of a challenge and I can only guess this is 1 hour in advance of the corresponding \"ZW\" timezone, otherwise there is no reason to have a different abbreviation. So the table in the LDD article is, unfortunately, not correct, and it doesn't appear there is a correct published table. And it's also worth bearing in mind the offset is what to add to UTC, not how to adjust the timezone to return UTC. So here's the adjusted table I've used:</p> Domino Timezone Long Name Conversion to Return UTC RFC 3339 Timezone GMT Greenwich Mean Time Same as UTC Z YW1 Daylight savings for timezone one hour west of GMT Same as UTC Z ZW1 Timezone one hour west of GMT Add 1 hour -0100 YW2 Daylight savings for timezones two hours west of GMT Add 1 hour -0100 ZW2 Timezone two hours west of GMT Add 2 hours -0200 YW3 Daylight savings for timezone three hours west of GMT Add 2 hours -0200 NDT Daylight savings, Newfoundland Add 2.5 hours -0230 ZW3 Timezone three hours west of GMT Add 3 hours -0300 ADT Atlantic Daylight Savings Add 3 hours -0300 NST Newfoundland Add 3.5 hours -0330 AST Atlantic Standard Time Add 4 hours -0400 EDT Eastern Daylight Savings Add 4 hours -0400 EST Eastern Standard Time Add 5 hours -0500 CDT Central Daylight Savings Add 5 hours -0500 CST Central Standard Time Add 6 hours -0600 MDT Mountain Daylight Savings Add 6 hours -0600 MST Mountain Standard Time Add 7 hours -0700 PDT Pacific Daylight Savings Add 7 hours -0700 PST Pacific Standard Time Add 8 hours -0800 YDT Alaska Daylight Savings Add 8 hours -0800 YST Alaska Standard Time Add 9 hours -0900 HDT Hawaii-Aleutian Daylight Savings Add 9 hours -0900 ZW9B Nine and a half hours west of GMT Add 9.5 hours -0930 HST Hawaii-Aleutian Standard Time Add 10 hours -1000 BDT Bering Daylight Savings Add 10 hours -1000 BST Bering Standard Time Add 11 hours -1100 ZW12 Twelve hours west of GMT Add 12 hours -1200 ZE12C Twelve and three quarters hours east of GMT Subtract 12.75 hours +1245 ZE12 Twelve hours east of GMT Subtract 12 hours +1200 ZE11B Eleven and a half hours east of GMT Subtract 11.5 hours +1130 ZE11 Eleven hours east of GMT Subtract 11 hours +1100 ZE10B Ten and a half hours east of GMT Subtract 10.5 hours +1030 ZE10 Ten hours east of GMT Subtract 10 hours +1000 ZE9B Nine and a half hours east of GMT Subtract 9.5 hours +0930 ZE9 Nine hours east of GMT Subtract 9 hours +0900 ZE8 Eight hours east of GMT Subtract 8 hours +0800 ZE7 Seven hours east of GMT Subtract 7 hours +0700 ZE6B Six and a half hours east of GMT Subtract 6.5 hours +0630 ZE6 Six hours east of GMT Subtract 6 hours +0600 ZE5C Five and three quarter hours east of GMT Subtract 5.75 hours +0545 ZE5B Five and a half hours east of GMT Subtract 5.5 hours +0530 ZE5 Five hours east of GMT Subtract 5 hours +0500 ZE4B Four and a half hours east of GMT Subtract 4.5 hours +0430 ZE4 Four hours east of GMT Subtract 4 hours +0400 ZE3 Three hours east of GMT Subtract 3 hours +0300 ZE2 Two hours east of GMT Subtract 2 hours +0200 CEDT Central European Daylight Savings Subtract 2 hours +0200 CET Central European Time Subtract 1 hour +0100 GDT British Summer Time Subtract one hour +0100 <p> This is the basis of the code in Volt MX LotusScript Toolkit, so if you're aware of any errors, please let me know so all can benefit.</p>","tags":["Domino","LotusScript","Volt MX"]},{"location":"blog/2021/03/24/adventures-in-cache-land-1/","title":"Adventures in CacheLand 1","text":"<p>Recently I've been involved in a project with a lot of LotusScript. As a team our approach has been to structure the code according to best practices and leveraging what we've learned from other languages. Standards are always good, but there are always peculiarities that impact what you do. The crucial skill is to be able to work out what is happening when the standard ways don't produce expected results. And most importantly, work out how to work around them.</p> <p>One of the standards we chose was to use a lot of constants and store those in a single Script Library. That Script Library is then used in the other Script Libraries (and there are quite a few). This makes the structure cleaner. There are two constants that need to be changed during deployment, so it also simplifies where people need to go to make the changes.</p> <p>Unfortunately, we found that when changing the constants in Script Library A, the old values were retained by dependent Script Libraries B, C, D etc. We knew Script Libraries are cached. But it appears LotusScript Script Libraries effectively caches the constant value.</p> <p>So the code did not retrieve the value of the constant from Script Library A at run-time. If I remember correctly, we also tried closing a re-opening the Notes Client after changing, but I can't be certain. The conclusion we came to was that constants were cached at compile time of Script Libraries B, C, D etc.</p> <p>Of course recompiling all LotusScript would solve the problem. But there were reasons why that was not an optimal solution. Our working hypothesis was that changing to reference the constant in a function call would ensure it didn't cache the value. Because the constant was then only used within the Script Library being saved, caching would not be a problem.</p> <p>But rather than use a function, we used a Property statement. For those who have not used them, the resulting code is quite similar to a function:</p> <pre><code>Property Get VERSION As String\n    VERSION = g_VERSION\nEnd Property\n</code></pre> <p>The differences are you need to specify whether you are defining how to Get or Set the property. Then it's just a case of passing the constant value into the result of the property call.</p> <p>As expected, this solved the caching problem, but ensured changes just needed to be made in the Constant declaration. So no need to go hunting through code and make changes within the function or property get statement.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/03/31/adventures-in-cache-land-2/","title":"Adventures in CacheLand 2","text":"<p>In my last blog post I talked about challenges we had to overcome as a team with regard to caching of constants. But a bigger challenge we hit was caching of design elements.</p> <p>Part of the solution we built required copying design elements from one database to another. Part of the beauty of Domino is that everything is a Note - including design elements. Design elements are just Notes with a special flag. So just as you can copy a document from one database to another by getting a handle on the note, you can also copy a design element from one database to another by getting a handle on the design note. The API is exactly the same - <code>Call NotesDocument.copyToDatabase(targetDb)</code>.</p> <p>And that all worked fine for a start. But then we got the error message that someone else had edited the document. We were able to pinpoint the specific document throwing the error to an agent. It was being edited - but by the developer running the agent. And we were able to reproduce the error regularly - all the developer in question had to do was edit the agent.</p> <p>Our hypothesis was that the agent design element was somehow being cached. So the cached version was being retrieved, its underlying NotesDocument had been modified (by the developer running the agent!), and Notes was identifying a conflict and so throwing an error. With a bit of googling we found that some design elements like Script Libraries were cached (http://www.redbooks.ibm.com/redbooks/pdfs/sg245602.pdf, p95). But there was no reference here to agents. The redbook was written quite some time ago, in 2000 - and there is still some very advanced learning in there that should not be lost. So maybe agents are also cached now?</p> <p>Thankfully, when you are working with people who have been closely involved in the product for a very long time, there is a great deal of knowledge. One of our team (not me!) remembered that Notes was changed to cache agents back around the Notes 6.5 timeframe - the key bit of information required for the hypothesis to be feasible.</p> <p>So how to clear the cache? We tried various things. The solution may seem a radical one, but actually very simple. UNIDs are set automatically when you create a note, but they're actually read/write. So we updated the UNID for agents and script libraries at the start of the process...and then copy it. Immediately the error stopped occurring, and the code we needed to run worked consistently.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/04/12/goodbye-nathan/","title":"Goodbye Nathan","text":"<p>Over the weekend we lost another long-standing member of the Domino community, Nathan T Freeman. Nathan was outgoing, often controversial, but passionate about open source and helping others. Everyone who met him will have stories about him. But I know he is one of the individuals I have to thank for being where I am today.</p> <p>Back in 2011 I took two weeks holiday to learn Java in XPages. Ease of troubleshooting has always been a key concern of mine for the applications I build. So one of the parts of the project was to take Julian Robichaux's OpenLog Java Script Library and convert it to Java for XPages. My first pass sort of worked, but Nathan took my Java class and cleaned it up quite a bit. That is why he is credited on the project. He also helped with my fledgling Java skills through that project, answering questions very promptly, like how a NotesDatabase object could be null and still give a filePath property.</p> <p>When Nathan left GBS and set up RedPill, I welcomed the opportunity to get involved in OpenNTF Domino API. It was Nathan who tasked me with adding logging to ODA, something I wasn't sure I could do, but achieved it anyway. Over the years, our discussions and my work on ODA taught me a lot about how Domino works under the hood, knowledge that will be used over the coming years. But we also shared many good times at events around the world. Everyone knew when Nathan was in the room. Over the last couple of years I've missed his presence at those community events. I thought there was always a chance we would see him again, as Domino developed. Alas, no more.</p> <p>R.I.P. Nathan.</p>","tags":["Domino","OpenNTF"]},{"location":"blog/2021/04/13/lotusscript-declarations/","title":"LotusScript Declarations","text":"<p>How can you get a \"Type Mismatch\" error in a Forall loop in LotusScript?</p> <p>This was the question a few of us hit with a recent bit of coding. You can't declare the forall variable, and if you're iterating over a variant containing only strings, surely this shouldn't happen. The loop was quite basic:</p> <pre><code>ForAll element In me.getContent()\n    ' Do something\nEnd ForAll\n</code></pre> <p>The syntax was correct, so the next step was to look for something unusual. The one thing that was out of the ordinary was that the forall loop was duplicated within the sub - exactly the same code, so it was just copied and pasted. But the forall loops were within an if statement, and only one of them should ever be encountered. Stepping through in debug, only one of them was being encountered. So why the problem?</p> <p>Our first step was to put <code>me.getContent()</code> into a variable so we could step through in debugger and verify the datatype, so:</p> <pre><code>Dim values as variant\nvalues = me.getContent()\nForAll element In values\n    ' Do something\nEnd ForAll\n</code></pre> <p>Obviously we needed a different variable name for each loop. Because they were in an if statement, I suggested putting the dim within the if statement, so it only got declared if the code went into the if statement. Rocky (Oliver) pointed out though that every dim gets loaded at the start of the class. So there is no performance benefit in declaring variables within your code.</p> <p>This suggested the actual cause of the problem, that for some reason there was some problem converting the forall variable to a string, because the same variable name was used for two loops. Sure enough, changing the forall variable name in one of the loops solved the problem.</p> <p>Of course, the correct solution was to refactor the code so that the relevant chunk of code was moved to a function.</p> <p>After some time developing with Java, the obvious desire though was to be able to select the chunk of code, right-click and refactor automatically. But that's not possible. And while we do difficult immediately, the impossible takes a little longer.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/","title":"REST API Gateways","text":"<p>Three years ago I presented at IBM Think I presented a session called Domino and JavaScript Development Masterclass with John Jardin. In that session I talked about the various REST options for Domino and used the term \"API gateway\". Particularly at this time it makes sense to expand more on what was in the slides.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/#domino-as-an-application-server","title":"Domino as An Application Server","text":"<p>The usual approach with REST for NoSQL databases is direct CRUD access. And that has been done for Domino in the past with thing like Domino Data Services. But there's a crucial difference. Domino is not just a NoSQL database server, it's not just a database and indexes of documents. It's an application server.</p> <p>Let me explain the impact of that. It means with the same authentication, access can be via a Notes Client, via HTTP (in various technologies) and via gRPC if App Dev Pack is installed. More crucially, by default the database can be accessed via all three protocols - which is why some Domino customers are reluctant to turn on the Domino HTTP task. And HTTP access could be via a web application, a custom REST service, a programmatic service via agents or even web services. Whereas a traditional NoSQL server may be locked down to certain servers or even locked down behind a firewall, that's not likely to be an option for Domino.</p> <p>Even more critically, the same Domino database can often be intended to be used via user interfaces. A REST service may need to be built for an NSF that already has a Notes Client front-end. It may also have a web UI in some format as well.</p> <p>But Domino isn't just a NoSQL database. It can contain business logic stored quite literally at the same level as the data. After all, agents and script libraries are stored in design notes, just as the data is stored in design notes. That can be leveraged and typically would be leveraged by Domino developers. To ignore that undermines what Domino is.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/#api-gateway","title":"API Gateway","text":"<p>This all means Domino needs to have a REST API gateway. Authentication is not enough. Validation is required. The use of computed and computed for display fields means some pre- and post-processing may be required. The ability to leverage some of that business logic may be relevant.</p> <p>This is critical whether or not the Domino developer is building the web application that uses the REST API, because rarely will it be possible to secure the Domino server and database so that the web application is the only REST entrypoint. It may be unlikely for REST to be leveraged outside of a provided web UI, but it's not impossible. And no in-built method has given Domino developers the tools to be certain that REST access is not via e.g. Postman. It may seem unlikely, but as a developer, I was never satisfied with relying on that.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/#division-of-responsibility","title":"Division of Responsibility","text":"<p>And this leads me onto the next critical point from my experience working for a business partner. In most cases, the Domino developer was not the one building the web application that consumed the REST service. That was also the case for IBM Think 2018 - I built the Domino REST API gateway, John Jardin built the web application.</p> <p>But who is responsible for supporting data integrity issues? It's not the person building the web application, it's the Domino developer.</p> <p>So who should be responsible for ensuring data integrity? The Domino developer.</p> <p>And if you're wondering how important that is, think of the REST service in this way. It's basically an import agent that third parties can call. And how often has any Domino developer created an import agent with no validation in it?</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/#how-to-code-the-api-gateway","title":"How To Code The API Gateway?","text":"<p>Historically, advanced XPages developers like myself may have created an OSGi plugin using Apache Wink. Although it's a dead open source project, it's the only REST API technology that comes out-of-the-box with Domino HTTP. For less experienced XPages developers, XAgents can also be done - providing developers are comfortable using Apache Wink. Domino developers who know NodeJS or Java can also use the App Dev Pack, if they have installed all the components required for authenticated access. Another option is SmartNSF, if developers are brave enough to try Groovy.</p> <p>But what proportion of Domino developers fit those categories?</p> <p>The Volt MX LotusScript Toolkit is an option almost all Domino developers could leverage. But it means the entry point is always agents. It's far from complete, even though some enhancements were made for DOMI.</p> <p>And whichever of those options is chosen, all still require skills in writing an OpenAPI specification for a third-party to use it. None of the options allow auto-generation of an OpenAPI spec, which is what modern web developers will expect. The OpenAPI spec then needs hosting, although I did write a blog post on how to achieve that.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/06/13/api-gateway/#dont-code-it","title":"Don't Code It!","text":"<p>The best answer is to allow Domino developers to configure an API gateway rather than code it. Domino REST API, a.k.a Project Keep, can be used by any Domino developer. If customers have hired JavaScript developers, they will also have the skills to use it. More importantly, if Domino is ready to take to non-Yellowverse conferences, it can also be used by any developer of any language. It only exposes the NSFs - and parts of those NSFs - that the developer chooses. And it exposes - out-of-the-box - an OpenAPI spec. For LotusScript developers it allows agents to be leveraged.</p> <p>And most importantly, it keeps control in the hands of the developers who will have to support the data.</p> <p>It won't cover all scenarios, which is why extensibility is still important. And although integration with third parties is a clear use case, there is still education required on building a web application outside of HCL technologies. That is actually an easier discussion with whitespace markets.</p> <p>Whether it's an effective REST API tool is unclear. But the right REST API has to provide an API gateway that existing and new developers can use.</p>","tags":["Domino","LotusScript"]},{"location":"blog/2021/12/12/docker-java/","title":"Docker, Java and Processes","text":"<p>Whether it's Docker or Kubernetes (or some flavour thereof), running microservices in containers is a powerful option. It can make it easy to deploy development or test systems, ensuring consistency across your development team. But when you move from consuming to building, particularly when you're building less out-of-the-box containers, there are some deeper elements that you need to be aware of.</p>","tags":["Java","Docker","Containers"]},{"location":"blog/2021/12/12/docker-java/#cmd-vs-entrypoint","title":"CMD vs ENTRYPOINT","text":"<p>At the end of your Dockerfile, there will be a command for how to start the container. There are two options here:</p> <ul> <li><code>CMD</code> followed by a command to run and parameters to pass to it.</li> <li><code>ENTRYPOINT</code> passing a file to run using the default shell.</li> </ul> <p>For those who are not Linux masters, it's important to bear in mind that the default shell differs for different Linux base images. If your starting image is Ubuntu, the default is dash (Debian Almquist Shell) and possibly also in Debian. This may have unexpected consequences. If bash is in use on all base images you're using, it may be preferable to use <code>CMD [\"/bin/bash\", \"myfile.sh\"]</code>.</p> <p>There is a more significant point with regard to how the Java program runs and how stopping the container interacts with it.</p> How Started Process / Sub-Process SIGTERM Received By ENTRYPOINT, <code>java -jar</code> Sub-Process Shell process ENTRYPOINT, <code>exec java -jar</code> Process Java JVM CMD [\"/bin/bash\"], <code>java -jar</code> Sub-Process bash CMD [\"/bin/bash\"], <code>exec java -jar</code> Process Java JVM CMD [\"java\"] Process Java JVM Google Jib image Process Java JVM Lightweight init system, e.g. \"dumb-init\" Process Java JVM <p>Again for those who are not Linux experts, in the entrypoint, <code>java -jar myJar &amp;</code> will ensure the stdout goes to the container log. If switching to <code>exec java -jar</code> you need to remove the <code>&amp;</code> at the end.</p> <p>If the Java JVM is running as a sub-process, by default it will not receive the SIGTERM signal that shuts down the container or other signal. So if you want to shut down your Java program gracefully, the shell script needs to pass on the signal. There are various ways to do that, and the right way may depend on Java program.</p> <p>If the Java program is the main process, it will receive the SIGTERM signal. You can catch that by adding a <code>Runtime.getRuntime().addShutdownHook()</code> and action accordingly. This, for example, is how Vertx catches the SIGTERM signal and calls the <code>stop()</code> method of each verticle to gracefully shut them down.</p> <p>You may want to confirm which processes are running in the container and which is the main process. I found this StackOverflow question which gave the answers: <code>docker &lt;container id&gt; top</code> lists all processes, and <code>docker inspect -f '{{.State.Pid}}' &lt;container id&gt;</code> gives the main process.</p>","tags":["Java","Docker","Containers"]},{"location":"blog/2021/12/12/docker-java/#java-as-the-sub-process","title":"Java As The Sub Process","text":"<p>However, there are scenarios where, even though Java is the main process and a shutdown hook has been added via <code>Runtime.getRuntime().addShutdownHook()</code>, it doesn't trigger. I had this recently and, although I was able to identify the code that caused this, I wasn't able to completely understand why. The symptom was that no output was printed from the <code>stop()</code> method of Vertx verticles and a SIGKILL was sent to the container, killing the JVM. The status for the Docker container shutting down was 137, which means the SIGKILL was sent. If the container was created with <code>stop-timeout=-1</code>, to set no timeout when sending the SIGTERM, and so never send the SIGKILL, the container could not be stopped.</p> <p>In this scenario, to get around the problem of the shutdown hook not working, the solution was to not run the Java program as the main process.</p> <p>Then the shell script needs to remain running, trap the SIGTERM, and shut down the Java program. Daniel Nashed's Docker entrypoint script for Domino gives a good example of how to do those first two points. The first part is the loop at line 120:</p> <pre><code>while true\ndo\n  sleep 1\ndone\n\nexit 0\n</code></pre> <p>This triggers an infinite loop at the end of the shell script. The final line's <code>exit 0</code> never gets reached.</p> <p>The second part - catching the SIGTERM - is covered at line 64, <code>trap \"stop_server\" 1 2 3 4 6 9 13 15 19 23</code>. The <code>trap</code> command receives a function name to trigger when the signals are trapped (\"stop_server\"), and a list of signals to trap. This catches a variety of signals and (thanks to Daniel for the information) these are listed in a table halfway down the man page on signals. The key ones is 15 (SIGTERM).</p> <p>The \"stop_server\" function is at line 44. Any <code>echo</code> commands will also print to the container's log, so you can verify that your function has triggered. Once you have shut down your Java program - that will depend on your Java code - calling <code>exit 0</code> will cleanly shut down the container.</p>","tags":["Java","Docker","Containers"]},{"location":"blog/2021/12/12/docker-java/#summary","title":"Summary","text":"<p>It took me quite some time to troubleshoot and understand all of this. But it has given me a much deeper understanding of containers, which I'm sure will be important for the future.</p>","tags":["Java","Docker","Containers"]},{"location":"blog/2022/02/05/rancher-desktop/","title":"Rancher Desktop, A New Dev Tool","text":"<p>Docker has been a significant development tool for me for some time. The ease of spinning up a clean, standalone development environment for applications is a great benefit. The ability to switch seamlessly between different versions is a big benefit when testing. Another benefit is the ability to create demo environments for conference sessions and share them via GitHub for others to easily deploy samples.</p> <p>But when Docker announced new licensing terms last year, it shook up the desktop development landscape. Rancher, who have a long history of expertise with Kubernetes, stepped into the desktop game by announcing Rancher Desktop. I was aware of the open source project last year, but when Daniel Nashed pointed out to me that 1.0 had recently been released, I decided it was time to give it a try.</p> <p>First it's worth giving a little background on my experience of containerisation. I've had virtually no exposure to Kubernetes on desktop or production. My knowledge of Kubernetes in a DevOps environment is probably less than my knowledge of kubernetes in the classical world, being the first declension Greek noun for a helmsman which was adopted and modified in Latin as the noun gubernator. I've used Docker for a number of years. Initially I did a lot of work via command line, both building containers and images, heavily using volumes and networks, and writing dockerfiles. More recently, most of my work with Docker containers has been via the excellent VS Code plugin. Although I still use command line for creating containers and connecting a shell as root user, most of my interaction is via the VS Code menus and so my Docker CLI fu has slipped a little.</p> <p>Rancher Desktop is available for all operating systems - Mac (both Intel and Silicon), Windows and Linux. For those running Kubernetes in production and wanting to mimic their production environments, the ability to choose the Kubernetes version running will be a big advantage. The lay developer will probably not change from the default. The big decision comes on the container runtime - containerd or dockerd. Developers like me who have never used Kubernetes may be lost here. The good news is you can switch between the two.</p> <p>If you are used to Docker, don't have to worry about deployment to Kubernetes environments and don't want to step outside your comfort zone, then dockerd will be the go-to choice. This allows you to use the docker commands on the command line that you're used to. I believe you can also use Docker Compose, though I've not tried that yet. Those who use the Domino Docker image will find this very essential, though I have no doubt that the community project will expand its scope to use helm in the future as well. Even better, the Docker plugin for VS Code works seamlessly with it. This is not too surprising when you realise that it's using Moby, the open source framework from Docker for containerisation.</p> <p>There are a couple of caveats here. The first is that Rancher Desktop can only create bind mounts to directories in your Users directory on Mac. If you try to create a bind mount to another location (I have files on an external SSD), it will create the mapping, but the container will not see any of the files. Creating a symlink may work, but I had already moved the files. The other caveat is that uninstalling Docker Desktop on Mac did not remove the symlinks in /usr/local/bin, which means Rancher Desktop could not initially be set as the source for usr/bin/docker. However, that was resolved by manually removing all remaining symlinks to Docker in usr/local/bin.</p> <p>If you are likely to have to deal with containers in production, you should probably use this as an opportunity to get experienced with running containerd and nerdctl / helm. At this point I must emphasise that I'm not experienced with them, but it's definitely on my development roadmap. Once you switch the container runtime to containerd then docker commands are no longer available and the Docker plugin for VS Code will not work.</p> <p>Instead of docker on the command line, you can use nerdctl, the Docker-compatible CLI for containerd. This will probably be the initial starting point for those coming from Docker Desktop and switching to containerd. According to the documentation nerdctl also supports Docker Compose, although again I've not tried that yet. On the whole, for every command that you would previously have prefixed with <code>docker</code>, you can prefix with <code>nerdctl</code>. However, it's worth bearing in mind that some flags are not supported. For those coming from the Domino world, one big flag that will not work is <code>stop-timeout</code>. Another key point to be aware of is that if you start a container using <code>nerdctl run -it ...</code>, you cannot shut it down and subsequently restart it. In this scenario, the approach I found recommended and which worked for me was to use <code>nerdctl run -d ...</code>.</p> <p>As mentioned, you can quickly switch between dockerd and containerd as the container runtime. But bear in mind that the images and containers you create with one runtime won't be available when you switch to the other.</p> <p>It's worth bearing in mind that Rancher Desktop is still very new. It was only started late last year. Documentation is an area that will be improved over coming months. But the community is very active, with a Slack channel available and issues already regularly being created. The demand is certainly there for Rancher Desktop and the use of Moby means dockerd is a first class citizen. But, as mentioned previously, it is certainly a good time to build your skills on containerd and helm.</p>","tags":["Docker","Containers"]},{"location":"blog/2022/03/10/ls-profile/","title":"LotusScript Profiling","text":"<p>LotusScript agent profiling is not new, but there are still some developers who are not aware of it. It's something I blogged about more than ten years ago. At that time talking about the relative performance of specific API calls, in that case the relative performance of checking <code>.count</code> was greater than zero vs getting the first entry and checking if it was nothing. On other occasions it's also identified mistakes in my code, because it demonstrated more API calls than I expected or needed.</p> <p>But sometimes it's not a single API call that affects performance, sometimes it's a combination. That's the scenario I came across recently.</p> <p>Unlike a lot of databases, Domino has two identifiers for a document, the UNID and the Note ID. The UNID is the key one when code needs to work across servers, and so it's typically what's stored in documents as a foreign key. The Note ID is shorter but is only specific to a document in this particular replica. On a different server, the same document's Note ID will be different. Some APIs will give you the UNID, but there are a number of APIs that return a NotesNoteCollection, which is a list of Note IDs.</p> <p>Both can be used to access the document. The following code sample demonstrates the usage of both. Line 55 gets the document by UNID, line 56 gets it by Note ID.</p> <p></p> <p>You might not think there is much difference in the performance of those two lines. You would be right.</p> <p>But would you expect that the performance of the overall agent differs by almost ten seconds depending which one you choose? That may seem astounding, but it was true. Agent profiling proves it, but also gives the reason. This is the agent profiling document for <code>getDocumentById()</code>.</p> <p></p> <p>And this is the agent profiling document for <code>getDocumentByUNID()</code>.</p> <p></p> <p>There are additional API calls, because it's also profiling the retrieval and writing to the agent profiling document itself, but those have minimal impact on performance. The key is in the highlighted lines. As I mentioned, the performance of <code>getDocumentByID()</code> vs <code>getDocumentByUNID()</code> is very similar. But it's the <code>NotesDocument.LastModified</code> which has a massive impact on performance.</p> <p>Does this mean you should use <code>getDocumentById()</code> if you need to get the last modified date? I doubt it. Most likely is that the modified date is accessed and cached via another API call, so retrieving it does not require additional reading from disk. My guess would be that it's retrieved by the method of getting the documents - in this case <code>NotesDatabase.GetModifiedDocumentsWithOptions()</code>. That seems more plausible than it being cached by <code>getDocumentById()</code>.</p> <p>But this highlights how performance of your code can be impacted by not just by performance of individual API calls, but by combinations that make up your whole code. It's always worth using agent profiling. You may find you learn something new, something you didn't expect.</p>","tags":["LotusScript","Domino","Performance"]},{"location":"blog/2022/06/10/voltscript-a-unique-opportunity/","title":"VoltScript - A Unique Opportunity (Paul Withers and Jason Roy Gary)","text":"<p>At Engage 2022 Volt MX Go was announced including features like Volt Formula, a JavaScript-based fusion of Notes formula syntax and Open Formula, and VoltScript, a derivative from LotusScript, and inspired by many modern implementations of BASIC, which will run in Foundry, Volt MX Go's middleware layer. Already at Engage we demonstrated live running code of VoltScript with Try/Catch/Finally, an alternative declaration keyword \"Def\" for \"Dim\" and deprecation of GoSub; which let\u2019s be honest is Satan\u2019s spawn. These are the first changes to the core LotusScript language keywords in over 30 years. In addition, we showed live demos of code running triggered from Foundry as well as standalone VoltScript outside of HCL Notes or Domino, for the first time since the end of life of Lotus 1-2-3. We also showed a number of new extensions (LSXs / VSXs) that will obviously be required, as well as developer productivity tooling like unit testing, mocking and a POC of dependency management.</p> <p>Already we have created more enhancements than were made when LotusScript was added into Lotus Notes from the rest of the Lotus SmartSuite family of products. But this is just the start - both for language enhancements and developer tooling. The intention to bring an additional backend language to Volt MX that will be familiar not only to Domino developers but also VB6 and VB.Net developers, modernised and enhanced for the current development landscape. We already have a wish-list of enhancements - Big Hairy Audacious Goals probably beyond what many can conceive. And of course, we have also looked at what is already on Aha. Will we achieve them all? Who knows? But we will dare to try, and perhaps even catch.</p> <p>Very few software engineers in our limited but highly colourful history have had the opportunity to create a new programming language. And yet, we have that chance along with an incredible legacy of LotusScript and its millions-upon-millions of running lines of code to do something that will truly inspire our customers and partners. And just as this is a once-in-a-lifetime opportunity for the research team tasked with the project, it's also a once-in-a-lifetime opportunity for the community. This is a call-to-arms to tell us what you think we should add.</p> <p>Bear in mind that the target runtime for VoltScript is not HCL Notes or Domino and the deployment target is not an NSF; VoltScript is designed to run anywhere it needs to in the HCL portfolio. From personal experience, we know this is a challenging change of mindset. For example, lsxbe (the LSX containing the Notes backend classes) will obviously not be available.</p> <p>We know many of you have already asked whether everything will be ported back to LotusScript. Once the non-Domino mindset is achieved, it becomes apparent that not all the new changes we need to make will be appropriate for Domino. Some work will be necessitated because the code is being deployed to and triggered from Volt MX Go\u2019s Foundry. That also means different tooling options, which opens up opportunities that may not be appropriate when the development IDE and target build artefact is an NSF. And some opportunities, like mocking Notes classes, are not possible when the actual Notes classes are already automatically added by the runtime. Some changes may be candidates for porting to LotusScript, but we are a research team for Volt MX Go, not a product maintenance or core engineering team. So, we are not the correct engineers to make such decisions and now is probably not the right time for such decisions.</p> <p>However, it cannot be understated how incredible VoltScript will be. Today\u2019s landscape is cluttered with low and mid-code tools and platforms that all require users, and in many cases business users, to be dropped to a text editor and expected to type working Java, JavaScript, or C#. We aim to change that just as our predecessors with Notes version 4 did with LotusScript more than 20 years ago. We will give inventive and adventurous non-developers the opportunity to create their own applications using a programming language that is easy to understand, simple to test, and painless to extend to the limit, and beyond, of their imagination.</p> <p>This is not an existing product, so Aha is not the appropriate route to share your feedback. We don't intend to choose how VoltScript evolves based on the most popular requests as there are different priorities based on minimum viable product and skills required to implement.</p> <p>In the coming weeks we will share here and elsewhere our current list of features we wish to implement. Contact Paul Withers through various social media or OpenNTF's Discord or Slack if you want to be a part of what will prove to be yet another adventure.</p>","tags":["VoltScript","Volt MX Go"]},{"location":"blog/2022/07/02/reproducers/","title":"The Importance of Reproducers","text":"<p>So you think you've found a bug. What next? Create a support ticket, right? Wrong!</p> <p>First off, let's point out something critical in that first line - \"you think you've found a bug\". You may be a consumer for the code you think you've found a bug in. But you're also a committer to other code. How many times has someone raised a problem where the code is actually working correctly? How many times has someone raised a problem but given insufficient evidence to know what's going on, resulting in the famous \"works for me\"? And how many times was it PEBKAC (problem exists between keyboard and chair)?</p> <p>And let's be clear. Everyone is capable of PEBKAC errors. There's a reason the \"Midvale School for the Gifted\" cartoon by Glen Larson is so famous.</p> <p>But even if you're 99% certain there's a bug, even if you can consistently reproduce with your code, still stop.</p> <p>Open source projects typically use GitHub Issues. If you've raised issues on good open-source projects, you've seen a section that says \"Steps to Reproduce\". If you maintain open source projects, you should be creating those GitHub Issue Templates that include an area for \"Steps to Reproduce\".</p> <p>That's good, but a reproducer is better.</p>","tags":["Support","Testing","Java","LotusScript","Domino","Vert.x","Voltscript"]},{"location":"blog/2022/07/02/reproducers/#why-should-i-bother","title":"Why Should I Bother?","text":"<p>If you've had to support an open source project, hopefully you should not be asking this and you can skip this section. But if you are asking this question, you need to read this.</p> <p>Let's put open source to one side for the moment. With open source you have the right to expect what you pay for - nothing.</p> <p>But if it's a paid product, and you have a support contract, you're paying for the right to raise tickets and the right to expect a technician to spend some time looking at the problem. You're not paying for the right to get fixes. That's what your renewals pays for. But renewals only pay for fixes and new features the vendor chooses to deliver.</p> <p>If you want a fix, there's a quid pro quo. You're expecting someone who owns the code to change the code. More importantly, you wanting the right fix, one that's fully thought through and QA'd to avoid or at least minimise knock-on effects. That takes time, time that - whether you want to accept it or not - you are not paying for.</p> <p>If you want the owner of the code to put in the time, show your respect for them by putting in some time yourself. And you can greatly minimise their time by creating a reproducer.</p> <p>A reproducer doesn't give you the right to expect a fix. It doesn't even give you the right to expect a workaround. Providing a reproducer creates a favourable impression with the owner of the code, increasing the likelihood of receiving the most positive response - not only for this support ticket, but also for future ones.</p>","tags":["Support","Testing","Java","LotusScript","Domino","Vert.x","Voltscript"]},{"location":"blog/2022/07/02/reproducers/#purpose-of-a-reproducer","title":"Purpose of a Reproducer","text":"<p>A reproducer should allow the owner of the code to quickly reproduce - and thus cleanly troubleshoot - the cause of a bug. It should:</p> <ol> <li>Allow the symptom to be consistently reproduced on any device, or at least any device of a specific type.</li> <li>Minimise additional code.</li> </ol> <p>The first avoids the dreaded \"works on my machine\" response. It can also allow you to prove that before raising the ticket. The second minimises other possible causes or distractions.</p> <p>Together the code provides a simple test case - or set of test cases - to debug, verify the bug and identify the cause.</p> <p>Additionally a reproducer could include code that tests other scenarios, to help the technician pinpoint the cause. I've been in this scenario, which I covered in a blog post on GetAllDocumentsByKey with Doubles. In this case I experienced it with Java in XPages. I provided a reproducer specifically for XPages, but also a reproducer in a Java agent. This was specifically to rule out that the problem was specific to XPages. Yes, that was highly unlikely. But the reproducer allowed anyone picking it up to know that within seconds.</p> <p>The result of all of these should be fewer questions from the support technician and quicker agreement that the code is doing what you think. Note, I did not say \"agreement that it's a bug\". You have code that is not behaviour as you expected. The code may be working as it should because of other factors you had not considered when expecting a specific outcome. Your expectations may even have been correct, but the code is doing what the owner of the code wants it to. Or it may be a bug, meaning the code is not behaving as the owner wants it to. But the reproducer ensures you get to a conclusion as quickly as possible.</p> <p>With open source that conclusion may be you providing a pull request - for code or documentation - that gets accepted more quickly. Or the conclusion may be not wasting your time on a pull request, or creating a fork to make it work the way you want it to. Whatever the outcome, the reproducer will increase the chances of a swift solution and avoid a support ticket going into \"discussion hell\", wasting your time as well as the support technician's.</p> <p>Why is time important? Because time spent on support is time that isn't spent on other things. For open source, which is often done in personal time, that's very important. But it's also important for products.</p> <p>As we're working through VoltScript, I'm both owner and consumer of code, as well as the person responsible for delivering the whole project. If it takes time to verify if the code is working as we want, that's time lost on adding new features. And so I'm always trying to include a short reproducer when querying whether code is functioning correctly.</p> <p>Of course, there is also another advantage of a reproducer. It will enhance your reputation with the support technician, improving your credibility. If you're just a consumer, that will be a benefit for future support issues. If you're working together on a product, it's even more important.</p>","tags":["Support","Testing","Java","LotusScript","Domino","Vert.x","Voltscript"]},{"location":"blog/2022/07/31/ls-classes-1/","title":"LotusScript Classes Deep Dive Part One","text":"<p>Classes have always been a part of LotusScript, an aspect used heavily in many open source projects. Recently I came across an OpenNTF project OpenDOM, which not only includes a number of sophisticated classes and design patterns in its design, but also in the UI covers a variety of projects that provide Object Oriented extensions for Domino. Unfortunately, the project seems to be one of many on OpenNTF that have become unsupported, and many of the projects it points to have either been lost or are unsupported. With so few people in the community covering these kinds of development topics in blogs or conference sessions, I wonder how much knowledge on more advanced topics has been lost. Indeed over the last few years I have also learned things about LotusScript that I did not know before, as I have had to create more sophisticated LotusScript / VoltScript functionality. So this blog series is intended to explicitly share some of that knowledge, specifically relating them to experience of other languages.</p> <p>To give some context, everything that will be covered about LotusScript will also be relevant for VoltScript, although there may be additional functionality available.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#what-is-a-class","title":"What Is A Class?","text":"<p>In Java, all code has to be in classes. However, for our languages, like in Visual Basic or JavaScript, functions can be outside of classes and called just using the function name. This means you don't have to create classes as often, particularly in Domino where you will typically be running server-based functions in agents or triggering functions from events. And when interacting with data and resources, the in-built classes of lsxbe (Notes... classes) and lsxui (NotesUI... classes) provide adequate object-based support for many. But a Class is basically a template for how you will interact with individual data items as a related collection of variables, instead of just using unrelated variables. Consider the following code.</p> <pre><code>Dim firstName as String\nDim lastName as String\nDim age as Integer\n</code></pre> <p>You could repeat those variables for every function where you wish to use them. Or you could create a Class.</p> <pre><code>Class Person\n    Public firstName as String\n    Public lastName as String\n    Public age as Integer\nEnd Class\n</code></pre> <p>Classes are more effort. But if the variables are related and repeated, typically there are functions you need to run that are also repeated and related. The Class gives two obvious benefits:</p> <ol> <li>The functions are stored with the class they act upon.</li> <li>The functions have direct access to the variables they need to use.</li> </ol> <p>There is a variety of tooling opportunities that can also help make Classes more attractive. IDEs can provide some of that or, with the code as text files, any developer could contribute such functionality with imagination and code conventions.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#scoping","title":"Scoping","text":"<p>Classes and Class members (variables, subs, functions, properties) have a scope. LotusScript / VoltScript provides two options - Public or Private. Public means they are accessible within the current script file or any that references it. Private means they are only accessible within the current context. For a Class that means it is only accessible from the current script file. For Class members (variables, subs, functions, properties) that means within the current Class.</p> <p>The default scoping can be managed with the <code>Option</code> keyword, and Domino Designer now automatically adds <code>Option Public</code> to Agents and Script Libraries.</p> <p>Other languages have other modifiers. Java has Protected, which means a class is accessible to other classes that are siblings. Visual Basic has Protected, Friend, Protected Friend and Private Protected. There are also a variety of other keywords relevant to classes, like Shadows, MustInherit and NotInheritable. But LotusScript / VoltScript retains a less complex set of options and this simplicity arguably ensured the language was more accessible to developers who did not come from a Computer Science background.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#members","title":"Members","text":"<p>The class can contain:</p> <ul> <li>Variables</li> <li>Properties</li> <li>Subs</li> <li>Functions</li> </ul> <p>Visual Basic and Java allow nested classes, so a Class can contain the definition for another class. That is not the case in LotusScript or VoltScript.</p> <p>All can be scoped by being prefixed with the keywords Public or Private. Variables must be prefixed with Public or Private, for other members it is optional. However, bear in mind that based on my testing, if a Class is Public, then properties, subs and functions contained within it are by default public. This is actually consistent with what Visual Basic also does, according to its documentation.</p> <p>Variables, Subs and Functions are well known to any LotusScript developer. Properties are probably less well-known, but are quite simple.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#properties","title":"Properties","text":"<p>Any developer who has worked with Java or JavaScript understands what properties are, the terminology is used for what LotusScript refers to as member variables. And they work exactly the same in LotusScript, the same as member variables. But there are particular reasons to use them:</p> <ul> <li>Properties can be getters, setters or both. This means you can make them Read Only, Write Only or Read-Write. Of course a read-write property works exactly the same as a variable.</li> <li>Properties can take parameters</li> </ul> <p>A property that takes parameters does not have an obvious use case. The example in the documentation allows an increment to be passed. I've not used a Property Set with parameters and I don't remember seeing one. The other thing to bear in mind when using properties is that the getter and setter need to take the same parameters.</p> <p>However, the option to make properties read only or write only is particularly useful, and I'm sure developers can think of good use cases for that. There is an easy one to demonstrate with our Person class.</p> <pre><code>Class Person\n    Public firstName as String\n    Public lastName as String\n    Public age as Integer\n\n    Public Property Get fullName as String\n        fullName = Me.firstName &amp; \" \" &amp; Me.lastName\n    End Property\nEnd Class\n</code></pre> <p>As ever with Domino, there are multiple ways to achieve the same outcome, and many developers may typically return a full name by using a function. But using a property makes a neater approach. As we move forward into the world of VoltScript, I can also see other valid use cases for properties.</p> <p>When it comes to a Property Set, the important thing to bear in mind with the syntax is that the developer does not explicitly name a parameter that the setter receives. The parameter it receives is the name of the property itself.</p> <pre><code>    Private storedFullName as String\n\n    Public Property Set fullName as String\n        Me.storedFullName = fullName\n    End Property\n</code></pre> <p>This also demonstrates that the property name (\"fullName\") cannot also be used for a variable. The variable (\"storedFullName\") needs to be different.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#subs-and-functions","title":"Subs and Functions","text":"<p>Each function name must be unique. The same function name cannot be used for different sets of parameters. That is possible if the class is written in an LSX, but not possible if the class is written in LotusScript / VoltScript. Classes can be overloaded in Java, JavaScript or Visual Basic, but not in LotusScript / VoltScript.</p> <p>The other difference is that subs or functions in LotusScript / VoltScript can not have optional parameters. Again, that is an option in Java, JavaScript and Visual Basic.</p> <p>This can provide some limitations, particularly in certain circumstances. But in most scenarios, with a little imagination, it is possible to work around it.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#delete-sub","title":"Delete Sub","text":"<p>The <code>Delete</code> sub is a special procedure automatically called when the runtime deletes an instance of this class. When that happens will depend on the scope of the instance. But this can be used to run special functionality. In Volt MX LotusScript Toolkit I used this technique for two useful purposes - erasing Lists that were member variables and calling the <code>closeRequest()</code> sub to send the HTTP response. These are probably the most common use cases for code in the Delete sub.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#referencing-the-class-instance","title":"Referencing the Class Instance","text":"<p>This code demonstrates a final point about classes for this blog post - referencing the current instance of the class. The keyword <code>Me</code> can be used to reference variables, properties, functions and subs in the current class, although Domino Designer does not enforce this convention.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/07/31/ls-classes-1/#links","title":"Links","text":"<p>Domino Class documentation</p> <p>Domino Property Get / Set</p> <p>Visual Basic Class documentation</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/","title":"LotusScript Classes Deep Dive Part Two","text":"<p>In the last part I covered the basics of what constitutes a Class in LotusScript / VoltScript. I also said that user-defined classes in LotusScript are often avoided because developers can interact with their data via the platform classes in lsxbe (Notes...) and lsxui (NotesUI...). When classes are used in proprietary applications they may typically be very straightforward - standalone classes with no inheritance. But there is much more possible, as Java developers will be aware of.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/#interfaces-and-abstract-classes","title":"Interfaces and Abstract Classes","text":"<p>Java developers will be aware of interfaces and abstract classes. Neither can be used directly by code, the compiler forces the developer to create a class that implements an interface or extends an abstract class.</p> <p>Interfaces can only contain variable declarations with a value defined. In older versions of Java, interfaces could not contain code, they could only contain method declarations - the method, its scope, the parameters it takes and its return type. Since Java 8 interfaces can also contain default implementations of methods.</p> <p>On the contrary, Abstract Classes can contain variable declarations. It can contain method declarations, if the methods are also declared as <code>abstract</code>, or it can contain actual method implementations.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/#lotusscript-voltscript","title":"LotusScript / VoltScript","text":"<p>LotusScript and VoltScript have a simpler structure, with just classes. But there are steps a developer can take to follow the design patterns from Java, although probably not regularly used.</p> <p>Subs and functions are automatically default implementations. However, it's also possible to have a sub or function without any implementation using the Declare keyword.</p> <pre><code>Class Animal\n    Declare Function makeNoise() as String\nEnd Class\n</code></pre> <p>Developers who have used On Disk Projects will be familiar with this, because the IDE automatically generates the forward declarations for Classes, Subs and Functions in Script Libraries. But developers can add their own <code>Declare</code> statements without actual implementations of the functions. This doesn't directly reproduce the functionality of interfaces, because neither the IDE nor the compiler force implementations of these declarations in sub-classes. Also, the IDE and runtime will allow different signatures for the same function name in a sub-class. So a sub-class can have <code>Function makeNoise(noise as String) as String</code>. But it can be used to indicate expected functions and subs for sub-classes, whether that's for developers looking at the class itself or in documentation, if you have tooling that automatically generates documentation for your classes.</p> <p>When it comes to constructors or abstract functions, it's not possible to generate compiler errors if sub-classes do not provide implementations. However, it is not difficult to generate run-time errors.</p> <pre><code>Sub New()\n    Error 1501, \"Not Implemented\"\nEnd Sub\n</code></pre> <p>This is a technique I've used, the error number echoing the HTTP Status 501, Not Implemented. This should certainly highlight during development if the developer has not implemented a function that needs to be implemented.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/#base-and-derived-classes","title":"Base and Derived Classes","text":"<p>When you're building classes for an application, there may be little benefit in creating classes that depend on one another. But when you're building tooling or open source projects, there are more reasons to build classes that are intended to be sub-classed. And when it comes to sub-classes in LotusScript / VoltScript, there are a few points to bear in mind.</p> <p>Firstly, if the base class - the class you are sub-classing - is written in C/C++ in an LSX, you cannot create a derived class from it. This is why you cannot create classes that extend e.g. NotesDocument, part of lsxbe, which was the first LSX. You can only create a derived class in the same language as the base class. So a class written in an LSX can be sub-classed in an LSX, a class written in LotusScript can be sub-classed in LotusScript, but a class in an LSX cannot be sub-classed in LotusScript.</p> <p>Secondly, in LotusScript / VoltScript functions and subs in a derived class must have the same signature as in the derived class. They must have the same parameters in the same order, and return the same type of variable. This should not be a surprise, bearing in mind function names cannot be overloaded. The <code>New</code> sub is an exception, as we will see shortly.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/#constructors","title":"Constructors","text":"<p>Creating the derived class is straightforward, <code>Class DerivedClass as BaseClass</code>. And if you don't need to call the base class's constructor, creating the constructor is also straightforward, exactly the same creating a normal constructor. But sometimes you may want to call the base class's constructor. Then the syntax gets a little more sophisticated.</p> <pre><code>Class Animal\n    Private name as String\n\n    Sub New(animalName as String)\n        Me.name = animalName\n    End Sub\n\nEnd Class\n\nClass Dog as Animal\n\n    Sub New(petName as String), Animal(petName)\n\n    End Sub\n\nEnd Class\n</code></pre> <p><code>Dim dog as New Dog(\"Baxter\")</code> will pass the petName \"Baxter\" down to the Animal class's constructor, passing it to the Animal's <code>name</code> variable. (Of course, in a real use case, <code>name</code> would be a public property with just a Get.)</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/01/ls-classes-2/#calling-base-functions","title":"Calling Base Functions","text":"<p>The other slightly unusual syntactical approach is interacting with the base class's properties and functions / subs. Using <code>BaseClass..subName</code>, <code>BaseClass..functionName</code> or <code>BaseClass..propertyName</code> you can call subs, functions and properties of the base class from the derived class. The particularly useful part here is that this syntax can be used to call Private properties or functions from the derived class. So this is possible.</p> <pre><code>Class Animal\n    Private name as String\n    Private owner as String\n\n    Sub New(animalName as String)\n        Me.name = animalName\n    End Sub\n\n    Private Property Set ownerName as String\n        Me.owner = ownerName\n    End Property\n\n    Public Function getOwner() as String\n        getOwner = Me.owner\n    End Function\n\nEnd Class\n\nClass Dog as Animal\n\n    Sub New(petName as String, ownerName as String), Animal(petName)\n        Animal..ownerName = ownerName\n    End Sub\n\nEnd Class\n</code></pre> <p>Note here the constructor for the <code>Dog</code> class takes two parameters, the pet's name and the owner's name. But the <code>Animal</code> class only takes a single parameter, the animal name. The <code>New Dog</code> constructor first creates an instance, setting the <code>name</code> property. It then calls the base <code>Animal</code> class's <code>ownerName</code> property setter. Even though that property is private in the base class, it is still accessible from the derived class. This may overcome the need for the <code>Protected</code> or <code>Friend</code> keywords in some circumstances.</p> <p>One caveat to point out is that this syntax cannot be used to interact with private variables in the base class. Private variables are private to the base class and inaccessible outside. But even still, this provides a great degree of flexibility of base and derived classes.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/02/ls-classes-3/","title":"LotusScript Classes Deep Dive Part Three","text":"<p>In part one I covered the basics of what constitutes a Class in LotusScript / VoltScript. In part two I covered abstract classes, base and derived classes. But there are more things that can be done with classes and properties.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/02/ls-classes-3/#properties-outside-of-classes","title":"Properties Outside of Classes","text":"<p>First, a little digression on Properties. Classes are not the only place where properties can be used. They can also be used as Globals in Script files. In the Domino Designer IDE, this means any Declarations area, including in Forms and Views. One good reason for using them is that you can just define the Get part.</p> <p>In DOMI we used properties in the domiConstantsBE Script Library. But we also used Constants. So why both? Well, a constant is...constant. It is cached, so even if you change the constant value in the script library, dependent code still picks up the old version. Variables don't help, because they cannot have a value assigned to them in the Declarations area, they can only be declared. However, if you use a Property and call the constant, it doesn't cache the value. Obviously for variable values like the Sametime URL, it would be a support nightmare if we had a scenario where a developer would update the value but it would not be applied. Properties solved that problem.</p> <p>The syntax is exactly the same as in a class. But there is an additional keyword that can be applied to properties when they are outside of a class, as well as Private and Public - static.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/02/ls-classes-3/#static","title":"Static","text":"<p>I blogged a number of years ago about the Static keyword, a blog post a number of people have pointed me to when the topic has come up. At the time I only talked about using <code>Static</code> for variables within a Sub or Function, and I was only using it to lazy load something like a keyword view, with code like this:</p> <pre><code>Function getKeyView As NotesView\n    Dim session As New NotesSession\n    Static mkeyView As NotesView\n    If (keyView Is Nothing) Then Set keyView = session.currentDatabase.getView(\"Keywords\")\n    Set getKeyView = keyView\nEnd Function\n</code></pre> <p>But <code>Static</code> can be used for more than just variables. It can be used at Function or Sub level and it can also be used at Property level. When used at Property level, all variables within the Property are automatically static. Presumably this is also the case when using <code>Static Function getFoo()</code> or <code>Static Sub doBar()</code>, I'll leave it to someone else to verify that.</p> <p>This may seem of academic interest but of little benefit. However, there are three points I've covered in this blog post that make this very useful:</p> <ol> <li>Variables can only be declared as globals, a value cannot be assigned.</li> <li>Properties can be declared as globals - which includes the code within the Getter or Setter.</li> <li>Applying <code>Static</code> to the whole Property or a variable within a Property makes it persist between calls to the Property.</li> </ol> <p>When you combine this with the topic of this blog series - Classes - you get a very interesting outcome. I've only seen it mentioned in one blog post, and it's extensively used in a certain OpenNTF project. That project and the blog post about it brings me full circle to the start of this blog series, because the project is OpenDOM and the blog post is by the originator of that project, Alain H Romedenne. There may have been other blog posts and other projects that used the technique, but so much knowledge on this platform has been lost as those who drank deep from the well have moved on and their projects left to languish by the community.</p> <p>As I read Alain's blog post, and after years of working with Java, the technique sounded very familiar. Alain uses <code>Static Property</code> to create variables that created an instance of a class and be able to refer to it in a static manner. In Java this is similar to the Singleton Pattern with enums. The technique could be particularly useful if you have classes where you only need a single instance, for example helpers or builders, and maybe configuration objects.</p> <p>Imagine the following class.</p> <pre><code>Public Class Config\n    Private version_ as String\n\n    Property Get version as String\n        If (me.version_ = \"\") Then\n            Call loadVars()\n        End If\n        version = Me.version_\n    End Property\n\n    Private Sub loadVars()\n        ' E.g. load from a document\n    End Sub\nEnd Class\n</code></pre> <p>Here we are using static values, but imagine this was loading values from a Notes document. We can then add a property to lazy load and always return a single instance of the class with the following code.</p> <pre><code>Public Property Get Config As Config\n    Static this As Config\n    If (this Is Nothing) Then\n        Set this = New Config()\n    End If\n    Set Config = this\nEnd Property\n</code></pre> <p>Developers can then just use <code>Config.version</code> without needing to do <code>Dim Config as new Config</code>.</p> <p>One point of difference to Alain's blog post is that he has Static against the Property as well. Based on my testing, that isn't needed. The <code>Delete</code> sub is a good way to test this, and that is only called at the end of all code. As I mentioned before, if you wanted multiple variables within the Config Property, you could place <code>Static</code> at the Property level instead of the variable level. But adding it at the variable level is sufficient.</p> <p>The second point is scope. In Java Singletons are scoped to the JVM, which is why you need to use AtomicBoolean, AtomicInteger etc instead of scalars. In LotusScript they are scoped differently.</p> <ul> <li>If the Property is in an Agent or a Script Library that the Agent uses, the property is scoped just to the run of that agent.</li> <li>If the Property is in a Form or a Script Library that the Form uses, the property is scoped to that Form. All events, fields and actions on that Form will share the same instance. However, if you have an agent that uses the script library and run it while you have the Form open, it will use an instance scoped to the Agent and delete it at the end of the agent call.</li> <li>If the Property is in Database Script or a Script Library that the Database Script uses, the property is scoped to the Database Script and all events within it. The Form will use its own instance of the Property. The agent will use its own instance of the Property.</li> </ul> <p>In VoltScript it is much simpler. Everything is most analogous to an Agent, so it persists for the life of the call.</p> <p>I enigmatically raised the topic of singletons on OpenNTF Discord last week. As I mentioned, this seems similar to the singleton pattern. We had some discussions internally about adding a <code>Singleton</code> keyword to VoltScript to wrap this functionality. However, it doesn't quite work like Singletons in Java or other languages, so we have decided it would cause confusion because it could not work as developers would expect. It would be syntactic obscurantism, a term I came across in a blog post about ES6 classes. However, as a design pattern, it is very useful and we are definitely looking at useful ways to highlight this and other useful design patterns in documentation or tooling.</p> <p>This may seem everything about classes. But there is one more LotusScript function that is relevant, admittedly obliquely, but very usefully.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/03/ls-classes-4/","title":"LotusScript Classes Deep Dive Part Four","text":"<p>Before going onto that one more LotusScript function that is relevant to classes, there is another topic worth discussing with regard to classes. This is not functionality in LotusScript classes itself, but a design pattern which has been available ever since LotusScript began, but one which has become very commonly used in other languages as they have developed. It fits well after the discussion on using <code>Static</code> for creating builder classes, because the builder pattern in Java is where it's most widely used. The design pattern I'm referring to is a fluent interface.</p> <p>A fluent interface is an object oriented API that relies extensively on method chaining. The benefits come when you're wanting to call multiple functions in a row, it is easily readable and improves brevity in code. As I mentioned, it has become very common in Java builder classes. It's a pattern I've used in LotusScript for a little while, first in Volt MX LotusScript Toolkit. The <code>NotesJsonNavigatorFluent</code> and <code>NotesJsonArrayFluent</code> provided fluent functions to speed up building JSON in a NotesJsonNavigator.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/03/ls-classes-4/#creating-the-fluent-function","title":"Creating the Fluent Function","text":"<p>The approach is very straightforward. Typically classes will have a Sub like <code>NotesDatabase.SetOption()</code>, which don't return anything. Or they have a Function like <code>NotesJSONNavigator.appendElement(passedElem) as NotesJsonElement</code> which return the object being passed as a parameter. To convert this to a fluent function is straightforward, and done with <code>NotesJsonNavigatorFluent</code>. The function signature is changed from <code>Function NotesJsonNavigator.appendELement(passedElem) as NotesJsonElement</code> to:</p> <pre><code>Function NotesJsonNavigatorFluent.appendElementFluent(passedELem) as NotesJsonNavigatorFluent\n    Call m_NotesJsonNavigator.appendElement(passedElem) 'm_NotesJsonNavigator = internal NotesJsonNavigator\n    Set appendElementFluent = Me\nEnd Function\n</code></pre> <p>The function does whatever it would normally do, then returns the current object. This allows you to do <code>NotesJsonNavigator.appendELement(passedElem).appendElement(passedElem2)</code>.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/03/ls-classes-4/#improving-the-code-further","title":"Improving The Code Further","text":"<p>That is good, but it's not as easy to read as its Java equivalent would be. And what happens if there's an error with one of the calls? Imagine the following code:</p> <pre><code>Class Foo\n    Private elem List As String\n\n    Function appendElem(key As String, value As String) As Demo\n        Me.elem(key) = value\n        Set appendElem = Me\n    End Function\n\nEnd Class\n\nClass Bar\n\n    Public myString As String\n\nEnd Class\n\nSub Initialize()\n\n    On Error GoTo errLog\n\n    Dim myFoo as New Foo\n    Dim myBar as Bar\n\n    Call myFoo.appendElem(\"1\", \"One\").appendElem(\"2\", bar1.test)\ngetOut:\n    Exit Sub\n\nerrLog:\n    MsgBox \"Error \" &amp; Error() &amp; \" on line \" &amp; Erl\n    Resume getOut\nEnd Sub\n</code></pre> <p>This will throw an \"Object variable not set\" error on the line with <code>appendElem()</code> calls. In this case, it's easy to see it's the second <code>appendElem()</code> call produces the error. But it won't always be so obvious. Fortunately, just as Java allows you to use separate lines for each method call in the chain, so does LotusScript. We just need to change the code like so:</p> <pre><code>Call myFoo.appendElem(\"1\", \"One\")._\n    appendElem(\"2\", bar1.test)\n</code></pre> <p>This throws an error where the error line is specifically the line with the second <code>appendElem()</code> call. So we can immediately tell which part of the chain it failed on. And it also makes it more readable.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/03/ls-classes-4/#caveat","title":"Caveat","text":"<p>One caveat is that you need to understand what is happening when in the chain, and to avoid manipulating parameter values. Debugging would highlight that mistake. On order of processing, imagine is we had a <code>Function setTest(string) as String</code> in the Bar class and had this code.</p> <pre><code>Dim bar1 as New Bar()\nCall demo.appendElem(\"1\", bar1.setTest(\"One\"))._\n    appendElem(\"2\", bar1.setTest(\"Two\"))\n</code></pre> <p>This code will call:</p> <ul> <li><code>bar1.setTest(\"One\")</code></li> <li><code>demo.appendElement(\"1\", \"One\")</code></li> <li><code>bar1.setTest(\"Two\")</code></li> <li><code>demo.appendELement(\"2\", \"Two\")</code></li> </ul> <p>This may not be a surprise, but knowledge is always better than assumptions.</p> <p>Although fluent coding is not the norm in LotusScript, it starts to become very attractive after some years coding in Java, particularly more modern Java. It makes code feel more modern and look cleaner, even if it's not.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/04/ls-classes-5-execute/","title":"LotusScript Classes Deep Dive Part Five: Execute","text":"<p>In the previous parts one, two, three and four I talked about various aspects of LotusScript / VoltScript classes. But there's a LotusScript function which has become very relevant for me recently, in the context of classes.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/04/ls-classes-5-execute/#the-basics","title":"The Basics","text":"<p>Almost all Domino developers are used to writing LotusScript, compiling it and running it. But for those who have dug into the mail template or maybe other advanced coding, there is a less well-known approach to running LotusScript - <code>Execute</code>. Execute takes a string of LotusScript.</p> <p>This is actually nothing new for Domino developers. There are at least two direct comparisons I can think of. The first is also in LotusScript - <code>Evaluate</code>, which also takes a string, this time a string of Formula Language, and runs it using the Formula Language engine. The second is XPages, where properties take a string prefixed with \"javascript:\" and run it using a Server-Side JavaScript engine; or it takes a string prefixed with \"el:\" and runs it with the Expression Language engine. In the case of XPages and Server-Side JavaScript, there is an editor that performs some validation of the string to catch runtime errors. But it's still a string, parsed and processed at runtime.</p> <p>To give a simple example, imagine the following simple piece of LotusScript:</p> <p><code>Print \"Hello World\"</code></p> <p>The same code can be achieved with Execute:</p> <p><code>Execute \"Print \\\"Hello World\\\"\"</code></p> <p>Of course LotusScript allows different delimiters for strings, which makes things a lot easier. So we can have:</p> <p><code>Execute |Print \"Hello World\"|</code></p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/04/ls-classes-5-execute/#passing-context","title":"Passing Context","text":"<p>On the surface, this may seem of little benefit. There is not much point is switching a <code>Print</code> statement for an <code>Execute</code> statement that prints. There may be use cases for running dynamic LotusScript - maybe LotusScript that's stored in a config document. But the real benefit, as with <code>Evaluate</code> and EL / SSJS, comes from running dynamic LotusScript with context.</p> <p>To explore those a little further, there are two main use cases for <code>Evaluate</code> in LotusScript. The first one is to use <code>@Round</code>, to perform half-up rounding as opposed to bankers rounding, which is what the LotusScript <code>Round</code> function does. The second is to run contextual formula language, passing a string and a NotesDocument as the context in which to run the Formula Language. Imagine the following LotusScript:</p> <p><code>Evaluate(|@Round(unit_price x qty)|, doc)</code></p> <p>This is taking the <code>doc</code> variable and using it as a context in which to perform the <code>@Round</code>. So the variables <code>unit_price</code> and <code>qty</code> map to fields of the contextual Notes Document, <code>doc</code>.</p> <p>SSJS and EL do the same thing. <code>#{database.title}</code> or <code>#{javascript:database.getTitle()}</code> run using the <code>database</code> variable for the current context. Of course there are other variables that can be used, like <code>session</code>, <code>view</code> or the variable defined in the <code>var</code> property of a repeat control. Take it to a more advanced coding level, and you can contribute additional variables as we did at various times with OpenNTF Domino API. And with the <code>binding</code> property of a component, you can access that component from outside of its current context.</p> <p>The same can be done with <code>Execute</code>. It's just that the variables need to be scoped appropriately. For <code>Execute</code> that means declaring the variables as Globals. Then you can just reference those variables within the Execute statement. So imagine the following code:</p> <pre><code>Private doc as NotesDocument\n\nSub calcTotal(idex as Integer)\n    Execute(|doc.total_| &amp; idex &amp; | = doc.qty_| &amp; idex &amp; |(0) * doc.price_| &amp; idex &amp; |(0)|)\nEnd Sub\n</code></pre> <p>The <code>calcTotal()</code> sub can be used to set a relevant total field to the product of its quantity and price. If the parameter passed in is <code>1</code>, the string that is executed is calculated as <code>doc.total_1 = doc.qty_1 * doc.price_1</code>. If <code>2</code> is passed in, the string that is executed is <code>doc.total_2 = doc.qty_2 * doc.price_2</code>. Because <code>doc</code> is a global variable, defined in the Declarations of an Agent or Script Library, the Execute statement has access to it.</p> <p>Of course with the NotesDocument class, there are other ways of doing this. But with custom classes, especially if the class has not been written yet, that can't be guaranteed. Imagine you want generic code to set variables in a class. Now we can do this:</p> <pre><code>Private obj as Variant\n\nSub setVariable(varName as String, var as Variant)\n    Execute(|obj.| &amp; varName &amp; | = | &amp; var)\nEnd Sub\n</code></pre> <p>There are a wide variety of use cases here for dynamic processing of an object. But there are three points to bear in mind:</p> <ol> <li><code>var</code> here can only be a scalar or array. If it's an object, you need to pass it to a global variable and pass that to the <code>Execute</code> statement, as is being done with <code>obj</code>.</li> <li>If <code>var</code> is a String, then the <code>Execute</code> line isn't actually setting the variable as a string - it's setting it to a variable with the string name. You'll realise if you transpose the variables into the statement, because if <code>varName</code> if \"foo\" and <code>var</code> is \"bar\", what we get us <code>obj.foo = bar</code>. What we need is <code>obj.foo = \"bar\"</code>. So the execute statement needs to be <code>Execute(|obj.| &amp; varName &amp; | = \"| &amp; var &amp; |\"|)</code>.</li> <li>If <code>var</code> is an array, for example a String array, the variable cannot be declared as <code>Public foo() as String</code>. That's because you cannot use assignment to load an array declared like that, you would need to redim the variable (<code>foo</code>) to the same bounds as the source array (<code>var</code>), iterate the source array, and assign each element in turn. The cleaner way is for the variable to be declared as a Variant - <code>Public foo as Variant</code>.</li> </ol> <p>The final point to bear in mind is returning values. As with normal code, if you're returning an object you need to use <code>Set</code>, if you're returning a scalar or array, you need to not use <code>Set</code>. This can create some more complex code if you're creating something generic. Once you have the return value, <code>IsArray()</code> will tell you if it's an array of something. <code>IsObject()</code> to know whether or not something you're dealing with is an object or, against an element in an array, whether it's an array of objects - and so you need to use <code>Set</code>. <code>IsScalar()</code> can be used to do the opposite.</p> <p>There are a number of scenarios where you might want to run custom code, and a variety of places those custom code strings may reside. But <code>Execute</code> provides a great deal of power.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/11/ls-classes-singleton-addendum/","title":"LotusScript Classes - Singleton Addendum","text":"<p>After my blog post last week on LotusScript classes and using <code>Static Property Get</code> to create singletons, there was some discussion on OpenNTF's Discord about the challenges of forcing use of the singleton. \"Singleton\" is a misnomer really, because it's not scoped to the JVM asaJava singleton would be. But I can't think of a better name yet, so I'll stick with that terminology, but be aware of the specific scope for static instances in LotusScript / VoltScript.</p> <p>Firstly, it's not possible to create a public static property of a private class. So you cannot hide the class in the script file. And the <code>New</code> method is a Sub, so I couldn't find a way to define what the current instance is during the New sub, and so always return the same instance. That of course is possible in an LSX, as shown with the NotesSession class. No matter how many times you use <code>Dim session as New NotesSession</code> or <code>Dim s as New NotesSession</code>, every time you are returning the same instance of the NotesSession class - it is truly a LotusScript singleton.</p> <p>I played about with a few scenarios, and the best I could come up with was a way to prevent users creating new instances. First the script file needs a private global variable for whether or not the call is from the static instance: <code>Private fromInstance as Boolean</code>.</p> <p>Next, we need the static public property of our class - we'll use a class called <code>Config</code> as in the previous blog post. The code for our public property is this:</p> <pre><code>Public Property Get Config As Config\n    Static this As Config\n    If (this Is Nothing) Then\n        fromInstance = true\n        Set this = New Config()\n        fromInstance = false\n    End If\n    Set Config = this\nEnd Property\n</code></pre> <p>This looks very similar to what we had before, except before instantiating the <code>Config</code> class, we set the global <code>fromInstance</code> variable to true, then reset it to false. It's a global and global variables persist, so we need to reset it.</p> <p>Now, in the constructor for the Config class, we just need to check the <code>fromInstance</code> variable, and throw an error accordingly:</p> <pre><code>Sub New\n    If (Not fromInstance) Then\n        Error 1403, \"Config cannot be instantiated in this way, use FooInstance variable\"\n    End If\nEnd Sub \n</code></pre> <p>Now if you use <code>Dim Config as New Config()</code>, you will get a runtime error. If you use <code>Print Config.version</code>, you will be fine.</p>","tags":["VoltScript","LotusScript","Domino","Volt MX Go"]},{"location":"blog/2022/08/15/github-pages-on-domino/","title":"GitHub Pages Sites on Domino 1: Why","text":"<p>Source control is a topic that periodically crops up around Domino. And if source control is important, there is only one choice for documentation - Jekyll. It's not hard to justify why. I'll come onto more detailed coverage of the technologies involved in the next blog post. But suffice to say, for now, that some key reasons are:</p> <ul> <li>Very widely used, so plenty of answers already to many of the questions you don't know you want to ask.</li> <li>Separation of content from display, the holy grail for portability of content.</li> <li>Light-weight coding to create sophisticated layouts and passing variables into HTML files.</li> <li>GitHub Pages can generate and host the sites for you.</li> <li>Everything is text, so perfect for source control.</li> <li>Local generation creates an _site directory that contains your generated website, which can then be passed to anything that can host HTML, CSS and JavaScript.</li> </ul> <p>If you're building a blog or documentation for an open source project that's already on GitHub, GitHub Pages makes the most sense. GitHub Pages still provides an intuitive URL linked to your profile or project. But you can also choose a different domain name for the GitHub Pages site.</p> <p>If that doesn't provide what you want, taken as a whole, these points provide a compelling and unbeatable set of options.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/15/github-pages-on-domino/#hosting-on-github-pages","title":"Hosting on GitHub Pages","text":"<p>If you're building a personal blog or documentation for an open source project that will itself be delivered on GitHub, GitHub Pages makes most sense. Particularly with blogs, content should be as free to use as the design, so there is no reason not to use GitHub Pages for both. If your documentation is only for use internally but you host your source control on an internal GitHub server, it also makes sense. But if you want limited access and you're using free GitHub, a private repository cannot have a GitHub Pages site.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/15/github-pages-on-domino/#hosting-on-domino-server","title":"Hosting on Domino Server","text":"<p>If you already have Domino, and are willing to run the HTTP task (not all customers are), then it offers a ready-made web server where the site can be hosted. You could just put the content in the /domino/html directory and host directly from the server. This requires access to that directory on the Domino server, which might be problematic. But you can also host within an NSF.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/15/github-pages-on-domino/#hosting-in-nsf","title":"Hosting in NSF","text":"<p>An NSF provides a variety of benefits for hosting documentation content. There are a variety of ways this can be done, and when we get to that point, I will cover the one that makes most sense for me. But let me list some benefits the NSF provides out-of-the-box.</p> <ul> <li>Rapid deployment, just paste the site contents into the NSF and you're ready to go.</li> <li>Replication to other servers is not always relevant, but is a huge benefit where required.</li> <li>When upgrading a server, I have seen some customers where content in /domino/html directory is forgotten about and lost. This won't happen if the content is in an NSF.</li> <li>Authentication can be controlled by Domino, ensuring the site is only accessible to authorised individuals.</li> <li>The site can be hosted without needing to worry about challenges of licensing conflict between framework and content.</li> </ul> <p>There are limitations. The Domino HTTP server has not kept pace with other HTTP servers, so there may be caching options that are less straightforward to leverage for those who are not seasoned Domino admins. But whatever your choice, there are always going to be compromises. It's just a question of which compromises you're willing to accept.</p> <p>But if you're hosting in an NSF, why not write your content in an NSF? I've tried going that route before. I built a website builder tool on XPages where content was written as Domino Rich Text and configuration on documents provided some flexibility of layout. But modernising the look and feel was far from straightforward, even though the content was separate from the overall website. XPages Help Application was another project I created for documentation, but again updating the look and feel was not a trivial task. And neither provided source control of the content. And although the XPages Help Application was intended to allow business users to manage content, in reality I found business users were not interested in taking ownership of training materials. (Ironically, that was a key step on me becoming a citizen developer and eventually a full-time Notes developer. But maybe that is not a route IT departments or business users are interested in today.)</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/15/github-pages-on-domino/#content","title":"Content","text":"<p>I've also moved well away from rich text, whether that be Domino Rich Text or rich text as the wider web development world consider it - which incidentally does not include Domino Rich Text, but is reproducing a lot of what has become painful with Domino Rich Text.</p> <p>Content writing should always focus on content, rendering should always be handled separately. Content editors should not be defining which font is used or even font size, that should be standard across the site and managed separately. And if you want any kind of source control, it has to be stored as text only. For me, that means Markdown.</p> <p>HTML, even when provided by web editors, creates inconsistent results. And that is why so many web editors, particularly those that wish to focus on co-editing, manage and store content not as HTML but as JSON. If they could standardise on the schemas they manage and store the content with, maybe the output of web editors could be a valid content format for source control. But we are years away from that, probably more than a decade.</p> <p>The other point when it comes to source control is that images and attachments - binary content - has to be separate from the textual content. Most web renderers separate them for display purposes. But for source control purposes, packaging them together is a convenience that makes reconciling differences messy, difficult or plain impossible.</p> <p>Source control has been a key part of my life for many years now, it's saved me on more than one occasion, and I have adapted my development on occasion to work around limitations. There may be some who will not use source control on Domino until it fully supports all possible code they have or may have in their application, and I'm not interested in getting into those discussions. For the purpose for which I'm writing this blog series, source control of the content is important to me and I will adapt accordingly.</p> <p>Developing a site as a GitHub Pages site, managing it as a standard GitHub repo, and deploying to Domino is a compromise I'm willing to make. The NSF - and its content - becomes deployment only. The source is in files in Git.</p> <p>In the next blog post I'll give some background on the technologies behind GitHub Pages sites, because this is probably unfamiliar to many Domino developers.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/15/github-pages-on-domino/#table-of-contents","title":"Table of Contents","text":"<p>Jekyll - Why Jekyll - What Jekyll - How</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/","title":"GitHub Pages Sites on Domino 2: What","text":"<p>Hosting a GitHub Pages - or more properly, Jekyll - site on Domino may not fit everyone's requirements. But it did fit mine. The \"how\" is relevant for any static website, although there are other options that I'll mention. But this blog post is covering the \"what\", the technologies involved. There is a lot that will be unfamiliar to many Domino developers, but technologies worth getting familiar with.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#content","title":"Content","text":"<p>The content is written in Markdown, without a doubt the only choice if you want formatted and portable content which can be written by a variety of people. It allows combination of text, tables, code, images and attachments. It can include HTML, though there are other ways of handling that. Most importantly, it focuses on content and not presentation. This flexibility makes it great for blogs, technical documentation, or even small files of developer notes. It is also the standard for readme files in repositories, so most developers are already familiar with it.</p> <p>Markdown itself has a variety of flavours, the most common being kramdown, because it is understood by a variety of editors and renderers, most importantly GitHub Pages. Kramdown even lets you specify the class to apply to a paragraph.</p> <p>There are also a wide variety of editors that can be used, both for desktop and mobile. On Windows, I started off using Markdown Pad 2, although when I last used it (a couple of years ago) there was a problem rendering on Windows 10, resolved with an Awesomium SDK. Now my usual editor is Visual Studio Code, primarily because I use Visual Studio Code for much more than just editor of markdown files. There are extensions that allow live preview of markdown files within VS Code, but the terminal also allows easy running of GitHub Pages sites from the IDE.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#jekyll","title":"Jekyll","text":"<p>Editors can let you write markdown files immediately. But if you're wanting to build or preview a whole site, you're going to get into Jekyll. Jekyll is not a product, but a tool-set, built on Ruby. This requires setting up your development machine with the pre-requisites, for which there are detailed instructions for various operating systems. Having installed on a variety of operating systems, it is straightforward to do. There is also a Docker image, which I haven't used but - knowing what Docker does and the requirements for running the Jekyll server - makes a lot of sense.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#ruby","title":"Ruby","text":"<p>Ruby may be unfamiliar to some. Ruby consists of Gems, packages of code. Jekyll is a gem, as are many Jekyll plugins, and so is <code>bundler</code>, which installs all gems in your gemfile.</p> <p>The gemfile is like the package.json in a Node.js project, it lists the Ruby gems to install and the versions. The important point to bear in mind is that, as with anything else that uses a dependency management tool, some dependencies require specific versions. And GitHub Pages also uses specific versions, which can limit the versions used.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#commands","title":"Commands","text":"<p>Commands are run from the command line, so you will get used to this. If you must have an IDE that does things for you with menu options, Ruby - and Jekyll, and GitHub Pages, and Markdown - are not for you. These commands you will become very familiar with:</p> <ul> <li><code>gem install</code> will be the most basic command used, to install a Ruby gem. <code>bundler</code> is one that you will want, because it lets you use the Gemfile, which lets you ensure the same versions installed across all environments.</li> <li><code>bundle install</code> will install all gems specified in a Gemfile.</li> <li><code>bundle exec jekyll serve</code> will load the local website, usually on port 4000, according to the <code>_config.yaml</code>. If you come across a site that's not using a Gemfile, <code>jekyll serve</code> will be the command to use instead.</li> </ul>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#configyaml","title":"Config.yaml","text":"<p>The <code>_config.yaml</code> file holds the config to use when rendering the site. It's written in YAML and all the variables there can be referenced throughout the site. This is part of the power of Jekyll, and comes because of Liquid.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#liquid","title":"Liquid","text":"<p>Liquid is a powerful open source templating language created by Shopify, with a wide range of functionality, as covered in the Liquid documentation. As well as referencing variables, it can create temporary variables, code flows and loops, as well as filtering. When combined with data json files and HTML include files, it can create some very sophisticated display options without needing to resort to JavaScript coding.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#layouts-and-includes","title":"Layouts and Includes","text":"<p>Layouts are templates for different types of pages. Each markdown file will define the layout it should use in the frontmatter of the page. They can include inheritance, if you wish to get particularly sophisticated. Includes are snippets of HTML to include in one or more templates. This allows you to define e.g. headers or footers once only. The layouts - and the CSS referenced - will handle the display of the headings, text, tables etc in the markdown content. Of course they can also include JavaScript files.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#pages","title":"Pages","text":"<p>Rather than focus on presentation, the pages are just markdown with YAML frontmatter. These are variables that the site will use to feed into the content, but most importantly the layout variable, to determine which layout to render the page with.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#and-more","title":"And More","text":"<p>There is much more that Jekyll can do, as covered in the docs. It is also used by many developers, so there is plenty of community support.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#themes","title":"Themes","text":"<p>But the greatest power for Jekyll is the themes you can apply - and extend. There are a variety of themes that GitHub Pages offers out-of-the-box. But there are also a variety of other themes that are out on the web. One I've used in a number of places is Just The Docs. This is great for small documentation sites. Another more sophisticated one I've used is I'd Rather Be Writing.</p> <p>Different themes will render the content in different ways. And once you run <code>bundle exec jekyll serve</code>, you'll see in the \"_site\" directory how it's actually building the website. The interesting thing is that this directory can be used for any static website hosting tool. Including an NSF.</p> <p>However, when it comes to links to other content, you will need to take care and may need to tweak the layouts. Relative links are always best, and this can be done using <code>../</code> to navigate up a level from the current URL, or using Liquid filters. But then you need to be aware of how your theme renders the pages in the static _site directory. Also, some links may omit the \".html\" suffix or more. I've come across a theme that generated every page as an \"index.html\" page in a sub-directory relating to part of the frontmatter. Some static web servers will automatically work that out, but Domino looks for a page at the specific URL provided. TO quote Stephan Wissel, YMMV.</p> <p>With this caveat, it's next time to move on to my preference on how to set up Domino and the NSF.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/16/github-pages-on-domino-2/#table-of-contents","title":"Table of Contents","text":"<p>Jekyll - Why Jekyll - What Jekyll - How</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/17/github-pages-on-domino-3/","title":"GitHub Pages Sites on Domino 3: How","text":"<p>There are many ways to host static websites on Domino, and Project KEEP provides another option. In my scenario, for reasons I won't go into, hosting within the NSF made most sense.</p> <p>GitHub Pages / Jekyll sites expect a specific base URL, defined in the _config.yaml. It may be possible to map that to a specific NSF filepath, but there is another option: Internet Site Documents with Substitution Rules. This does what it says, it substitutes an incoming URL pattern with a replacement.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/17/github-pages-on-domino-3/#internet-site","title":"Internet Site","text":"<p>The first thing to do is to create, or ensure you have already in place, an Internet Site Document for the current server. For backwards compatibility, Domino still allows HTTP settings to be defined in the main server document, but the modern approach is to use Internet Site Documents, and enable this using the \"Load Internet configurations from Server\\Internet Sites documents\" setting on the Basics tab of the server document.</p> <p>The \"Global Web Settings\" does not work for substitution rules, it needs to be an Internet Site Document for the relevant host name.</p> <p></p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/17/github-pages-on-domino-3/#substitution-rule","title":"Substitution Rule","text":"<p>Once you have an Internet Site document created for the relevant host name, you are ready to create the Substitution Rule document. This is created as a response to the relevant Internet Site document, within the Internet Site document using the \"Web Site...\\Create Rule\" button.</p> <p></p> <p>The Type of Rule needs to be set as \"Substitution\", then you just need to define the incoming root URL and the replacement pattern - whatever it should use instead of that part of the URL. I've shown an example with dummy values below, bear in mind the DOMI Jekyll template won't actually work as-is, because of the URLs the table of contents generates. But it gives sufficient detail to see what you need.</p> <p></p> <p>Be aware that after creating or changing the Substitution Rule, you will need to restart the Domino HTTP server - <code>res t http</code> is the command I always use, the ultimate abbreviation for <code>restart task http</code>.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/17/github-pages-on-domino-3/#content","title":"Content","text":"<p>There are a variety of places the contents of the _site directory could be pasted, some easier and more flexible than others. I've preferred an approach that is consistent with the NSF as an OSGi project - which is what it is for XPages. In an OSGi project, the <code>WebContent</code> directory is a special directory. If you put any content in here, it can be accessed directly within the NSF. If you have a \"Foo.nsf\" and put \"bar.png\" in the WebContent directory, you can go to a browser and - with the relevant authentication and authorisation as defined in the database ACL - you can go to \"https://myserver.com/Foo.nsf/bar.png\" and display the image.</p> <p>Looking back at the Substitution Rule, the replacement pattern is \"DOMI.nsf/site/\". That means it's expecting the content in a \"WebContent\\site\" directory. The main reason I did this was to make it easier for deleting and recreating the content - it's all self-contained. That means creating a \"site\" directory.</p> <p>Firstly, this can't be done from the Applications Navigator view that developers are probably most familiar with. If you've done Eclipse development or written XPages Java classes the standard way, you're familiar with the Package Explorer view. This is the view in which to manage adding your Jekyll website into the NSF.</p> <p>If it's a recently-created NSF, you will already have a WebContent directory with a WEB-INF directory inside it. This is where the xsp.properties file is found. The \"site\" directory can be created by right-clicking on the WebContent directory and selecting New &gt; Other.... Under the \"General\" category, there is the option of Folder. Click Next &gt; and enter the folder name \"site\". Then click Finish.</p> <p>You should now have a site folder, ready to receive the content of your \"_site\" directory in the Jekyll website. Populating it is very easy, because you're now in the realm of standard Eclipse functionality. Open up Windows Explorer and navigate to the \"_site\" directory. Highlight all content and drag it across onto the \"site\" directory in your NSF. When prompted, choose \"Copy files\". You should now have all your content.</p> <p></p> <p>You will now be able to access your Jekyll website within the NSF, secured with whatever authentication you choose for the database ACL, with a URL that does not give any hint that it's within an NSF.</p> <p>Yes, as mentioned, there may be some work to do to clean up the links, depending on the Jekyll theme used. And some optimisations may be required. But the Jekyll site can be composed using Markdown, look and feel managed centrally in templates and CSS (typically SASS / LESS), all versioned and backed up in source control, previewed locally quickly using <code>bundle exec jekyll serve</code>, and deployed to a Domino server by simply deleting the files in the \"site\" directory and copying the replacements in.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/08/17/github-pages-on-domino-3/#table-of-contents","title":"Table of Contents","text":"<p>Jekyll - Why Jekyll - What Jekyll - How</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/09/25/negotiating-enhancements/","title":"Negotiating Enhancements","text":"<p>No IT solution is delivered using code written solely by the solution provider. There is always dependent code written by a third party. Dependency management tooling has proliferated in every technology sector to support this. At the language level, this is handled by maven, gradle, npm etc. At the platform level, it\u2019s handled by Homebrew on Mac and various options on Linux. At the DevOps level, it\u2019s handled by Docker, Helm, etc.</p> <p>And every solution includes an implicit assumption that the dependent code will continue to work as it does and provide whatever the consumer requires. For product offerings, this may be in the form of OEM agreements. For open source, it still exists, whether consumers are willing to admit it to themselves or not.</p> <p>And the brutal truth for consumers is that the authors of your dependencies did not write their code knowing your requirements up front. Whether or not it fits your implementation falls in the realm of \u201ccaveat emptor\u201d. If it doesn\u2019t, if you need an enhancement or you find a bug, your priorities may or may not align with the priorities of the owners of that dependency. With a product, you\u2019re just one paying customer. With open source, you\u2019re just one consumer - not even a customer.</p> <p>So how can you get the optimal result when you want something from that product owner or open source manager?</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/09/25/negotiating-enhancements/#venting-is-counter-productive","title":"Venting is Counter-Productive","text":"<p>Venting on social media or forums is a common tactic. The only way it can have a positive reaction is if it embarrasses the owner sufficiently. This can only be achieved if you are venting from a position of sufficient leverage or expect sufficient support or damage to build a position of sufficient leverage. That is rarely the case.</p> <p>Even if you achieve this, venting will always alienate. The result will be damage to your personal standing with the owner. That will undermine your attempts to negotiate future enhancements, unless you are willing to spend the time to repair the relationship. But if your first thought was the vent, you\u2019re not likely to but the work in to build a good working relationship.</p> <p>Of course you may already have decided to move to an alternative provider. But then you\u2019re not seeking anything positive out of your action. If you\u2019re not recommending an alternative, or showing people how to more easily switch to an alternative, but you\u2019re just venting, you\u2019re not going to be improving your social reputation. But if you don\u2019t care about that, in all likelihood you\u2019ll hit something in the alternative provider that doesn\u2019t work how you want it, vent again, and end up having to spend time switching again.</p> <p>Venting is a lazy approach and in most circumstances counter-productive.</p> <p>So what are my recommendations, based on experiences, for maximising your chances of achieving the outcomes you want? Because the best you can do is maximise your chances, there is no magic formula to always getting what you want - even taking ownership yourself won\u2019t necessarily bring what you want.</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/09/25/negotiating-enhancements/#understand-the-product","title":"Understand The Product","text":"<p>Some may think this means understanding what the product can do. No. What\u2019s important is understanding what the product was intended to do. More important is looking at recent releases, recent features and the roadmap. This will inform you on where the product or project owner is willing to invest time. You may need to read between the lines a little. Also, look at other feature requests or issues. Pay attention to the responses. This can often inform on what the owner deems important. Most important of all, engage with the product or project teams. I always say that we were born with two eyes, two ears and one mouth. So we should do twice as much observing and listening as we do talking. And observing is different to looking, listening is different to hearing. Engaging with the product teams is about learning what they consider important. Only after all this can you understand what is important to the product or project owners. You should have gathered some understanding of this before choosing the product / project, and you need to regularly review it - directions change. But you need this information to know how to ask for what you want.</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/09/25/negotiating-enhancements/#negotiate","title":"Negotiate","text":"<p>You now have a better understanding of the priorities of the product or project owners. You now need to ask how what you want benefits them and their priorities. Even if you\u2019re contributing a pull request, the owners have to do work. Even if you\u2019ve fully covered all possible impacts, testing needs to be verified and documentation may need updating. If the work done doesn\u2019t fit with direction and strategy, that work will not be prioritised. Even if you\u2019ve paid for the product and pay for maintenance, that doesn\u2019t buy you your own way. You\u2019ve paid for the work spent on it so far, and the right to have your opinion listened to, nothing more.</p> <p>So an argument needs to be constructed that takes into account the owner\u2019s views on product direction. This may mean looking for an impact in an area pertinent to product direction. It may mean referencing strategy over recent years and highlighting how the work improves the quality or completeness, to maximise work done on that strategy. It may mean identifying how doing the work now will make future planned strategy easier or better received.</p> <p>Look for an outcome, not an implementation. Concentrating on the outcome will ensure you focus on benefits for consumers, not a specific piece of functionality. When you\u2019re speaking to someone whose focus is strategy, customer stories and customer benefits are key. By all means discuss possible implementations, but a specific implementation may result in a very narrow benefit. And the more micro you think, the more you might miss the macro.</p> <p>It may not be possible, but work with others. However, it\u2019s best to ensure you are not all wanting the same benefit. A broader set of stakeholders, outcomes and benefits produces a greater justification and impact.</p> <p>Ask also what you can do to help. If you\u2019re willing to do work, people are more willing to invest their own time.</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/09/25/negotiating-enhancements/#change-takes-time","title":"Change Takes Time","text":"<p>So you\u2019ve got a commitment to do the work you\u2019ve requested. But your work\u2019s not done. Priorities always change, strategy evolves and there may be impacts you had not anticipated. Keep the channels of communication open, keep working with all stakeholders, adapt your justifications and requirements where appropriate. But most of all, stay patient. Change is either very quick or takes time. If expectations are unrealistic, you can lose support.</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/09/25/negotiating-enhancements/#summary","title":"Summary","text":"<p>Not all your required fixes or enhancements will fit in with the priorities of product or project owners. Sometimes you have to accept your needs do not align with strategic desires. But even when they are more closely aligned, achieving the best outcomes takes time, effort, understanding and careful thought.</p>","tags":["Editorial","Community","Support"]},{"location":"blog/2022/10/30/mkdocs-on-domino/","title":"MKDocs Sites on Domino","text":"<p>In the last blog posts I covered using a Jekyll-based site on Domino. Jekyll is a popular documentation option based on Markdown, but another is MKDocs. MKDocs also has a number of themes - a default Bootstrap-based theme, one used by the Read The Docs service, and the one I've used, Material for MKDocs.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/10/30/mkdocs-on-domino/#technology","title":"Technology","text":"<p>Firstly, it's worth bearing in mind there will be different prerequisites for developing an MKDocs site vs a Jekyll site. Whereas the technology behind Jekyll is Ruby, the technology behind MKDocs is Python, so requires Python and its package manager, pip. As with Ruby, Python can be installed on both Windows and Mac. Once you have Python and pip installed, if you wish to use Material for MKDocs, installing that will also install all other dependencies required.</p> <p>Note that some older Python-based programs may still use Python 2, but MKDocs uses Python 3. So if you also have Python 2 installed, you will need to prefix all commands <code>python3</code> instead of <code>python</code>.</p> <p>Another option is to use a Docker container to develop against, and there is one readily available for Material for MKDocs. Note particularly the instructions for adding plugins to the Docker image, which you will need to follow if you wish to use some of the many MKDocs plugins available. However, bear in mind that as of October 2022 the Docker image uses the Alpine Linux Docker image as a starting point (which may change the Linux package manager commands you need to use) and Python 3.9.2 (which some Python dependencies for some extensions may not yet have upgraded to).</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/10/30/mkdocs-on-domino/#writing-differences-to-jekyll","title":"Writing Differences to Jekyll","text":"<p>The first is the configuration. Instead of a being managed by the <code>_config.yml</code> file in a Jekyll site, the configuration file here is <code>mkdocs.yml</code>.</p> <p>One of the biggest differences to a Jekyll site is with frontmatter. Although frontmatter can be used, it's not required. Page titles, for example, will automatically be picked up from the level one heading on the page.</p> <p>Navigation is also done in a different way in Material for MKDocs. If there is no specific navigation defined, all Markdown files will be picked up, in alphabetical order, with directories appearing after individual Markdown files. However, it's possible to manage navigation either with a <code>nav</code> element in the mkdocs.yml, defining all navigation for the whole site, or a <code>nav</code> element in a .pages file of each directory.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/10/30/mkdocs-on-domino/#limitations","title":"Limitations","text":"<p>There are pros and cons to any framework, and that holds true also for MKDocs. MKDocs works great for standard Markdown documentation, and there are a host of plugins. The in-built search is particularly powerful. But the power of Liquid to create flexible dynamic layouts within the Markdown is not available with MKDocs. Similarly, the ability to integrate content from JSON files and integrate into a Markdown page is also one I've been unable to find any reference to. There do not appear to be as many people seeking to customise the layouts in MKDocs. I suspect that developers with Python experience would be able to create that functionality, but it's not something I have familiarity with nor enough need to investigate further. You can integrate HTML into the Markdown and that was the approach I chose for the limited usage I required.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/10/30/mkdocs-on-domino/#building-and-deploying-to-domino","title":"Building and Deploying to Domino","text":"<p>When deploying to Domino you will want to ensure to set use_directory_urls to false. This means that links point to a \".html\" file instead of a directory, because Domino will not automatically handle this.</p> <p>But once your site is ready for publication, you can just use <code>mkdocs build</code>. If you're using a Docker container, you can just pass <code>build</code> instead of <code>serve</code> when creating the temporary container. The result will be a <code>site</code> directory containing your website. This can then just be copied and pasted into your WebContent\\WEB-INF\\site directory, as detailed in \"GitHub Pages Sites on Domino 3: How\".</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/10/30/mkdocs-on-domino/#summary","title":"Summary","text":"<p>The benefits of using Markdown for your documentation are that migrating or re-using content is much easier. Converting a large site from a Jekyll-based format to MKDocs took just a few hours, including some re-organisation of the content. Removing frontmatter and amending links was time-consuming rather than difficult.</p>","tags":["Domino","Markdown","Documentation"]},{"location":"blog/2022/11/06/parentheses/","title":"Understanding Parentheses in LotusScript Method Calls","text":"<p>Look at the following code and guess the error message.</p> <pre><code>Class Person\n    Public firstName as String\nEnd Class\n\nSub Initialize\n    Dim p as New Person\n    Call outerPrint(p)\nEnd Sub\n\nSub outerPrint(msg as Variant)\n    innerPrint(msg)\nEnd Sub\n\nSub innerPrint(msg as Variant)\n    If (TypeName(msg) = \"PERSON\") Then\n        Print msg.firstName\n    Else\n        Print msg\n    End If\nEnd Sub\n</code></pre> <p>The error message received will be a Type Mismatch, on the line <code>innerPrint(msg)</code>. But the cause might be harder to work out - although the title of this blog post might point you in the direction.</p> <p>Firstly, there's no coding error in the <code>innerPrint</code> sub. If you add error handling to <code>innerPrint()</code>, it won't log anything. Indeed if you try printing something on the first line of the <code>innerPrint()</code> sub, it won't print anything. When the Type Mismatch error logs on the line <code>innerPrint(msg)</code>, that is specifically the line and the Type Mismatch is on passing <code>msg</code> into that method.</p> <p>There are two uses for parentheses when calling a LotusScript method. And that is also why it won't be possible to remove the <code>Call</code> keyword for VoltScript. Parentheses are used to just pass arguments only if you include the keyword <code>Call</code>. But (accidentally) the <code>Call</code> keyword has been omitted in <code>innerPrint(msg)</code>. So the second usage for parentheses when calling methods is being used, namely that the parentheses mean \"pass the variable by value rather than by reference\". If it had been written <code>innerPrint (msg)</code>, it might have been more obvious. You could also use <code>Call innerPrint((msg))</code>, in which case the outer parentheses denote arguments to pass and the inner parentheses identify this argument as being passed by value.</p> <p>Of course pass by value can be explicitly required on a method by using the <code>ByVal</code> keyword. So <code>Sub innerPrint(ByVal msg as Variant)</code> means the argument being passed into <code>innerPrint</code> will always be passed by value rather than by reference. The calling code cannot override this. But wrapping an argument in parentheses when calling the method will also pass it by value. This isn't exclusive to LotusScript. It's also the same in Visual Basic.</p> <p>But only scalars can be passed by value in LotusScript. You cannot pass an array, a List, a Type instance or an object by value. If the code had been <code>Call outerPrint(\"Hello World\")</code>, it would have run fine. But because we're passing an object - an instance of the Person class - it throws an error at runtime. If we had called <code>innerPrint</code> directly from the <code>Sub Initialize</code>, the compiler would have generated an error. But because the Person object is being passed into the <code>outerPrint</code> method as a Variant and not being re-cast explicitly as an instance of the Person class, the compiler cannot generate an error.</p> <p>Typically this would only bite a developer if they're writing a framework, something where arguments are defined as Variants because the code needs to handle various different datatypes. But it's one that, when it bites, it can be harder to identify. This is because passing by value vs by reference is done so rarely in LotusScript.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/","title":"LotusScript Variants: EMPTY, NULL, Nothing","text":"<p>One of the great things about working on VoltScript with our team are the regular discussions about the inner workings of the language. Of course it's also nice how easy it is to write and run a test script with the language, to quickly test various scenarios. Recently, because of two separate initiatives we've been working on, the topic of conversation has been Variants, and the potential values that denote a variant without a value.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/#understanding-a-variant","title":"Understanding a Variant","text":"<p>Firstly, it's important to note that the Variant datatype is a catch-all variable datatype designed to hold any type of \"thing\". These different \"things\" are:</p> <ul> <li>scalars - strings, numerics, booleans, bytes.</li> <li>objects - instances of LSX classes or custom classes.</li> <li>arrays - indexed collections of scalars, objects, or both.</li> </ul> <p>This explains certain language functions that can be run against Variants:</p> <ul> <li><code>isScalar()</code>, returning true for scalars.</li> <li><code>isObject()</code>, returning true for objects.</li> <li><code>isArray()</code>, returning true for arrays.</li> </ul> <p>But there are three other \"values\" that a variant can have: EMPTY, NULL or Nothing. The explanation for why these exist comes down to those first two types of \"things\" a variant can contain.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/#empty","title":"EMPTY","text":"<p>When you create a Variant, it has a default value - EMPTY. This can be seen with the following code:</p> <pre><code>Dim foo as Variant\nPrint TypeName(foo) 'Prints EMPTY\nPrint IsEmpty(foo)  'Prints true\n</code></pre> <p>It's not possible to explicitly set a variable to EMPTY - <code>foo = EMPTY</code> will not compile. And you can't \"empty\" a variant. The only way to \"reset\" a variant is to set it to a variant variable that has never had a value put into it.</p> <p>But <code>IsEmpty()</code> is not the only function that is relevant to the EMPTY datatype. Try the following:</p> <pre><code>Print isArray(foo)\nPrint isScalar(foo)\nPrint isObject(foo)\n</code></pre> <p>It's not a surprise that the first line prints \"False\". What is more important is that the second prints \"True\" and the third \"False\".</p> <p>Yes, EMPTY is a scalar.</p> <p>This becomes important when you want to assign it to another variable. As Andre Guirard blogged recently, there are two ways to assign a value to a variable - again, depending what kind of \"thing\" it is. <code>Set</code> is the keyword to use for an object. <code>Let</code> - which is a NoOp and so can be omitted - is the keyword to use for a scalar or array.</p> <p>The following code will throw a Type Mismatch error on the third line:</p> <pre><code>Dim foo as Variant\nDim bar as Variant\nSet foo = bar\n</code></pre> <p>The mismatch is because we're trying to allocate a scalar - EMPTY - to a variant <code>bar</code> that we explicitly state (by using <code>Set</code>) will be an object.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/#nothing","title":"Nothing","text":"<p>Nothing, on the contrary, is an object. Try the following:</p> <pre><code>Dim foo as Variant\nSet foo = Nothing\nPrint isArray(foo)\nPrint isScalar(foo)\nPrint isObject(foo)\n</code></pre> <p>This will print \"False\", \"False\" and \"True\". And note particularly the syntax in the second line: we have to use <code>Set</code> to allocate the datatype Nothing to <code>foo</code>. Another interesting point is what <code>Print TypeName(foo)</code> returns: whereas the datatype for an empty variant is \"EMPTY\", the datatype for a variant set to Nothing is \"OBJECT\".</p> <p>Picking up from our last piece of code from the EMPTY examples, the following will not compile:</p> <pre><code>Dim foo as Variant\nDim bar as Variant\nSet foo = Nothing\nbar = foo\n</code></pre> <p>This doesn't generate a run-time error but a compile-time error, because our code has told the compiler <code>foo</code> will be an object. The following code, however, won't generate a compile-time error.</p> <pre><code>Dim bar as Variant\nbar = getFoo()\n\nFunction getFoo() as Variant\n    Set getFoo = Nothing\nEnd Function\n</code></pre> <p>That's because the <code>getFoo()</code> function returns a variant, but a variant can be a scalar or an object. So instead it will return a runtime error: SET required on class instance assignment.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/#null","title":"NULL","text":"<p>Now we come to the interesting part. EMPTY is not the same as \"nothing\". Instead it means \"not initialized\".</p> <p>This becomes crucial when you're allocating a variant to a variable that is not a variant. We know that variables are initialized with a value. A string has an initial value \"\". A numeric has an initial value 0.</p> <pre><code>Dim foo as Variant\nDim fooInt as Integer\nDim fooStr as String\n\nfooInt = 12\nfooStr= \"12\"\nfooInt = foo\nfooStr = foo\nPrint fooInt\nPrint fooStr\n</code></pre> <p>The first print statement will print \"0\". The second print statement will print \"\". Setting it to the \"not initialized\" value resets the Integer and the String to their default values.</p> <p>But what if a blank string or 0 means something else. What if you want to return potentially a string or integer, but also something that's not a string or an integer. For example, in <code>ArrayGetIndex()</code>?</p> <p>This is where NULL comes in, and why we have <code>IsNull()</code>. And this is also where variants come in.</p> <p>The following code will work, and print 1:</p> <pre><code>Dim arr(1) as String\nDim fooInt as Integer\n\narr(0) = \"Hello\"\narr(1) = \"World\"\nfooInt = ArrayGetIndex(arr, \"Hello\")\nPrint fooInt\n</code></pre> <p>But change the penultimate line to <code>fooInt = ArrayGetIndex(arr, \"Hellos\")</code> and you'll get an error - Invalid use of null. This becomes apparent from the following code:</p> <pre><code>Dim foo as Variant\n\nfoo = NULL\nPrint isArray(foo)\nPrint isScalar(foo)\nPrint isObject(foo)\nPrint isNull(foo)\n</code></pre> <p>This prints \"False\", \"False\", \"False\", \"True\". NULL is not an array (not surprisingly), not a scalar, not an object.</p> <p>The ArrayGetIndex example can be fixed like so:</p> <pre><code>Dim arr(1) as String\nDim foo as Variant\n\narr(0) = \"Hello\"\narr(1) = \"World\"\nfoo = ArrayGetIndex(arr, \"Hellos\")\nPrint foo\n</code></pre> <p><code>IsNull()</code> can be used to test if it's returned null, but otherwise it will return a variant of datatype \"INTEGER\". Thus ArrayGetIndex only needs to be run once, but can be used to check if the value is in the array and get its index, if it does.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/06/ls-variants/#caveat","title":"Caveat","text":"<p>But let's look back at the earlier example with a <code>getFoo()</code> function:</p> <pre><code>Function getFoo() as Variant\n    ...\nEnd Function\n</code></pre> <p>This function could return a scalar or an object. If the code is looking up something and actually finds something value, we check if it's an object and, if it is, use <code>Set</code> otherwise omit it.</p> <p>But what if we don't get something? The calling code knows whether it expects an object or a scalar. But our <code>getFoo()</code> function doesn't. I'll leave that problem with you for now, but it is a problem that generic code needs to handle.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/13/directories-challenge/","title":"Andre's Directories Challenge","text":"<p>I am sure that anyone who uses LotusScript has been following the excellent blog posts of Andre Guirard. Recent blog posts on large arrays and queues have been particularly interesting for those of us working on VoltScript. His blog post on a Queue data structure ended with a challenge. The root of the problem is that the LotusScript <code>Dir()</code> function is not recursive. Without a parameter it gives the next file or directory relative to its last call. So you can't have one loop using <code>Dir()</code> and an inner loop that also uses <code>Dir()</code>. Andre uses a Queue class to perform FIFO (First In First Out), which achieves what's needed, but not as required:</p> <p>For this task, if you care about the order of the returned results, this probably isn\u2019t what you want. You\u2019d prefer all the files in a given folder to be grouped together with the files from its subfolders, so those last two should be switched.</p> <p>Exercise for the reader: what data structure could be used instead of a queue to track the work and give the result in the desired order?</p> <p>Proper Collection and Map classes has always been on our wishlist for VoltScript. Immediately after the Factory Tour in Chelmsford in September Devin, Rocky and I stood at a whiteboard and scoped out Collection and Map classes. We took note of how other languages handle the challenge, most notably Java and JavaScript. We also bore in mind a key requirement placed on our work - simplicity. LotusScript has flourished because people without a Computer Science background - like myself - were able to add powerful coded functionality to our databases without the complexity of other languages.</p> <p>I've got extensive experience of Java and an abiding memory of Collections and Maps in Java is a PDF that lists which class to use for which purpose. For VoltScript we have just two classes - Collection and Map. When creating a Collection, you define whether it is sorted (every Collection is ordered, even if that is insertion order) and whether values should be unique. So what about Queues (FIFO) and Stacks (LILO)? We don't need them, because Collections have <code>getAndRemoveFirstRaw()</code> and <code>getAndRemoveLastRaw()</code>. This means a Collection can be used as a Queue, or a Stack, or both at the same time (\"nibbling\" the Collection from both ends, if there's such a need).</p> <p>So could our Collection solve Andre's problem?</p> <p>There was only one way to find out - after the latest round of major refucktoring, and revisiting unit tests to prove I'd fixed everything I'd broken!.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/13/directories-challenge/#converting-andres-code","title":"Converting Andre's Code","text":"<p>I initially thought about creating a Map for each directory holding its files / subdirectory names. But it turned out I was over-complicating it. Andre's idea of a Queue pointed me in the best direction, which proved very simple to adapt.</p> <p>First I created a directory structure:</p> <ul> <li>C:\\temp\\foods with \"tomatoes.lss\", \"bakery\", \"meat\", \"fish\"</li> <li>In bakery directory, three files \"baguette.lss\", \"bread.lss\", \"croissant.lss\"</li> <li>In meat directory, three files \"beef.lss\", \"lamb.lss\", \"pork.lss\" and a directory \"fish\"</li> <li>In fish directory, three files \"basa.lss\", \"dover_sole.lss\" and \"salmon.lss\"</li> <li>In vegetables directory, two files \"broccoli.lss\" and \"sprouts.lss\"</li> </ul> <p>Constants were swapped out (<code>NEWLINE</code> for <code>Chr(10)</code> etc), the path in Initialize amended accordingly, but no changes needed to <code>IsLikeAnyOf</code> function.</p> <p>The only changes needed were in the FileSearch function, which actually looks very similar:</p> <pre><code>Function FileSearch(ByVal baseDir$, ByVal patterns$) As Variant\n    Dim delim$, afile$, results$, adir$, patternArr\n    Dim workColl As New Collection(\"STRING\", Nothing, False, False)\n    Dim files List As Boolean\n    Dim dirCount as String\n    ' find what path delimiter the caller prefers.\n    If basedir Like \"/*\" then\n        delim = \"/\" ' linux style path\n    ElseIf basedir Like \"\\*\" Or basedir Like \"[A-Z]:\\*\" Then\n        delim = \"\\\" ' windows style path\n    Else\n        Error 20030, \"FileSearch needs an absolute filepath.\"\n    End If\n\n    patternArr = Split(LCase(patterns), \"|\")\n    ' internally we want all filepaths to end with the folder separator\n    If baseDir Like \"*[!\\/]\" Then baseDir = baseDir &amp; delim\n    workColl.add basedir\n\n    ' while there are directories in the queue, process the first directory.\n    Do While workColl.hasContent\n        dirCount = 0    'Reset, we're on a new directory\n        adir = workColl.getAndRemoveFirstRaw\n        Erase files\n        afile = Dir$(aDir &amp; \"*\", 6) ' all files but not directories.\n        Do Until afile = \"\"\n            files(afile) = True ' remember the names of the files\n            If IsLikeAnyOf(LCase(aFile), patternArr) Then\n                results = results &amp; Chr(10) &amp; adir &amp; afile\n            End If\n            afile = Dir$\n        Loop\n\n        ' now find the names of each subfolder.\n        afile = Dir$(aDir, 18) ' this will include files also\n\n        Do Until afile = \"\"\n            If afile &lt;&gt; \".\" And afile &lt;&gt; \"..\" Then\n                If Not IsElement(files(afile)) Then\n                    ' this is a folder. Add it to work queue for processing on later iteration.\n                    Call workColl.insertAt(adir &amp; afile &amp; delim, dirCount++)\n                End If\n            End If\n            afile = Dir$\n        Loop\n    Loop\n    FileSearch = Split(Mid$(results, 2), Chr(10))\nEnd Function\n</code></pre> <p>On line 3, I use Collection class instead of Queue. The constructor sets \"STRING\" as the data type of entries, third and fourth parameters mean non-unique and insertion order only, and as a result no comparator needs to be passed as the second parameter. Line 5 declares an additional counters required further down - we'll see why later!</p> <p>There are then no changes until we get into the Do Until loop. We reset dirCount to 0, we'll see why later. The next line sets <code>adir</code>. Whereas Andre's code calls <code>Get</code> to retrieve the first entry from the Queue, we call <code>getAndRemoveFirstRaw()</code> - raw because the function returns a Variant, because we don't know at compile-time what datatype a given implementation of a Collection will contain.</p> <p>There are then no other changes until the middle line of the second inner Do Until loop. Andre calls <code>workq.put adir &amp; afile &amp; delim</code>, which adds it to the end of the Queue. This is what causes the problem with ordering of the output. But the Collection class allows us to run <code>Call workColl.insertAt(adir &amp; afile &amp; delim, dirCount++)</code>. If the Collection is not sorted, we can use the <code>insertAt()</code> function - for sorted collections the Comparator should decide where entries get put, not the developer. Here we leverage the <code>dirCount</code> variable from earlier. This means we're inserting all directories at the start of the Collection. As a result, we can iterate any subdirectories before processing sibling directories. And this is why we reset <code>dirCount</code> as we process each directory, so subdirectories are always put at the start. You may notice <code>dirCount++</code> - we need to increment dirCount as we add subdirectories, so we're not always inserting at element 0.</p> <p>This then gives us the following output on Windows:</p> <pre><code>C:\\temp\\foods\\tomatoes.lss\nC:\\temp\\foods\\bakery\\baguette.lss\nC:\\temp\\foods\\bakery\\bread.lss\nC:\\temp\\foods\\bakery\\croissant.lss\nC:\\temp\\foods\\meat\\beef.lss\nC:\\temp\\foods\\meat\\lamb.lss\nC:\\temp\\foods\\meat\\pork.lss\nC:\\temp\\foods\\meat\\fish\\basa.lss\nC:\\temp\\foods\\meat\\fish\\dover_sole.lss\nC:\\temp\\foods\\meat\\fish\\salmon.lss\nC:\\temp\\foods\\vegetables\\broccoli.lss\nC:\\temp\\foods\\vegetables\\sprouts.lss\n</code></pre>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/13/directories-challenge/#linux","title":"Linux","text":"<p>On Linux things get a little more complicated. On Windows it appears <code>Dir()</code> automatically processes files in alphabetical order (I deliberately created them in different orders). However, on Linux that's not the case.</p> <p>So how do we address that?</p> <p>We create <code>New Collection(\"STRING\", Nothing, False, True)</code> and put the files and directories temporarily in that Collection, then process that Collection. The final parameter <code>True</code> ensures it the Collection is sorted (sort happens on insertion). The second parameter means it uses a default Comparator, sorting on datatype then <code>CStr()</code> of the value - the sorting on datatype is not required, but will have minimal performance impact on a small collection, so produces simpler code.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2022/12/13/directories-challenge/#summary","title":"Summary","text":"<p>The Collection and Map classes are very much POC at this point. As I mentioned, I've just completed the latest major refactoring, including adding new methods and leveraging some newer constructs we've added to the core language. They've only been used in one project so far, and that's not complete. So the tyres (\"tires\", for my US colleagues) have barely been kicked on them. Because we're using new language constructs, the classes could not be leveraged currently, although there is nothing done that could not be reverse engineered to work with existing LotusScript. And although the unit tests include creating large Collections, the primary focus at this point is functionality, not performance with large Collections / Maps. All of this points to the fact that as much as developers might be excited by this work, it's far too early for it to be made available and LotusScript developers' use cases might not align with ours.</p> <p>But it'e encouraging that our architecture solves real business problems.</p> <p>And I haven't even mentioned the synchronicity of Andre mentioning that <code>Dir()</code> is not recursive. But that's a completely different topic.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/","title":"Introducing Bali Unit Testing Framework","text":"<p>Today we've released two projects, one on HCL's GitHub and a fork on OpenNTF's GitHub. It will be useful to give a bit of background, as well as an introduction the the project.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#why-two-projects","title":"Why Two Projects","text":"<p>The version on HCL's GitHub is the original, Bali Unit Testing Framework, a unit testing framework written in and for VoltScript, the evolution of LotusScript currently in development for Volt MX Go. The documentation, as usual, is available on GitHub. There are a number of places where the code leverages new language functionality from VoltScript. As a result, although the code will be usable from VoltScript, the code cannot be used as-is by Domino developers.</p> <p>Therefore a fork has also been created on OpenNTF's GitHub and adapted for LotusScript, Bali Unit Testing Framework. The documentation is available also on GitHub. This can be used by Domino developers. The documentation has also been slightly modified, to be relevant to LotusScript developers.</p> <p>A corresponding project has been created on OpenNTF's website, Bali Unit for LotusScript, although this is just points across to the GitHub repo.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#aim-of-the-project","title":"Aim of the Project","text":"<p>The aims of the project have been:</p> <ul> <li>To make it easy to do unit and integration testing for LotusScript and VoltScript developers.</li> <li>To provide reporting that can be included in your project documentation.</li> <li>To provide output that can be used by CI/CD tooling to fail a build.</li> <li>To allow tests to be written quickly.</li> <li>To ensure it is clear what each test is intended to do.</li> <li>To provide common assertions out-of-the-box.</li> <li>To provide flexibility so that custom tests can be created.</li> <li>To ensure additional code can be run before and after each or all tests.</li> <li>To make writing tests fun.</li> </ul>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#rtfm","title":"RTFM","text":"<p>The documentation should cover everything you need to know, including FAQs, walkthroughs of code samples, full sample scripts, samples outputs and API documentation.</p> <p>But if you don't like reading, stay tuned for a video on OpenNTF.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#what-is-it-for","title":"What Is It For?","text":"<p>This may seem to have an obvious answer. Although it can and should be used for integration testing as well as unit testing, it's actually gained a second purpose. The framework has also proved very useful for validating content.</p> <p>When using the framework for testing, tests can output HTML files for your project reporting, in a format that will be familiar for users of e.g. JUnit. But it can also output a standard XML format that can be parsed by CI/CD systems like Jenkins.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#why-no-nsf","title":"Why No NSF?","text":"<p>There are a wide variety of reasons why it shouldn't be in an NSF, many very relevant for VoltScript. If the OpenNTF fork had, instead, moved the code to an NSF then there would be little point in it being a fork, because the differences would be so significant that it would be prohibitive to merging changes.</p> <p>The current format also gives greater flexibility for version handling.</p> <p>Yes, it requires copying and pasting the code into an NSF. But that is a common approach for Domino.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/02/01/baliunit/#what-is-the-future-for-the-projects","title":"What Is The Future for the Projects?","text":"<p>Bali Unit has already been used in a variety of places, so is quite mature. There have not been a huge number of changes since the code was initially written. No additional assertions have been needed for many months, and we don't anticipate adding others. The more assertions there are, the more a developer has to wade through in documentation and typeahead. An <code>assertTrue()</code> or <code>assertFalse()</code> can often handle more sophisticated checks.</p> <p>There is also no intention to merge the projects. Until the VoltScript language enhancements are available in Domino, there will always be changes in the core code. There are also some subtle but important differences in documentation.</p> <p>But where appropriate, fixes and enhancements will be considered for merging between the two projects.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/03/01/bali-unit-videos/","title":"Bali Unit Testing Framework Videos","text":"<p>Two weeks ago four videos were posted on OpenNTF's YouTube channel walking through the functionality of the Bali Unit testing framework. The videos are:</p> <ol> <li>Introduction to Bali Unit Repo: where to find the code and documentation, ask questions etc.</li> <li>Bali Unit Basic Introduction: the structure of test runner and test suites.</li> <li>Bali Unit Tests and Assertions: a deep dive into creating tests and the various assertion functions that are available.</li> <li>Bali Unit Advanced Functionality: custom testers and custom code to run before all tests, before each test, after each test, after all tests.</li> </ol> <p>These are intended to give you all the information you need to use the test suite, although full documentation is available in the repo, at https://openntf.github.io/bali-unit/.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2023/07/15/postman-importance/","title":"Postman: The Crucial Tool for Any Microservice Developer","text":"<p>My history with REST development is long. In 2018, before I joined HCL, I delivered a session \"Domino and JavaScript Development Masterclass\" at IBM Think. When I posted about my development tools in 2017, Postman - then just a Chrome plugin - was key amongst them. Then in 2020 I posted an overview of Postman. And just as both John and I used Postman as a crucial tool when building the application we showed at our session at IBM Think, it's the tool that should be used by anyone doing anything with microservices - which is what every scope your create for Domino REST API is.</p> <p>What does the following code do?</p> <pre><code>Dim session as New NotesSession\nDim db as NotesDatabase\nSet db = session.currentDatabase\n</code></pre> <p>Any Domino developer knows what this does. The more knowledgeable Domino developer may know that it calls C APIs to get the currently open Domino database. And for most Domino developers, this is a black box.</p> <p>But what does the following code do?</p> <pre><code>Dim session as New NotesSession\nDim httpReq as NotesHTTPRequest\n\nSet httpReq = session.CreateHTTPRequest()\nCall httpReq.get(\"https://httpstat.us/200\")\n</code></pre> <p>Some may think this calls a URL. The more knowledgeable Domino developer may know it triggers a process that uses C APIs to create a Domino object to store settings which will then be passed to libcurl to call a URL. Why is this important? Because although the first part is a black box, the second part most definitely is not.</p> <p>When doing anything that makes HTTP calls, there's a part that's never a black box.</p> <p>And that is crucial.</p>","tags":["Domino","Domino REST API","LotusScript","VoltScript","REST Clients"]},{"location":"blog/2023/07/15/postman-importance/#postman-use-1-postman-console","title":"Postman Use 1: Postman Console","text":"<p>The key with any HTTP request is understanding full details of headers and body of the HTTP request and response. There are different ways of finding those details for a successful request.</p> <p>If you've got a web application, the browser's developer console can show you those details. This should never be under-estimated or neglected, for working out what a successful HTTP request should look like (if the application is working as expected) or cross-referencing (if the application is not working as expected).</p> <p>For the hardcore developer, making curl requests via command line can give verbose details. Those who have seen Stephan's sessions on Domino REST API will be familiar with his command line example.</p> <p>Postman is a low-code IDE where authentication, headers and body can all be defined by filling in fields. But this is where it's good to use the menu option View &gt; Show Postman Console (or the shortcut code Alt + Ctrl + C on Windows, \u2318+Option+C on Mac).</p>","tags":["Domino","Domino REST API","LotusScript","VoltScript","REST Clients"]},{"location":"blog/2023/07/15/postman-importance/#postman-use-2-cross-reference","title":"Postman Use 2: Cross Reference","text":"<p>If you have a working Postman request and are coding programmatic access, Postman is still extremely useful to cross-reference if the code does not work. This is something I've used for coding Domino Online Meeting Integration, configuring integration services in Foundry, coding Node.js integration with GitHub, and coding dependency management in VoltScript.</p> <p>Many APIs, including Domino REST API, provide a Postman collection and environment, as I showed in the April's OpenNTF webinar. But the Import button in Postman allows you to pass a variety of content, including an OpenAPI 3.0 spec from either a downloaded file or a URL link. For Domino REST API, if the Domino developer is configuring a schema and scope for a third party, the OpenAPI page allows the third party to retrieve the OpenAPI specification for the specific scope.</p>","tags":["Domino","Domino REST API","LotusScript","VoltScript","REST Clients"]},{"location":"blog/2023/07/15/postman-importance/#postman-use-3-generate-code","title":"Postman Use 3: Generate Code","text":"<p>But what if you're coding the application yourself? This is the third use of Postman. From a Postman request, you can ggenerate a code snippet for a variety of languages and frameworks, including C#, Java, JavaScript, Node.js, C, PHP, Python and Swift. It's worth bearing in mind that frameworks evolve, and the standard JavaScript <code>fetch</code> now seems pervasive and the preferred option for Node.js as well.</p>","tags":["Domino","Domino REST API","LotusScript","VoltScript","REST Clients"]},{"location":"blog/2023/07/15/postman-importance/#javascript-fetch-and-chunked-processing","title":"JavaScript, Fetch and Chunked Processing","text":"<p>While mentioning <code>fetch</code>, it's worth drawing attention to Stephan's recent blog post about handling chunked responses. Domino REST API uses these for certain endpoints like views. At this point, the choice of front-end technology affects the code that is needed for specific endpoints. And being able to test in something like Postman is important to understand headers involved.</p> <p>But of course when you get to this point, it's just standard JavaScript / Java / fooLanguage for processing any REST service, nothing specific to the platform of the REST API. And if you're using a popular framework with a vibrant community of blogged and open sourcers, it's likely that it will be easy to find the answers.</p>","tags":["Domino","Domino REST API","LotusScript","VoltScript","REST Clients"]},{"location":"blog/2023/09/18/xpages-beyond-nsf/","title":"XPages Elements Beyond the NSF","text":"<p>I do very little XPages these days, I have one application for personal usage that rarely gets updated. But it's when applications rarely get touched that changes elsewhere on the server can have a big impact. I'm going to cover two here, the first raised by a discussion on Discord last week.</p>","tags":["Domino","XPages"]},{"location":"blog/2023/09/18/xpages-beyond-nsf/#osgi-plugins","title":"OSGi Plugins","text":"<p>One big element is with OSGi plugins. Although it is possible to install OSGi plugins as feature and plugin files directly on the server, the recommended approach from the start has been to use an Update Site database. The skills required to build OSGi plugins were always advanced, requiring strong Java skills, typically Eclipse, an understanding of extension points and the code required to load them. Nathan Freeman's XSP Starter Kit project was extremely useful for giving sample code to contribute OSGi plugins - or \"extension libraries\" as they became known - to particular areas of the XPages runtime. And without Nathan's great work and commitment to open source, the most prevalent community extension library, OpenNTF Domino API, would probably not have flourished. But before this, there was the first extension library, the XPages Extension Library, and its companion incubator project, ExtLibX.</p> <p>Since Domino 9.0.1 FP10 ExtLib has come installed on the server, reducing use of the Update Site database to heavy-duty XPages customers, with applications probably supported by very knowledgeable Domino developers. But there are probably still some customers who implemented XPages applications in the days when ExtLib was only installed via Update Site database. And here there is a risk.</p> <p>We know with agents and XPages, signer of the design elements is important. But it's also important for Update Site database as well. Just as a design element will not work if the signer is not authorised to run code, so too the OSGi plugin will not be loaded if the signer of the relevant documents in the Update Site database is not authorised to run code. This can result in unusual behaviour, particularly as the signer might tend to be an admin who could leave the company and not be expected to have dealt with code. Remove the rights of the ID and ExtLib will not be loaded. And if an older version of ExtLib was signed by someone else, that will suddenly take precedence.</p> <p>The best way to troubleshoot is <code>tell http osgi ss</code> + relevant library, e.g. \"com.ibmxsp.extlib\" (#TheresABlogPost). Then cross-reference the versions against the Update Site database. As that blog post highlights, \"ACTIVE\" is the key. And to resolve the problem, you can use the \"Sign All Content\" button.</p>","tags":["Domino","XPages"]},{"location":"blog/2023/09/18/xpages-beyond-nsf/#themes-etc","title":"Themes etc","text":"<p>Another advanced XPages approach was using themes. This allowed the developer to set styling for specific components across the whole application, by adding a Theme design element to the NSF. But what if you wanted the same styling across the whole server? Well then the theme could be added to /data/domino/html directory. Older servers may still have \"lotus\" and various \"oneui\" folders there. But you could create your own theme and upload it directly to the server. <p>However, like some other areas of the Domino server directories, the theme files can also get deleted during an upgrade. So it's important to have a backup and copy it back after an upgrade. On more than one occasion I've had a call saying that styling has changed on an application, and the culprit was a server upgrade without being aware of these directories. The pro admin will no doubt be aware of what should or should not exist for a standard Domino server install, and check before any upgrade. But with customers who are not as committed to the product, this may not happen.</p>","tags":["Domino","XPages"]},{"location":"blog/2023/10/03/domino-js-redux/","title":"Domino and JavaScript Development MasterClass Redux","text":"<p>Nearly six years ago I delivered a session at IBM Think with John Jardin called \"Domino and JavaScript Development MasterClass\", a session I also delivered at Engage in May 2018. The session was delivered at the request of Andrew Manby, product manager for Domino. It was shortly after the first Domino Jams when JavaScript development against Domino was a key point of focus and it pre-dated any specific developments to expose Domino for JavaScript development. Now seemed a good point to revisit and review, not only in the context of Domino REST API and Volt MX Go, but also with the future of VoltScript in Volt Foundry.</p> <p>I approached John because I didn't feel qualified to talk about building a JavaScript application. From my experience of customers and community, both at the time and since, that is not unusual for Domino developers. Nor is it specific to JavaScript - my impression is most Domino developers are equally unfamiliar with building web applications on Java, PHP or any technology outside Domino. So the separation seemed apt.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#architecture","title":"Architecture","text":"<p>The architecture of the application we built is very important:</p> <p></p> <p>The JavaScript application talks via REST, reading and writing JSON. But the architecture very specifically talks about an API Gateway. Slides 15 and 16 go into greater detail about this. REST means basic CRUD access, whereas the focus on an API gateway emphasised coding for data quality, managing security, and ensuring specific data items are only editable via specific APIs - not via basic CRUD REST API.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#development-approach","title":"Development Approach","text":"<p>With John, our API began with writing a Swagger specification using OpenAPI 3.0. We then each coded against that, using the agreed specification. John coded the React application, possibly using mock data based on the OpenAPI spec. Meanwhile I coded the API gateway on the Domino side.</p> <p>But this is where the challenge lay at the time and now - coding the API gateway on the Domino side. Neither then nor now is there a good approach for coding an API gateway on Domino. And it's not a problem specific to Domino for Domino developers. If they chose another NoSQL database, there is still no technology stack that would be easy for Domino developers to build an API gateway. The problem is not the database, it's coding an API gateway.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#coding-vs-configuration","title":"Coding vs Configuration","text":"<p>The good news is Domino developers don't need to code the API gateway. Enter Domino REST API. Now, instead of coding the API gateway, they can just configure one. It should come as no surprise that Domino REST API handles the data quality requirements, security concerns and different exposures to the same data structures. Requiring certain data and validating input options, as well as exposing the same form with different configurations, these are all covered in Form Access Modes. No surprise there, I remember sitting with Stephan in the Manila lab working through how they would work. OAuth was added by Stephan, applications manage API keys and secret, and different scopes/schemas for the same database would handle managing scheduled endpoints differently. \"Additional logging of transactions\" is not currently handled out-of-the-box, but there are ways that could be handled with an additional scope/schema.</p> <p>But you don't create a Domino REST API based on an OpenAPI specification. But because it's configuration instead of coding, why would you? Instead, Domino REST API produces the OpenAPI specification for you. Once schema and scope are created, you can view the OpenAPI 3.0 spec for the scope from the \"OpenAPI v3\" link from the Domino REST API homepage. And, because Domino REST API is API-first, you have an API to download the OpenAPI specification for any third-party consumer.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#ui","title":"UI","text":"<p>For our session, John built the UI application in React. Thankfully (and obviously) he shared the code with me. But even still, I would not profess to being capable of coding my next non-Domino application in React. Nor would I have a clue how best to secure it properly or scale accordingly. This was always the problem to solve for Domino developers building a web application outside Domino.</p> <p>In a scenario mirroring what we had - a third party building the web front end - there's not a need. And yet the Domino developer can be confident that their Domino REST API configuration will ensure the third-party developer doesn't do anything they shouldn't do with either the database or the rest of the Domino server. Because, unlike a CouchDB or MongoDB server, the Domino server is the same web server that delivers the REST API gateway. And this is the crucial difference between Domino and other NoSQL database servers.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#enter-volt-mx-go","title":"Enter Volt MX Go","text":"<p>But what if the Domino developer wants to create the web application UI outside of Domino? Yes, of course there is no reason not to deploy the UI using XPages or Nomad Web, if they fit the use case. But if there is a need to use something else and it needs to be built by the Domino developer, whether it's React, Spring, Vaadin, Vue.js, or something else - many Domino developers including myself would not feel comfortable building with something.</p> <p>This is why considerable effort has been invested into adding Domino-specific capabilities to Volt MX to produce Volt MX Go. Volt Foundry corresponds nicely to Agilit-e / Node-RED from the IBM Think session, being the middleware server that the UI talks to. The Iris application build in Volt MX Go corresponds to the React app John built. Iris and Foundry allow you to configure that middleware and build the front-end application with minimal web development skills. And integrating with an external database is standard for Volt MX, unlike many low-code platforms, particularly cloud-based ones, which can seek to lock data into their platform to ensure longevity of the front-end app.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#configured-vs-coded","title":"Configured vs Coded?","text":"<p>So the whole architecture John and I coded in 2018 can today be built by a Domino developer with zero coding on the API gateway side and minimal knowledge of JavaScript and HTML/CSS on the front-end.</p> <p>But coming back to that bullet point about \"additional logging of transactions\", let's look further down the road. Volt Foundry orchestration services may solve this problem, I've only recently begun to understand what they offer and how they work. But in terms of coding, VoltScript in Foundry will certainly allow you to call multiple Domino REST APIs in a single Volt Foundry REST service. But that's future.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2023/10/03/domino-js-redux/#why-domino-as-just-a-database","title":"Why Domino as Just a Database?","text":"<p>If XPages or Nomad Web deliver a web front-end that fits the needs, there is no reason not to use them. Then Domino is more than just a database, it's an application server as well. But that doesn't suit all use cases and all customers. And if it doesn't, why use Domino at all?</p> <p>If a developer has the skills to build everything without Domino, they're probably not reading this. If they're a Domino developer, and definitely if they're a Notes (sic) developer, they probably don't have the skills to set up and secure another database, they almost certainly don't have the skills to code an API gateway using another database. Domino REST API and Volt MX Go will get to the same outcome John Jardin and I achieved in early 2018.</p>","tags":["Domino","Domino REST API","Volt MX Go"]},{"location":"blog/2024/03/19/java-debugging/","title":"Eclipse Java Debugging","text":"<p>When XPages came to Domino it introduced many Domino developers to Java. Because the IDE was based on Eclipse, it also introduced Domino developers to standard elements of Java development on Eclipse.</p> <p>At the time I started developing Java in XPages, Java classes were created in a standard Java approach - in Package Explorer view, create a source folder in WebContent/WEB-INF and add it to the Build Path, create packages in the folder and Java classes within each package. This has the advantage of being standard for Java development when developing JAR or WAR files, as well as introducing developers to Project &gt; Properties, Build Path as well as project-specific settings for project-specific Java Code Style and Compiler settings. It's well worth developers being familiar with these, I've come across them regularly in bigger Java projects. Even though many of the Java projects I've worked on use Maven, occasionally I've also had to add jar files to the build path based in a <code>lib</code> folder, as I also did for XPages. Although the JAR design element was added, the standard approach was still my preferred option, which has been helpful since.</p>","tags":["Domino","Eclipse"]},{"location":"blog/2024/03/19/java-debugging/#class-files","title":"Class Files","text":"<p>Package Explorer was my standard view for coding Java and it's also been the standard view when developing Java projects in Eclipse. Project Explorer is another view, which also shows up in the Debug perspective. The main use for the Project Explorer view is being able to filter to display .class files. It's very easy when developing Java to forget that building the project generates the .class files and its those which are run. So knowing how to find them and view them can be useful.</p> <p>And it's important to understand the build process. When developing Java applications in Eclipse, the default option of \"Build Automatically\" makes a lot of sense. In XPages it can cause problems for two reasons and because of two very specific differences between an XPages application and a Java application.</p> <p>Firstly, when developing Java applications, the code is local and built locally. In XPages, the code is already in an NSF which can be on a remote Domino server. Consequently, building the application requires processing the .java files on the remote Domino server, generating the .class files and pushing them to the remove Domino server. So performance can be significantly different to developing a Java application.</p> <p>Secondly, the Java application is exported as a WAR and deployed to a Java application server like Apache Tomcat or exported as a JAR and pushed to a remote environment where it is started using a JRE (Java Runtime Environment). In XPages, the Java code is in the NSF which is on the Domino server, so already in the deployment environment. So Build Automatically can impact the application in its running environment. This could just remove scoped variables (sessionScope, applicationScope etc) leaving other users confused with unexpected behaviour. Or it could sign code with an ID that does not have permissions to run the code, breaking the whole application.</p> <p>So understanding the .class files that are generated is important for understanding the developer experience and managing your expectations.</p>","tags":["Domino","Eclipse"]},{"location":"blog/2024/03/19/java-debugging/#debugging-java-applications","title":"Debugging Java Applications","text":"<p>One of the major benefits of Java in XPages was the ease - and standardisation - of debugging. Those who developed Java agents in Domino databases will be aware of Bob Balaban's insightful \"two-headed beast\" approach to debugging Java agents. This requires copying the code into Eclipse and providing a separate entrypoint to the code.</p> <p>But debugging Java in XPages - or Java OSGi plugins - was more standard. The approach for developing OSGi plugins is completely standard, adding an OSGi launch configuration, which generates a pde.launch.ini in <code>&lt;Domino&gt;/data/domino/workspace</code> directory.</p> <p>The approach for Java development is mostly standard. One other notes.ini setting is more standard - <code>JavaDebugOptions=transport=dt_socket,server=y,suspend=n,address=8000</code>. The options passed - transport, server, suspend and address - are standard for debugging any Java application. The part before it is slightly less standard, it hides the reference to <code>agentlib:jdwp</code>, the Java Debug Wire Protocol. But the setting is essentially the same.</p> <p>The other except notes.ini setting is less standard, <code>JavaEnableDebug=1</code>. This starts XPages - and potentially other Java programs running on the Domino server - in debug listening mode. In remote Java application servers like Apache Tomcat this is not done with a configuration setting, but by adding <code>jpda</code> when launching the server, e.g. <code>catalina.sh jpda start</code>. This can be used to debug any code deployed to the remote server, as long as you have the code locally. For example, in HCL Volt MX, this can be used to debug a Java integration server deployed to the Volt Foundry server.</p> <p>Once the debugger is attached, the process of debugging is standard. Breakpoints are added by double-clicking the gutter. A caveat worth noting here is that a breakpoint that does not show a tick when the debugger is attached means the code there is not deployed to the server. This could happen if the project has not been built or if the latest code is not deployed to the server in the location it's being triggered from.</p> <p>The debugger and functionality available is standard for all Java development. When the breakpoint hits, you are prompted to switch to the Debug perspective unless Eclipse has been set to always switch to it. I would recommend setting this, I've only ever used the Debug perspective when a breakpoint has hit.</p> <p></p> <p>Let's get acquainted with the Debug perspective. The Debug view in the left-hand part of the page shows the stack trace. There's not much to see here, but in a more complex Java application you will see much more. This can help you see where the code went before getting to the breakpoint, as well as navigating up the stack to see the variables etc at the point the code called the next method in the stack.</p> <p>This brings us to the right-hand views.</p> <p>Variables shows us all the variables available in the current class. If they're scalars, it will just display the value. If they're objects (instances of a class), you can expand them to see the properties for that instance. And you can expand down multiple levels. This is extremely important for checking whether the values are what you expect.</p> <p>Breakpoints is pretty self-explanatory - it shows the breakpoints and allows enabling / disabling / deleting them. But if you click on a particular breakpoint, you can also set additional settings, like conditions for when the breakpoint should trigger. This can be particularly useful if a breakpoint could get hit multiple times because it's in a loop or triggered for different objects. You can set a condition so that it only hits on the nth iteration of a loop or if an object has a specific property value. The less skilled developer may just keep clicking continue as it hits the breakpoint over and over (which can still be a valid approach for a one-off debug), but the expert developer will identify the specific settings at the time to stop and set a condition accordingly.</p> <p>Expressions is another useful view, possibly not as heavily used as it might be. This allows you to set new expressions - e.g. <code>args.length</code> or <code>System.getent(\"foo\")</code> - which will be calculated every time you debug and allow you to see the value. This can speed up debugging by providing quick access to specific properties or the result of specific Java code.</p> <p>So this gives us ways to find a property of an object or run specific code every time you're debugging. But more often than not you'll want to test the outcome of a method or run alternate code at a specific breakpoint. This is where the Debug Shell comes in. This allows you to enter code with a <code>return</code> statement and evaluate the code on-the-fly. The code must return something and it will print the result immediately below the code. You can keep chunks of code here to re-run on future occasions. But unlike the Expressions view, they are not evaluated automatically on future debug runs.</p> <p>To evaluate code you need to highlight the relevant code - which can be multiple lines - then click \"Display Result of Evaluating Selected Text\", the second button on the right-hand header of the pane, the one that looks like a piece of paper with a \"J\" on it. The result will be printed immediately below the selected code. The fourth button - \"Create a Watch Expression from the Selected Text\" will add the code to the Expressions view, to auto-calculate it on future debug runs.</p> <p>One caveat when writing code in the Debug Shell view is that classes need to have been imported into the current class in order to just use the class name (e.g. <code>String</code> or <code>HashMap</code>). If not, you can still use them provided they're on the classpath, but you need to use the fully-qualified name, e.g. <code>java.util.HashMap</code>.</p>","tags":["Domino","Eclipse"]},{"location":"blog/2024/03/19/java-debugging/#debugging-server-side-javascript-in-xpages","title":"Debugging Server-Side JavaScript in XPages","text":"<p>I've done very little debugging of Server-Side JavaScript in XPages. Since I started writing Java, my usual rule of thumb was to write no more than a few lines of SSJS for any property / event. And all business logic was written Java, for ease of development, debugging and troubleshooting. So I'm not able to comment on what is similar / different for debugging SSJS. But I believe there are a number of similarities.</p>","tags":["Domino","Eclipse"]},{"location":"blog/2024/04/03/engage-2024/","title":"Engage 2024","text":"<p> Later this month I will be attending Engage 2024. It will be a bittersweet experience. Engage was the first conference at which I spoke, a session that was way ahead of its time, highlighting the power of repeat controls in XPages and advocating against using View Panels. Ironically, at Engage this year, one of the sessions I'll be delivering has some similarities. But I'll cover the sessions I'm involved in chronologically.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#volt-mx-go-for-domino-developers-workshop-monday-22nd-april-1330-hilton-antwerp-old-town","title":"Volt MX Go for Domino Developers Workshop - Monday 22nd April 13:30, Hilton Antwerp Old Town","text":"<p> First up I'll be helping deliver the Volt MX Go Workshop, alongside any other HCLites with relevant expertise. Volt MX Go has a lot of power and benefit for Domino developers, even more so this year as VoltScript will be added to Volt Foundry, allowing developers with LotusScript skills to create custom REST services. There are a variety of other sessions about Volt MX during the conference, from HCLites, business partners and customers. They will highlight the power of Volt MX Go for integrating with non-Domino non-HCL solutions as well as creating web, mobile and desktop native applications.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#engage-super-pro-code-mode-web-apps-without-frameworks-tuesday-23rd-april-1130-room-b","title":"Engage Super-Pro Code Mode: Web Apps without Frameworks - Tuesday 23rd April 11:30, Room B","text":"<p> Next I'll be speaking with the uber pro-code expert, Stephan Wissel, about how to develop modern web applications without frameworks, why you might want to and when you might not. The world of web development has moved on massively since XPages was launched 15 years ago, and hugely in just the last few years. There's a lot that can easily be done in vanilla web development now. But there are also valid scenarios where it's easier to leverage a framework. Domino REST API has also opened the doors to new opportunities.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#introduction-to-voltscript-tuesday-23rd-april-1330-room-d","title":"Introduction to VoltScript - Tuesday 23rd April 13:30, Room D","text":"<p> After a swift lunch, I'll be back speaking with Devin Olson about VoltScript. We've got a huge amount to pack into 45 minutes, showcasing the progress towards general availability later this year. The inclusion of VoltScript as a first-class language in Volt Foundry means all Domino developers can more easily create custom REST services and unleash the power of their Domino data securely for all technologies and developers.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#openntf-guide-to-open-source-for-hcl-products-wednesday-24th-april-800-room-a","title":"OpenNTF Guide to Open Source for HCL Products - Wednesday 24th April 8:00, Room A","text":"<p>The first OpenNTF session is highlighting a wide variety of open source solutions for the broad range of HCL products. You may know some, but we guarantee you won't know them all. So join the OpenNTF board members - and maybe some special guests - to find out some of what's available.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#openntf-repair-cafe-wednesday-24th-april-1530-room-b","title":"OpenNTF Repair Cafe - Wednesday 24th April 15:30, Room B","text":"<p>We'll have 45 minutes to discuss your questions and topics, whether admin- or developer-related, at the in-person repair cafe. We can share best practices on any topic.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/04/03/engage-2024/#wrap-up","title":"Wrap-up","text":"<p>It's bound to be a busy conference, as always. And I look forward to catching up with friends and colleagues.</p>","tags":["Domino","Conferences","Volt MX Go","Editorial","React","Web Components","CSS"]},{"location":"blog/2024/05/01/ai/","title":"Adventures in AI","text":"","tags":["Editorial","VoltScript","AI","GitHub Copilot"]},{"location":"blog/2024/05/01/ai/#recent-scenarios","title":"Recent Scenarios","text":"<p>Those who were at Engage will have seen some of the experimentation I've been doing with AI in the context of VoltScript. In the OGS Jason demoed how I used it to provide code for a loop, correcting it with information about APIs specific to VoltScript. Before my VoltScript session, I showed two videos demonstrating how I've used AI to add value to the VoltScript coding experience, firstly by checking unit tests for code coverage and then by checking code complexity of functions in VoltScript Collections. These are code quality features that are provided out-of-the-box for more standard languages like JavaScript and Java, features that have long been on my wishlist for VoltScript, but features that will not be available in the near future. But GitHub Copilot filled a gap to provide the required information as a stop-gap.</p> <p>But what was probably not as apparent was another use case where I leveraged GitHub Copilot. In the session with Stephan Wissel we showed how the <code>fetch</code> JavaScript API can be used in more modern browser in place of libraries like Axios. I've used both <code>fetch</code> and Axios a bit, often within Node.js. But running a script doesn't necessarily provide the best demo. A simple JavaScript web app is not necessarily rocket science, but not something I've been used to doing regularly. So I leveraged GitHUb Copilot to create that boilerplate and create a better demo, while writing manually the code that was the main focus of the demo.</p> <p>Another scenario where I used GitHub Copilot in recent weeks was troubleshooting Java code. The specific code is not important, the use case was. Some rather complex code across multiple Java classes was not providing the required output. Traditional debugging narrowed down the problem to a specific chunk of code. Looking at the code a potential hypothesis spring to mind. Typically, at this point I would resort to Google and search for blog posts. However, in this case I asked a targeted question of GitHUb Copilot concerning what the default behaviour would be for the code. This not only quickly verified that my hypothesis was correct, GitHub Copilot also added value by recommending potential solutions.</p>","tags":["Editorial","VoltScript","AI","GitHub Copilot"]},{"location":"blog/2024/05/01/ai/#lessons-learned","title":"Lessons Learned","text":"<p>Those who have attended my sessions and looked at the session materials will know that I'm not interested just in surface details, but what's under the surface. The lessons I took from my experiences of and use of GitHub Copilot also went below the surface. When I presented my learning internally, I drew upon the two pictures below:</p> <p>Those who were in my session at Engage will recognise the second image. It is the unfinished obelisk, quarried in Aswan in Upper Egypt during the reign of the pharaoh Hatshepsut. It would probably have stood alongside the first obelisk, the Lateran Obelisk, which was in Karnak before being moved to Rome.</p> <p>So what do obelisks shaped over 3500 years ago have to do with AI?</p> <p>Well, if you just use AI to generate code that you copy and paste, then move onto the next job, what you get is the Lateran Obelisk - something that looks pretty and makes you look impressive. But all you've done is provided something that looks pretty and added little value.</p> <p>The unfinished obelisk is not so pretty, but because of where it is, we know obelisks were quarried and shaped where the stone was originally found. We also see the tool marks, which tell us a lot about how they were shaped. It gives us key knowledge to knowing how the obelisks were made and - if we wanted to create an obelisk like the ancient Egyptians - it contributes knowledge to achieve that. Using AI in the same way not only achieves an outcome but can help a developer understand how to achieve the required outcome.</p> <p>The unfinished obelisk was also a picture I used in my session with Stephan at Engage. One of the things I've come to realise is that there are different kinds of IT professionals - those who want to complete a task and those who want to learn. The former are likely to leverage AI to \"do their job for them\". It is the attitude of a junior programmer who will only ever be a junior programmer. And these are the kinds of employees that AI could just replace. But the journey I have been on has been driven by understanding how things work the way they do. It's what's required to progress from a junior programmer to a senior developer. It's not just about generating code, it's about architecting solutions and being able to adapt. And this kind of person is more likely to be able to add value, will know when to use AI, and will be able to combine traditional and novel techniques to achieve a better outcome.</p> <p>It's an interesting time with AI and one which will create transformations in business, just as the previous \"artificial intelligence\" innovation, the chatbot, has. The key to being able to adapt is the same as it always has - understanding.</p>","tags":["Editorial","VoltScript","AI","GitHub Copilot"]},{"location":"blog/2024/07/22/research/","title":"Developing for Research","text":"<p>It's been nearly five years since I joined HCL Labs, progressing currently to Associate Director - Research. In that time I've been involved in:</p> <ul> <li>setting up HCL's Open Source Project Office</li> <li>researching the state of rich text editing on the web (as I covered in a session at Collabsphere in 2020)</li> <li>leading the modernisation of language, extensions and tooling of LotusScript as VoltScript</li> <li>integrating VoltScript into Volt MX Foundry as a first-class language adapter</li> <li>web components</li> <li>and a variety of other projects</li> </ul> <p>This has covered some degree of coding in a variety of languages and frameworks:</p> <ul> <li>Java</li> <li>Vert.x</li> <li>Node-RED</li> <li>Markdown</li> <li>Node.js</li> <li>React</li> <li>AngularJS</li> <li>LotusScript and VoltScript</li> <li>JavaScript</li> <li>HTML, CSS, SCSS</li> <li>Ruby</li> <li>Rust</li> </ul> <p>But there are significant differences between traditional product-focused development and development for research. And it's useful to highlight those differences, because it takes a certain mind-set and skill-set.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#language-skills","title":"Language skills","text":"<p>As a professional developer, the aim is to gain expertise with whatever language and/or framework is relevant for the job and career you desire. Some languages remain popular for a long time and a developer who began with C/C++ or Java could go throughout their career without using another language. However, frameworks have a shorter lifecycle, so a Java developer may have worked with a number of Java frameworks throughout their life, but something like Spring may take up a large proportion of a professional developer's career. JavaScript has changed much more in a shorter period of change, and someone who began with JavaScript in the 2000s may already have spent years coding with Dojo, then jQuery, AngularJS and React. ANd that JavaScript may now have morphed into TypeScript.</p> <p>However, as a researcher, the need is different: rather than expertise in a specific language and/or framework, more important is the ability to implement or extend whatever technology is required. One project may require extending a Java product. Another may require manipulating a React UI with some functionality. A third may require changing both the JavaScript front-end of an application and its Java backend - there is not the luxury of having multiple researchers splitting the work. Yet another may demand that you look at open source options to achieve a specific requirement, and restricting options to specific languages means you cannot properly evaluate the merits of all options. And there may not be the luxury of multiple researchers to \"divide and conquer\". The ability to be a \"jack of all trades\", to know \"enough\" and get something working regardless of language or framework is crucial.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#adaptability","title":"Adaptability","text":"<p>This is one aspect of adaptability - the ability to use a variety of languages and frameworks, to know enough to get something working and know who to talk about for extra expertise. But it's more than that. Research is a lot about being agile, embracing the challenge rather than being paralysed by fear and diving in. Once you've started, another key aspect is identifying multiple possible approaches, getting something working quickly and, if you hit a dead-end, identifying the cause so you can resolve it or pivoting quickly to another option.</p> <p>Many research projects I've been on have been no more than a matter of weeks, build a proof of concept quickly that can be demoed and handed on. So speed is important, tapping into expertise where required to clear blockers, and having excellent troubleshooting skills to solve problems. Even if you can't identify the actual cause, being able to rule out options and reduce the potential causes quickly avoids mental paralysis and exhaustion.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#understanding","title":"Understanding","text":"<p>At the heart of troubleshooting is understanding. It's not about knowing the languages in-depth or being an expert. It's about understanding how the technologies work, knowing where to look when things go wrong, being able to see what worked, knowing what tooling to use to troubleshoot the technology, and knowing how to use alternative techniques to pinpoint where in the process it went wrong.</p> <p>And this is the most crucial aspect: understanding that everything in IT is a process. A REST service call is a request, which often uses a specific library, travels over the HTTP protocol, hits a server, which receives and processes the request, then sends a response, which again must travel back over the HTTP protocol, is received as a response and processed by code. There are a variety of places where it can go wrong and a variety of places where you can verify how far it got before the problem occurred. Using Postman or another language can verify if the problem is in the code making the request or somewhere else. Using a Swagger spec on the relevant website and browser developer tools can allow you to check what it should look when it works. Understanding how to troubleshoot and verify is crucial to effective research development.</p> <p>Moreover being able to read the existing code and work out what it's doing is key. Sometimes documentation may be lacking or the process may be very convoluted. Being able to track a complex process through a lot of code is important. And in order to do so quickly, you need to be able to identify what you need to understand and what you don't, and you need the willingness and mental discipline to ignore what you don't need to know. Research does not afford the time to understand everything in the code or get something working in \"the right\" way.</p> <p>Being able to understand how things work can also help you see the parallels between one technology, language or framework and another. This can help you get to understand them more quickly and maybe solve problems without deep knowledge - because the same approach worked somewhere similar.</p> <p>And when you need additional expertise, a good research developer will ask the questions to fill their knowledge gap rather than just get an answer. It's about learning how to know, not learning per se.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#i-have-a-plani-have-part-of-a-plan","title":"\"I have a plan...I have part of a plan","text":"<p>The other aspect of understanding also builds on the knowledge that everything is a process. There is a requirement to achieve a specific outcome. The approach for a research developer is to break that into the required steps. Then it's a case of identifying possible solutions to achieve each step in the process.</p> <p>But it's a mistake to need to need to solve the whole problem up front. Part of a plan is enough, but preferably more than 12%! One of the skills I value most in myself is that I think throughout the development process. The solution is constantly evolving and I may say or read something or some code that triggers an idea for solving part of the process.</p> <p>Two other important aspects here: firstly, I mentioned identifying possible solutions, plural. That's important, because there may be more than one way to achieve a particular step. Identifying alternatives allows you to choose the best option, which may be prioritised by speed, language, familiarity or a number of other deciding factors. A prototype may deliberately choose an approach that needs to change when it's taken to production, and there are a number of scenarios where that's valid. But knowing it's a step in the process and understanding alternatives is important to handing it over to someone else.</p> <p>Secondly, the plan should not be linear. It's perfectly valid to have a plan for most steps but be missing solutions for a step in the middle. As long as you understand what needs to feed into it and what needs to be fed out of it, you can always solve that problem later on. Tackling the process in a linear fashion is more likely to go down a dead-end and spend more time before you realise it. Looking at the elephant from both ends may make it easier to meet in the middle.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#prototyping","title":"Prototyping","text":"<p>A plan is good. But code is better. In HCL Labs we are always encouraged to write code and show running code rather than slides, even if what is shown is not complete. One notable example I remember is the VoltScript Testing Framework. I began creating a spec for it, but quickly abandoned that to dive into coding. I had an idea of what I wanted it to do, I had enough of a plan to try something. Three and a half days later, I had a fully working prototype of the whole framework demoed on our weekly sprint, and it's only needed a few small enhancements since.</p> <p>Getting a prototype working has a number of benefits. It helps solidify ideas, turning theory into proof. It helps identify areas that can be or need to be improved. And it helps others visualise what a solution will look like, which may be better than just a written spec to prompt limitations or enhancements.</p> <p>Another important aspect here is AI. It is a hot topic currently and using AI-generated code may not be appropriate for product-related work. But research and prototyping is about speed. So use of AI is very valid. But again we come back to understanding - it's important to understand the code provided rather than just focus on achieving a specific outcome.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#try-it-show-it-improve-it-not-perfect-it","title":"Try it, show it, improve it - NOT perfect it","text":"<p>Building on the previous two sections, the key here is to have an idea what you want to do and try it. That way you find out quickly whether it can work and get a better idea of its strengths or weaknesses. Build something, show it, get clarity if you're on the right track, and iteratively improve it. A lot of aspects may be hard-coded for a start.</p> <p>And more importantly, quite a bit may be hard-coded when you stop. As a research developer, an important aspect to embrace is that it is not your job to perfect it. It's not your job to code it in a best practice way. It's not your job to write all the unit tests, though some tests may speed up development or refactoring of the prototype. And it's certainly not your job to solve all the problems. Research is done in order to hand on to someone else. But what's important is to have at least a first pass on problems not addressed and areas for improvement.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#dare-to-fail","title":"Dare to fail","text":"<p>Research is, by its very nature, bleeding edge. It's about trying to do something that doesn't have an easy answer, and may not even have a possible answer. It's important to embrace failure and learn from it, whether that's hitting a dead end and having to pivot to a different solution to part of a process, or learning that the intended target was unachievable. There is the oft-quoted comment from Thomas Edison that he didn't fail 700 times, he succeeded in proving 700 ways not to build a lightbulb. Learning how not to do something is still valuable, even if not as valuable as finding out how to do it. And knowing why is was not possible is important, because IT is constantly evolving and that single cause of failure may be resolved or no longer relevant in the future.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#document-it","title":"Document it","text":"<p>Another important point is that research may not get picked up straight away. So document what it's doing, document what needs to be considered by whoever comes next, document what you haven't solved, and document assumptions you've made.</p> <p>As a researcher, it's not typical to be working on a project for months. It happens, but it's not the norm. Chances are you'll be on something completely different, in a different technology and maybe for a totally different project. The documentation may avoid questions or, more importantly, mistaken assumptions or code just used as-is when you intended it to be improved or limitations addressed.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#re-use-it","title":"Re-Use it","text":"<p>The other reason for documenting is that the research may not get picked up at all for its intended purpose. But the whole project or some of the learning may become reusable somewhere else later on. I've seen a number of cases where research projects have been resurrected for a completely different purpose and, in some cases, be used in that new incarnation in production systems. Be aware of the research that's done and where some or all of either the learning or the code can be used. Even a failed project may be relevant later on, when circumstances change or technology solves a problem that was previously unsolvable.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/22/research/#embrace-change","title":"Embrace Change","text":"<p>The final important point I've learned is to embrace change. It is said the only constant in IT is change. In research, change is constant. The need to pivot at a moment's notice is crucial, as is the need to work on multiple things at the same time, to be aware of new learning and current trends. The role of research developer is not so much about expertise, it's about problem solving, about deconstructing and reconstructing.</p>","tags":["Editorial","VoltScript","Community"]},{"location":"blog/2024/07/31/drapi-405/","title":"Domino REST API Proxy Problems","text":"<p>Earlier this week I was working with Domino REST API for a personal project and encountered what appeared to be a bug. It was a very strange issue, but one that had a simple cause that was ultimately easy to verify. Shortly after joining HCL I wrote a blog post on troubleshooting support. If you didn't read that blog post at the time or need a refresher, it's probably one of the most important blog posts I've ever written and most of what I covered in that blog post was relevant to solving this problem. Coincidentally, what I wrote about understanding in my most recent blog post was also crucial.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#the-bug","title":"The \"Bug\"","text":"<p>First off, I regularly see developers hit a problem and instantly claim it's a bug, often with little investigation and even less understanding of the process. Worst still, developers are typically at the receiving end of such claims. You would think they would be more judicious or maybe it's a touch of hubris.</p> <p>Unless I have looked in depth at the code - and even often when I have - I would hope I always say that I think I've found a bug. After all, the way I think I expect the code to work may not be what was intended and rarely is a system commented with total clarity. Key to proving a bug is reproducing the symptoms, and key to reproducing the symptoms is minimising external factors and creating a simple reproducer.</p> <p>In this case, Domino REST API's `/lists/{name}' endpoint was resulting in unexpected results. For certain views on the server I was using, e.g. a view with a name \"vwFoo\" was returning expected results. But for hidden views, e.g. \"(luKeys)\" I was getting a 405 \"Method Not ALlowed\" error when accessing via Postman.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#verify-behaviour-from-another-entrypoint","title":"Verify Behaviour From Another Entrypoint","text":"<p>Domino REST API provides Swagger specs (/openapi/index.html). This allows you to test authentication and test APIs, generically or for a specific endpoint. This was a part I coded many years ago, to allow developers to share the OpenAPI specification with third parties and also test live on the server. I've used DRAPI from Postman for many years, I helped review the Postman collections and environments before the early access release. So I was fairly certain I had not made a mistake, I thought the brackets for the hidden view name needed escaping and I thought I had done it correctly. But the Swagger page allows me to test programmatically without the chance of user error, by selecting the view name from a combo box instead of typing the name manually. It also lets me see the curl command used, which I can then use elsewhere and allows me to cross-reference the intended URL. This confirmed the URL format - <code>/lists/%28luKeys%29?dataSource=myScopeName</code>.</p> <p>Postman has an option to view the actual requests and responses, by going to the menu and selecting View &gt; Postman Console. This confirmed what was being sent from Postman was the same as the website, and both approaches returned 405 Method No Allowed error.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#know-expected-behaviour","title":"Know Expected Behaviour","text":"<p>There are a few known returns, which we can easily verify:</p> <ul> <li>if it's right, content is returned.</li> <li>if there are no documents accessible, an empty array is returned.</li> <li>if the scope name is wrong, an error \"A scope definition with that name cannot be found\" and 404 response code is encountered.</li> <li>if the view name does not match something configured, the error says \"The list is not configured for this database\" and a 404 response code is encountered.</li> <li>using a verb other than GET gives a 405 Method Not Allowed error.</li> </ul> <p>But the viewI was using was definitely GET.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#verify-behaviour-in-another-environment","title":"Verify Behaviour in Another Environment","text":"<p>If you do not already use Domino on Docker, I would highly recommend it for quickly setting up an environment with standard setup. It can help reproduce in a vanilla environment. Checking a hidden view in a different database on a different server did not reproduce the problem. But the server was slightly more up-to-date. The difference might be important. Upgrading the server to the latest official release took a little longer for various PEBKAC reasons and trying to set up a debuggable server to step through the code - a luxury not all have - hit some different challenges with pro code developer environment setup. Fortunately someone else was able to confirm they could not reproduce the problem.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#understand-the-process","title":"Understand the Process","text":"<p>Here we encounter the most fundamental rookie mistake I made. I saw DRAPI in my browser, saw Postman request and response, knew DRAPI was on the server and assumed no other relevant steps. Rookie error, there's always a process. This particular server had nginx as the proxy server, a fact I remembered when prompted by my colleague.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#know-your-logs","title":"Know Your Logs","text":"<p>Looking at the access logs for nginx showed the 405 error being returned there and the URL that was being sent, with %28 and %29.</p> <p>Next step was to look to see if the request was being blocked by nginx or routed to DRAPI. The point of reference was the domino-keep.log in IBM_TECHNICAL_SUPPORT directory. Here I saw logging that verified the request was hitting DRAPI, had a url <code>/lists/(luKeys)?dataSource=myScopeName</code> and, more importantly, the 405 response was being sent from DRAPI.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#know-expected-behaviour-part-two","title":"Know Expected Behaviour Part Two","text":"<p>Again, at this point, the assumption may be a DRAPI bug. But I had not verified expected behaviour. There are two ways to do so:</p> <ol> <li>Test in a working environment and compare logging.</li> <li>Test without intermediaries.</li> </ol> <p>My colleague, Ron, was able to share his log output and proved that DRAPI expects the view name without URL decoding, so \"%28luKeys%29\", not \"(luKeys)\". Running the curl command (remember what the Swagger page provided us?) from the server Domino was running on, using localhost instead of the host name, allowed me to confirm the logging. It also confirmed the problem was not a bug, and not DRAPI, but nginx configuration.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/07/31/drapi-405/#the-cause","title":"The Cause","text":"<p>So nginx was decoding the %28 and %29 rather than just passing the URL as received. I'm not an nginx expert, so some googling led to a variety of StackOverflow questions. At some point I hit this answer or one similar. The configuration had:</p> <pre><code>location /api {\n   proxy_pass http://localhost:8080/api;\n}\n</code></pre> <p>The solution was changing it to:</p> <pre><code>location /api {\n   proxy_pass http://localhost:8080;\n}\n</code></pre> <p>This prevented nginx decoding (or normalising) the path parameter, which avoided the 405 error. Problem solved, no bug, environmental issue, not caused by DRAPI, just triggered by a call to DRAPI and returned by DRAPI.</p>","tags":["Domino REST API","Support","Errors"]},{"location":"blog/2024/08/01/xpages-tags-renderers/","title":"Understanding Tags and Renderers","text":"<p>There are a few people in software development who have shaped my career by their approaches. Three of those whom I'm particularly glad to have known are Nathan T. Freeman, Tim Tripcony and Jesse Gallagher. Although Tim and Nathan are no longer with us, I am fortunate enough to have experienced sessions by and with them, got to know the open source code and videos they created, and worked with them on open source projects. They were brilliant developers, willing to spend time with those who wanted to learn and give back. Fortunately, I'm still able to draw on the knowledge of Jesse and the XPages community should be rewarding him for his continued work. And when you encounter such clever people, it's foolish not to want to learn all you can.</p> <p>In the case of Nathan, Tim and Jesse, and in the context of this blog post, three pieces of work are particularly relevant:</p> <ol> <li>Nathan's XPages Starter Kit</li> <li>Tim's NotesIn9 video on global custom controls</li> <li>Jesse's Notesin9 video on custom renderers</li> </ol>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#tags","title":"Tags","text":"","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#xpages","title":"XPages","text":"<p>The XPages interfaces for \"tags\" for developers are two-fold:</p> <ul> <li>Design pane uses no-code approach of components picked from a visual list of available components which developers drag onto a canvas and use a properties panel to hard-code values, map to Java getters and setters, or enter pseudo-code which gets stored as string values with a prefix to denote the ScriptEngine Java should process the string with. If you don't recognise this description, I'm talking about Server-Side JavaScript.</li> <li>\"Source\" pane uses a low-code approach of XML tags with attributes which can be manually typed into the XML or set via the properties panel.</li> </ul> <p>Why \"Source\" pane? Because, if you understand what XPages is, you'll know the XML there is not the actual source code. The actual source code is auto-generated as Java classes and can be found in Package Explorer view, in the \"Local\" folder. Here you'll find Java objects for each tag where the XML attributes map to properties and the attribute values passed to Java property setters.</p> <p>Of course the actual actual code that is run is not even this, it's the compiled byte code in .class files.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#vaadin","title":"Vaadin","text":"<p>Some years ago I dabbled a bit with Vaadin. Like JSF, the framework behind XPages, it's also Java code, with Java objects also mapping to components. Vaadin also has a low-code WYSIWYG editor, like the design pane in XPages. The concept is the same.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#html","title":"HTML","text":"<p>The XML in XPages looks a lot like XHTML, which also has opening and closing tags with attributes. Indeed standard XHTML tags like <code>&lt;span&gt;</code> and <code>&lt;div&gt;</code> can be appear to be used in XPages.</p> <p>But the truth is, you're not actually putting an HTML span or div tag on your XPage, as you'll see if you inspect that Java class that holds the actual source code. If this comes as a surprise to you, there's a blog post for that which I wrote over ten years ago. Go read it, I'll still be here when you come back.</p> <p>XHTML in standard .html files does the same kind of thing as tags in XPages. Most of the time there's not much between what you see in your source code and what you see on the web page. Think of a script tag or a span tag with style or other attributes.</p> <p>But then there are other tags that don't work in such a basic way. Create an HTML page, add an input tag with type set to \"date\". Now preview it in a browser, inspect the element and see what happens when you click on it. What's happened with what's inside the input tag? We'll come back to that later.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#web-components","title":"Web Components","text":"<p>Web components use the same concept again as XPages. You use custom tags in your HTML and define attributes on the tags. So how do those custom tags map to the content in a browser?</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#renderers","title":"Renderers","text":"<p>Renderers are the key to all this magic. This manages how the XML or XHTML gets rendered on the browser.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#xhtml","title":"XHTML","text":"<p>Everything in HTML is backed by a renderer. If browsers didn't use a renderer, you'd see the HTML text in the browser instead of a UI. There's another layer of rendering as well, UI in the browser's main area and XHTML on the Elements tab of a browser's developer tools.</p> <p>A script tag, span tag or other basic HTML tag just gets rendered as is. The browser knows those specific tags and handles mapping functionality or user interface.</p> <p>But let's come back to that input tag with type set to \"date\". The Elements tab has something that wasn't in the HTML file when you click into the field: something called a Shadow DOM with a bunch of tags. How did this get here? The browser's renderer converted the tag to a much more complicated layout, multiple tags. And the HTML elements get modified depending where the cursor is.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#renderers-all-the-way-down","title":"Renderers All The Way Down","text":"<p>But it's not just a browser that has a renderer. The same concept is used throughout web development. XPages, Web Components, Vaadin: they all use renderers to convert the tag or Java class to HTML comprising one or more tags.</p> <p>And like the browser's renderer, sometimes it's a one-to-one mapping. XPages <code>&lt;xp:br&gt;</code>, <code>&lt;xp:hr&gt;</code> or <code>&lt;xp:head&gt;</code> tag map to HTML <code>&lt;br&gt;</code>, <code>&lt;hr&gt;</code> or <code>&lt;head&gt;</code> tags. Others are more complex. <code>&lt;xp:viewPanel&gt;</code>, <code>&lt;xp:dataView&gt;</code>, <code>&lt;xp:navigator&gt;</code> have renderers that write out much more complex HTML.</p> <p>Web components do the same thing: the tag maps to a JavaScript class that has a <code>render()</code> method, some of which adds event listeners which map back to JavaScript functions which may modify the rendered HTML even more.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#rendering-children","title":"Rendering Children","text":"<p>And whether it's XPages, Vaadin or Web Components, sometimes the tag contains child tags. The renderer needs to handle three parts: what HTML to render at the start, rendering child tags, and what HTML to render after rendering the children.</p> <p>And this is the difference between a renderer and a tag.</p> <p>A tag has its own renderer and automatically handles converting the tag, its attributes and its children. You get what it gives, in the order it gives it.</p> <p>A custom renderer (which you'll have to write for a Web Component) allows you to choose what HTML gets written and in what order.</p> <p>And in XPages, the highest level tag is <code>&lt;xp:view&gt;</code> with its renderer \"com.ibm.xsp.renderkit.html_basic.ViewRootRendererEx2\".</p> <p>But Domino developers have been writing custom renderers with web development for years.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/01/xpages-tags-renderers/#domino-agents","title":"Domino Agents","text":"<p>Every time you write a Domino web agent, you're writing a custom renderer. Everything that you return using <code>Print</code> statements is your renderer. Sometimes it will be rendered as a JavaScript instruction, for example to redirect to a new page. Sometimes it may be a JSON object. Sometimes it will be HTML. Sometimes the content type might tell the browser to render an Excel spreadsheet.</p> <p>It's all a custom renderer, written by the developer.</p> <p>And though the technologies are different, the fundamentals of the process that happens under the hood are the same.</p>","tags":["Domino","XPages","Web Components","Vaadin"]},{"location":"blog/2024/08/15/xpages-web-1/","title":"XPages to Web App Revisited: Part One - Introduction","text":"<p>Many years ago I wrote a series of blog posts on the topic of XPages to web app. At the time my target technology was Vaadin running in an OSGi plugin on Domino HTTP server (initially) and then CrossWorlds - Daniele Vistalli\u2019s innovative approach to use Domino data via OpenNTF Domino API on a Websphere Liberty server running as a sidecar to Domino. My experience of developing with Vaadin lagged behind the technology, because it quickly evolved not only to Java 8 (and undoubtedly beyond) but used annotations which required Servlet 3.0.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#technology","title":"Technology","text":"<p>Today there are a variety of options for web application. Adjacent to Domino is Jesse Gallagher\u2019s JakartaEE approach. Domino REST API can host applications as well. Frameworks like Angular and React have gained prominence. JSP is still seen in some places, but seems to have slipped from prominence. But after my session at Engage with Stephan Wissel, and particularly the rapid evolution of browser support over the last few years, my target is traditional web, hosted in Domino REST API\u2019s server.</p> <p>Let\u2019s address the elephant in the room. Why not Volt MX Go? This is a small app, one user and I don\u2019t have a hosted environment or want to set one up for something this small. I know Volt MX, and this is about growing my learning. I want to see what I can do with a vanilla web development approach.</p> <p>But it is still very relevant for Domino developers looking to use Volt MX Go - or indeed any other technology. Domino REST API will be the mode of access, as it is for Volt MX Go. And the primary focus of this series will be about developing a web application, not taking a Notes or XPages user interface and reproducing it on another framework. Whether you\u2019re using a JavaScript framework like <code>fetch</code> or Foundry operations is barely relevant, both could be used for any web development framework. The CSS techniques are relevant if developing for modern browsers. Even the web components can be used within Iris.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#requirements","title":"Requirements","text":"<p>As I said, the app is a small app - a handful of forms and views. There is no workflow for the data, it\u2019s just data entry, retrieval for lookups and basic analysis. The original app is still expected to work in parallel. I could manually verify the data, or I could write integration tests - in LotusScript, using the unit testing framework developed for VoltScript - to target data provided by XPages and from the new app. The same tests should provide the same results. If they do - and if the tests have been written well - the new app will be proven to be working correctly with the database and providing valid data.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#navigation","title":"Navigation","text":"<p>When the original application was developed, a traditional XPages approach was taken - bootstrap theme for responsiveness, navigator component in custom controls for main navigation and administration views, linking to Data View components.</p> <p>The new app will not be doing any of this!</p> <p>Why?</p> <p>First up, this app has been in use for a few years now. If you\u2019re modernising an app that\u2019s been around any length of time, the first step should be working with the users to see how they use it, their most common actions, their pain points, the logical steps they take to do their job with the app. I\u2019m not talking about technical steps (go to this view, click this button, fill in the fields etc). I\u2019m talking about use case tasks.</p> <p>In this case it\u2019s:</p> <ul> <li>create a ship spot. If the ship is not picked from previous entries, a ship is created as well as the spot.</li> <li>look for a ship by name to edit it or see when it was spotted. There are a lot, typical approach is jump to page x, then page y until the relevant ship is found. This is not efficient, but it's the typical view-driven approach.</li> <li>names change occasionally, but there is a unique ID. There\u2019s no easy way to find by that currently.</li> <li>look for ships by line, which is a combo box. There are not many options.</li> <li> <p>create a port and/or country. These were originally under an \"admin\" area, but the role is not relevant. Currently the flow is:</p> <ul> <li>create a ship spot, realise the port doesn't exist.</li> <li>ship spot needs saving.</li> <li>switch view.</li> <li>create port.</li> <li>if country doesn\u2019t exist, create country.</li> <li>then go back to create port</li> <li>then edit spot and add port.</li> </ul> </li> </ul> <p>This should raise red flags to any developer. Modernisation of the app should improve this and there is significant scope to do so. How? That\u2019s not sure yet, but improving this should be a must. - each trip, create a trip with start and end date. Previous trip needs changing to inactive. - retrospectively, look at what what created during the trip. Currently this is a table of data. Again, there\u2019s an opportunity to improve this. - trips may no longer be required, so I'll leave that functionality for now. - spots are not regularly reviewed directly, occasionally for spots a day or two previously. - countries, ports and ships are used for pickers in ship spots.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#analysis-of-requirements","title":"Analysis of Requirements","text":"<p>A few things should be apparent here:</p> <ul> <li>scrolling through views is not done. So why make that the entry point to documents?</li> <li>access to documents is targeted on a few use cases.</li> <li>there are a few \u201ccreate\u201d actions that are key.</li> <li>creating a port, ship or a country is only done when adding to a ship spot.</li> <li>the original navigation and editing paradigms were based on the framework. This takes a great deal of self-awareness and an ability to challenge instinctive choices. But it's key to modernisation and an optimal user experience.</li> <li>when the original XPages application was developed, creating and viewing multiple related docs in a single UI was a shift from Notes Client. Data View gives innovative layouts. But the rest of the user experience was typical for a Notes or XPages app. And most significant of all, it doesn\u2019t fit how the app is used.</li> <li>the app is used on mobile and network connection can be patchy.</li> </ul>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#challenging-instinctive-choices","title":"Challenging Instinctive Choices","text":"<p>Every developer should fully understand the decisions they instinctively make because of the platform and framework they are using. As stated, views are the standard entry point for Notes Client. And having lots of documents in a view is normal, especially when business owners fail to prioritise archiving of data.</p> <p>But think of the most commonly-used web application: Google. Do you scroll through a view of documents categorised on a type or sorted? Of course not. You search. How often do you page through 100 matches? You don\u2019t. If it\u2019s not in the first couple of pages, you typically try a different search. Think of another web application: Wikipedia. The same is true. Web applications have a paradigm that has become standard - and become standard since many Domino developers built their first Notes database. But it\u2019s become standard for a good reason: the approach of users for some years now is \"find something quickly, do what you need to do with it, move on\". Modernising a Domino application should take into account how user expectations have changed - even if some of the users are more resistant to change. Similarly modernising a spreadsheet into a web application will not satisfy if it just reproduces the spreadsheet experience in a browser.</p> <p>But that\u2019s not the only aspect to bear in mind for Domino and Notes. The data is programmatically accessed locally. Move to a web application hosted elsewhere and you will typically need to pull data over the HTTP protocol. If you don\u2019t appreciate the difference there, your application will probably fail and you must bear at least part of the blame. \u201cBut the customer said\u2026\u201d is no excuse - you are the expert, it is your job to educate.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#user-experience","title":"User Experience","text":"<p>So the \u201cnavigation\u201d will not link to views of documents of a specific type sorted in a particular way. It will focus on specific actions. The primary action is not to look at a specific view, so the home page will be tiles. There are only a small number, so it will work nicely on a home page. The home page will display the current active trip, clicking it will open the trip, with actions to \u201cswitch trip\u201d or create a new one. It\u2019s not likely to be a huge list, so a massively scrolling view makes no sense.</p> <p>Comparison is not done, so again no point in using a view.</p> <p>\u201cFind ship\u201d will allow searching by name (with wildcards) or unique reference. We\u2019ll not commit to a specific layout of results yet. Let\u2019s see what options we have. Results should be limited, so re-sorting makes no sense. The key purposes are checking it exists, editing the ship or viewing it.</p> <p>If we can cache some data - trip, ports, countries, maybe even ship names - across the whole (front-end) app, we will get better performance. Only ships will be a reasonably significant data set, so we will not commit yet. None of the data has any sensitivity, so we don\u2019t need to worry about that. Because there is only a single user, we don\u2019t need to worry about webhooks to update the list or background web workers to check for updates. We won\u2019t provide a way to clear the cache, we\u2019ll keep it clean and add it if required. We\u2019ll fetch the data after login (async), providing a get-out. And the XPages app is not going anywhere (yet), so worst case scenario the old app can be used instead. This gives us zero downtime.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#technologies-involved","title":"Technologies Involved","text":"<p>Let\u2019s talk technologies here. We\u2019ll use FlexBox (just html and css) for layout. We\u2019ll use web components for many of the piece-parts of the application (login form, \u201cview entry\u201d, \u201cdoc\u201d). We\u2019ll use event subscribers to interact with JavaScript function. We may use some other external web components, maybe not.</p> <p>We\u2019ll use CSS. But CSS has evolved massively since the 2000s, helped by the fact that most end users have modern browsers. No longer do we have to develop for Internet Explorer 6 - indeed developers are probably not developing for Internet Explorer at all. Businesses and all consumers should be on Edge, if they\u2019re not on another browser.</p> <p>And we\u2019ll use fetch to talk to Domino REST API to perform CRUD operations. An early step will be create DRAPI modes and get the swagger spec. We will also use mock data at times for local coding and testing.</p> <p>This is about learning, and quite a few aspects will be new to many Domino developers, maybe some web developers, and certainly to me. It will certainly not answer all the questions you will have as you develop web applications. For example, I don't have to worry about 100% up-time or supporting massive numbers of users.</p> <p>But I hope others in the Domino community will pick up that baton and share their expertise. A community is smarter because many share. When people stop sharing, the smart look for new challenges, the rest stagnate and the community will never recover.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/15/xpages-web-1/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/20/xpages-web-2/","title":"XPages to Web App Revisited: Part Two - Dev Tools","text":"<p>Many Domino developers may not have used anything other than Domino Designer. We\u2019re stepping into a different world of development here. So we\u2019ll be using different tools. Plural.</p> <p>First up, source control is not optional. That mean git and a client. Pick your favourite, mine is SourceTree. I know Stephan Wissel is a fan of Git Tower. But there are times to use command line, and if you want to call yourself a pro code developer, you should always be willing to use command line.</p> <p>The IDE I\u2019ll use for coding HTML and CSS is Visual Studio Code. I use it daily but to get the most you need to have it configured properly. That means extensions. For me, it\u2019s a no-brainer to immediately install NotesSensei\u2019s collection of extensions. Just search NotesSensei in the VS Code marketplace. Another option for some may be Atom. Eclipse is not my choice for HTML and CSS, even though it\u2019s my go-to for Java development.</p> <p>I\u2019ll be developing locally. That\u2019s fine until you want to include JavaScript files. Then you get bit by CORS problems (which I'll cover here too). Of course Visual Studio Code has a solution: Live Server. This starts a local web server to host the folder open in VS code, typically running on port 3000. Go to http://127.0.0.1:3000 and you avoid CORS problems. It even picks up changes as you type.</p> <p>This is great if you only want to test on your laptop's browser. But there are times I need to check how things look on a mobile device. You can switch user agent in the browser, but that doesn't show you the keyboard you see in the device. Maybe there are desktop options. But if you want to test from a different device, ngrok is very useful. It gives an external IP address that allows me to connect to my laptop from my mobile device(s). When I tried that, I got an error \"upgrade required\" instead of the web page. It appeared that this was caused by Live Server, but I didn't spend long investigating. As a workaround, NodeJS's http-server allowed me to start a local HTTP server that provides access to a specific directory. This solved the problem.</p> <p>Of course you need a browser, and you need to get comfortable with the developer tools. Looking at the console with be key, as will be inspecting HTML elements and looking at the CSS. ANd browsers' developer tools have added yet more over recent years.</p> <p>Every web developer hits problems with CSS. And we'll be digging into HTML and browser JavaScript aspects that might be unfamiliar. My go-to is w3schools but Mozilla's API docs are also useful, if a little brief. But if you have an AI integrated like GitHub Copilot, that may be a preferable choice. And you will want to know what browsers support what you use, so https://caniuse.com is a key tool.</p> <p>And a lot of what I\u2019ll be using is based on the session I did with Stephan Wissel at Engage this year, so you\u2019ll want https://stwissel.github.io/super-procode-mode/#/. There is always more stuff I find. And web components is a very new area. Fortunately I have access to O'Reilly's library. To get some background on web components I read \"Web Components in Action\" by Benjamin Farrell. It's a few years old now, but very useful.</p> <p>We\u2019ll use a browser to set up Domino REST API schema. And you\u2019ll want a REST client to test with, my preference has been Postman. There is now a VS Code extension for Postman, but there are benefits of using the desktop client. The downside is you have to connected to the cloud to use it and it pushes all your collections, environments etc to the cloud. Just today Stephan Wissel pointed me in the direction of bruno. I've long held the opinion that when NotesSensei speaks, you listen. That advice has helped me learn a lot over the years, so I intend to look into that, probably separate from this development.</p> <p>Next up, we'll get started on preparing to build the application, with configuring Domino REST API and digging into the DRAPI configuration and some aspects many developers may not have used.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/20/xpages-web-2/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/","title":"Framework App to Web App: Part Three - Frameworks and the Internet","text":"","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#a-brief-history-of-web-development","title":"A Brief History of Web Development","text":"<p>Until now, the series has focused on XPages. That's understandable considering my previous series that this is inspired by. However, recently it's become apparent that much of this series is relevant to a much wider audience than just XPages developers. Most web developers are used to developing with a specific framework. That's understandable considering the history of the last 20+ years. But that means this series is relevant to a much wider audience than HCL Domino developers using the JSF-based XPages framework.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#the-rise-of-corporate-it","title":"The Rise of Corporate IT","text":"<p>In the 1990s the internet started to become more prominent. Home computers were still mainly focused around gaming, but desktop computers began to become more prominent in companies. By the time I started my first office jobs in the late 1990s, desktop computers and email were starting to gain prominence. But a main use of word processing products was for generating paper letters that were mailed to other companies and customers. Emailing documents or spreadsheets was not a common task. The internet was used mainly for websites, applications were in desktop platforms like Lotus Notes, Microsoft Access or Excel.</p> <p>Slowly web applications started to spring up and exploded in the 2000s. Developers learned hacks to work around differences in how Internet Explorer, Firefox and (later) Google Chrome implemented or ignored the slowly-evolving web standards. In UK, Safari and Mac computers were a rare occurrence outside graphic designers. As a result, and because Internet Explorer came pre-installed on Windows devices, Internet Explorer - and specifically IE6 - became the de facto standard corporate browser.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#the-rise-of-frameworks","title":"The Rise of Frameworks","text":"<p>Then in the second half of the decade, frameworks started to appear. Dojo and jQuery came out around the same time. The differences were that Dojo covered a broad array of web development needs, not only libraries for browser and web-page manipulation but also \"dijit\" Dojo widgets. jQuery,on the other hand, focused only on browser and web-page manipulation and AJAX. This initially gave Dojo a major advantage. The Java world were slower to the party, with JSP and JSF coming in around the turn of the decade.</p> <p>Around that same time AngularJS was also released. This also provided components, but also brought the MVC (Model-View-Controller) approach to prominence amongst JavaScript developers. Dojo lost some favour, not least when they changed the way components were coded. React came along a few years later and, after a few years of competition with AngularJS, took over when AngularJS also changed their whole API. Other frameworks like Vue, EmberJS, Svelte and NextJS have their proponents and prominent applications.But frameworks were still the way to go...because we still had to fight with Internet Explorer 6!</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#browser-companies-playing-nicer","title":"Browser Companies Playing Nice(r)","text":"<p>Microsoft Edge was a major game-changer. Not because it was a better browser than the competition, but because it finally killed off Internet Explorer 6. The other key was that Edge was the last browser that auto-updated. This means no longer do developers have to wait for reluctant IT departments to update all their end users' laptops. Now business users are likely to be on a modern browser. End users are often on a mobile device or a reasonably modern browser. And the browser companies are often collaborating together on web standards.</p> <p>There are also key advances in web development, which will be used throughout this series. Rather than spoil the fun, I'll leave them for when it becomes relevant.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#frameworks-vs-browsers","title":"Frameworks vs Browsers","text":"<p>Now we're in an era where frameworks still exist. Indeed there are probably a lot of applications built on old versions of frameworks like Dojo or AngularJS, stuck with technical debt, supported by frustrated developers working for IT companies that are unwilling to justify the ROI of rewriting the application. Some applications will even be using multiple frameworks in a single application.</p> <p>But whether it's one or many, the problem still exists: frameworks have a specific way of working. There's a whole website arisen to try to break developers out of the knee-jerk use of jQuery called You Might Not Need jQuery. But there are frameworks and components out there almost inextricably tied into the technical debt of jQuery. Sooner or later the developers will move on and it's not advisable to be one of those left struggling. Combining jQuery with native <code>document.selector()</code> or <code>fetch</code> calls is not too difficult.</p> <p>But frameworks have their own way to do things like light and dark mode or components. And as I've found, trying to integrate native options into a framework is far from straightforward. Indeed, it's probably not worth it. So what does the future hold? Well, as we saw with Internet Explorer 6, it will take a long long time for companies to move their applications to options browsers natively support. And developers will be stuck with technical debt for probably over a decade. Developers may also be reluctant to adopt those options too. Despite the one constant in IT being change, humans are inherently resistant to change.</p> <p>But it's easier than ever to develop a web application without using a framework. And that's the focus of this series, and why the name is changing.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/24/framework-web-3/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/","title":"Framework App to Web App: Part Four - DRAPI","text":"<p>Domino REST API is (in my admittedly somewhat biased opinion) the best easy method for creating a secure REST API into Domino. If you have very strong Java skills, an OSGi plugin using JAX-RS is the standard supported way. If you have good Java skills, Jesse Gallagher's JakartaEE project is the community approach. But even if you have those skills, Domino REST API may provide what you need. It certainly provides what I need for this project.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#super-crud","title":"Super-CRUD","text":"<p>Traditional NoSQL databases provide basic CRUD APIs. That works where for those databases, where access is strictly regulated and an API gateway built on top. But Domino developers rarely have the skills to create that API gateway. And Forms often have fields on them that are automatically set depending on the workflow step, only accessible to certain individuals at certain workflow steps, or require specific validation.</p> <p>Domino REST API evolved with this purpose front and foremost - to allow developers to configure access, to leverage Domino formula language skills, to keep code close to the database, to promote from dev to test to production environments leveraging Domino's in-built capabilities, and most of all to ensure the developers and administrators only allow a granularity of access.</p> <p>A \"super-CRUD\" API, as it were.</p> <p>And in this particular scenario, I'm configuring it with some quite advanced capabilities.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#why-first","title":"Why First?","text":"<p>So why do all the configuration of Domino REST API first?</p> <p>Well, I may be the only developer, but I will be developing using mock data.</p> <p>Why do I do this? Well, first and foremost, it allows me to develop quickly offline. It also allows me to code the UI and test creating or updating data without polluting the database or sending bad data that then needs cleaning up. That has come in very useful. It's not something that's been possible with Notes Client or XPages development, because the database and the UI are inextricably bound together. But because I know and realise that, I can take advantage of the difference.</p> <p>It also means I can test the configuration of Domino REST API without needing to build the UI first. That can come in useful.</p> <p>Plus, it means I can visually look at the format of the data I'm working with as I'm writing the code. And I can test the API in Postman or bruno to see what the response should be as I'm writing the code. This speeds up development.</p> <p>But most importantly - and this is the key when developing non-monolith applications - it means I can verify at any point - before, during or after development - if a problem is because of coding in my UI or because of data issues. I will have the Postman or bruno collection and environments. So if I get unexpected results, I can quickly post the same data from another application. If it fails in both, it's a problem on the Domino end. If it works in Postman or bruno, I can compare the request with what I see in the Network tab of the browser to verify what's different. As a developer, you should always endeavour to have a way to verify the cause of a problem, to quickly rule out as much as possible, and provide comparable logs.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#planning-data","title":"Planning Data","text":"<p>First up, I'm configuring the Forms and Views. Key here is to understand the extent of the data being retrieved. HTTP is not the best protocol for sending and retrieving large amounts of data. On mobile devices, network connectivity varies, even in a country that has 4G or 5G. If a laptop is involved, VPNs can negatively impact network performance. I've sat next to someone who was going via a VPN, connected to the same website, and immediately received a reply on my laptop while theirs times out. And many developers (and admins) are not skilled at troubleshooting why performance is bad.</p> <p>Best to minimise the data sent.</p> <p>HTTP/2 allows chunking of data. Domino REST API uses HTTP/2, it can do so because it provides its own HTTP server and does not rely on Domino's in-built HTTP server. But that's not a simple solution in itself. The code receiving the request needs to be able to take advantage of the chunking. If the language does not provide asynchronous handling of the response chunks, chunking on the sending side doesn't help. As Stephan pointed out in our session at Engage, Axios in a browser cannot deal with a chunked response. Even if it can handle chunked data, it might be far from straightforward to process the chunks, as Stephan blogged about when he found that JavaScript's <code>fetch</code> API doesn't nicely chunk as complete JSON objects. You need to manipulate the response to ensure you process only complete JSON objects.</p> <p>So, again. Best to minimise the data sent.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#local-storage","title":"Local Storage","text":"<p>It may be possible to store some data locally on the device, depending on security. But again, it's important to know the amount of data. There are two storage options - localStorage and sessionStorage. It's important to understand what they do and how they differ.</p> <p>localStorage is stored locally for all browser tabs for the specific \"origin\", i.e. scheme, hostname, and port. It is not time-limited, the developer has to handle removing it. This can be done in the <code>onbeforeunload</code> event, calling <code>window.localStorage.removeItem(key)</code>.</p> <p>sessionStorage is specific to the current browser tab, although it can be copied if you duplicate the tab. Unlike localStorage, it does get removed when the tab is closed.</p> <p>Both are limited to 5MiB (mebibytes) according to the documentation. But your CRUD operations will need to update locally stored content as well as updating the remote systems.</p> <p>There is another consideration to take into account here, one you may have encountered as a user. Updating local state may give the impression data has been successfully submitted to the server when it hasn't. It's important to handle failed updates, knowing if it's because of expired authentication, lack of response, or bad request. Some failures can be submitted again later. Some need rolling back. You may wish to leave handling failures until a later phase of (initial) development.</p> <p>The final aspect of note for localStorage and sessionStorage are that the keys and values stored can only be strings and are in UTF-16 format. This may differ from what is sent.</p> <p>Again, the benefit of creating the REST APIs first means you can check the size of the data.</p> <p>I will be splitting the data across localStorage and sessionStorage. But none of the data from the server will be persisted between browser sessions. We want to ensure we get the up-to-date data each browser session. I also don't have to worry about multiple users. If I did, one option might be async periodic requests to the browser for updates, and modify localStorage or sessionStorage accordingly. Trying to implement websockets is complicated by needing to get notified of updates on various clients, which I personally wouldn't bother fighting to achieve.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#agents","title":"Agents","text":"<p>In this particular application, not surprising considering it was an XPages application, there are no agents to be used. It's important to understand what agent types can only be used from the Notes Client. It's also important to understand that the way the user is notified of success or failure will vary compared to Notes Client.</p> <p>A typical use for agents can be for bulk updates or updates of related documents. Domino REST API provides specific endpoints for bulk updates. Updating related documents may best be handled by separate REST requests, chained. There is such a requirement in the previous incarnation of my app, with Trips. Creating a new Trip was intended to mark the previous Trip inactive. It never quite worked and I never bothered to solve it. It may not be required in the new application. But if it is, I will perform it with two separate requests:</p> <ul> <li>know the current active Trip UNID.</li> <li>create the new Trip.</li> <li>set the previous active Trip to inactive.</li> </ul> <p>There's a process that has to be handled here, potentially with a need to roll back. But most Domino databases I've supported have not typically been coded to roll back a whole transaction in the event of a failure. There are ways to minimise the impact, like sorting the view of active trip both on whether it's active and date created descending.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#dql","title":"DQL","text":"<p>I mentioned bulk updates. Typically in Domino a full-text search or Database.search has been used. Database.search varies in its performance and should not be relied upon, particularly for the web.</p> <p>Domino REST API doesn't use either, it only uses DQL. DQL, or Domino Query Language, was initially shown at the first HCL Factory Tour and released in Domino 10. It has had continued enhancements in subsequent releases, including after the retirement of its creator, John Curtis. DQL has three verbs - execute, parse and explain. This allows you to understand what it's doing and optimise the database for the query. To adapt a well-known phrase, there is great power here for those who want to take responsibility.</p> <p>DQL is used to perform bulk get or update calls, either using the \"/query\" endpoint or the \"/bulk/update\" endpoint. Domino REST API has also added access to DQL's QRP views.</p> <p>DQL will be used in this application for retrieving small collections of documents. Reporting will be done via view categories instead, to minimise the amount of data the server needs to send. Again, performing the statistical analysis on Domino and returning the numbers will be better performant than returning lots of data and performing the analysis on the browser.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#locking-down-functionality","title":"Locking Down Functionality","text":"<p>I don't need the ability to run code from Domino REST API here and none of the data is encrypted. SO I can turn off those settings. The way to do that is via the \"Source\" tab on the schema. This allows you to edit the overall schema's JSON object (more of that later). Edit buttons on each object or value allow you to change the settings. In this case I've changed \"allowCode\" and \"allowDecryption\" to false.</p> <p></p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#form-access-mode-specifics","title":"Form Access Mode Specifics","text":"<p>There are a couple of specific tweaks I make to the Form Access Modes.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#port-form","title":"Port Form","text":"<p>A Port has three fields: Port, Country and Abbreviation. The Abbreviation field is set based on the two-letter abbreviation for the relevant Country. In the XPages application, this was computed when the Country was selected. Technically, the browser made a REST request back to the server, retrieved the alias for the selected Country, and refreshed the page.</p> <p>We can improve on that.</p> <p>Domino REST API has an \"On Save Formula\" section. This allows you to run formula whenever a user saves a document with that Form at that Form Access Mode. In this case, I can set the Abbreviation dynamically during the save.</p> <p></p> <p>This improves in several ways. Firstly, the Port form only needs to contact the server on save, it doesn't need to round-trip to update the Abbreviation just to set on save. Of course we could have performed that update client-side based on local storage of the ports. But this way we ensure it cannot be set incorrectly by a user, whether it's sent properly through the application, whether the browser's request is tampered with, or whether the request comes in via another source like Postman.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#trip-form","title":"Trip Form","text":"<p>Similarly, Trip form has an \"On Save Formula\" to set \"Active\" field to \"Yes\", a simple @SetField formula which I'm sure you can work out.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#spot-form","title":"Spot Form","text":"<p>In the XPages application, whenever a Spot was created, I set a field for the created date and mapped it to the current active Trip. Both can also be done with the On Save Formula. Spots were never updated outside the current Trip, but occasionally I forgot to create the Trip before creating the first Spot. We can improve here too, thanks to the \"On Save Formula\" again. We can look up the active Trip, and set its UNID.</p> <p></p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#required-fields","title":"Required Fields","text":"<p>Any good REST API defines what is required when submitting content. I initially thought this was done via the \"On Save Formula\" or \"Formula for Write Access\". But \"On Save Formula\" is just to set fields, you can't use it to send back error messages if a value is not correct. And \"Formula for Write Access\" cannot be used to run formula against the submitted payload, it's just to check based on current saved values or database settings.</p> <p>However, there is a way, again on the Source tab of the schema.</p> <p>If you scroll down through the Forms and Form Access Modes, you'll see a setting for \"required\". This is an array of fields that have to have a value in order to save the document. Needless to say, I went through and set this on all Form Access Modes.</p> <p></p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#validation-rules","title":"Validation Rules","text":"<p>If you scrolled through the Source tab to see the \"required\" option, you probably also saw the \"validationRules\" option. This is a bit more complex, but even more powerful. \"validationRules\" are a formula type (currently just \"domino\"), a formula that must pass and an error message if the formula validation fails. You can have multiple validation rules, as you can see I've added here for the Ship form.</p> <p></p> <p>Here I'm validating that YearBuilt is 4 characters, that conversion to a number doesn't throw an error, and that the first two digits are \"18\", \"19\", or \"20\".</p> <p>Again, the power of Domino REST API here is that this validation happens regardless of the application. It's not hard for a technically savvy person to look at the Network tab and test a request from a REST client like Postman or bruno. A more skilful developer might even use curl. Now, the data is validated according to the rules the Domino developer expects, no matter how the data got to Domino REST API - and good luck if you're a Domino developer trying to verify that it was indeed submitted through the application you expected it to be!</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#modifying-scope","title":"Modifying Scope","text":"<p>With so much flexibility on Form Access Modes, it's a challenge finding a good user experience to customise everything. No wonder these aspects are not currently (as of August 2024) available in the Form Access Mode UI.</p> <p>Here we've seen editing the scope via The \"Styled Object\" option in the Source tab. That validates the data types and helps you avoid mistakes. Two things to bear in mind are that it repaints the object whenever you submit a change. And you also need to remember to click the \"Save\" button when you've finished your edits.</p> <p>Another option is to export the JSON (or perform a GET on the relevant API), and make the change via a REST API call. You can find the relevant request by going to the OpenAPIv3 page on your server, changing the drop-down at the top to the \"setup\" API, opening the \"schema\" section and going to \"POST\" for \"/schema\". You just need to get the schema format correct.</p> <p>!POST Schema</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#downloading-data","title":"Downloading Data","text":"<p>Once the schema and scope are set up, its easy to make Postman requests to test it works as expected. Then it's easy to download the JSON data ready to start development. That's what will happen in the next part. We will need to make another minor tweak to Domino REST API later on, but this is enough for us to start development.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/08/26/framework-web-4/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/","title":"Framework App to Web App: Part Five - Home Page","text":"<p>So it's time to start with the application. Any development - team or individual - should use source control. My usual approach is to create the repository in GitHub (or your preferred repository), then clone it locally. A README is best practice of course. And I'll be creating two subfolders, \"webapp\" and \"bruno\" - because bruno allows me to store the REST service collection and environment in the github repo.</p> <p>We'll start off in the webapp directory with three files - index.html, index.js and style.css. This is a small web application, so I'm doing a single page application (SPA), so this will be the only HTML file. There will be other JavaScript files and another CSS file. But to get started, this is all we need.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#html-file-basics","title":"HTML File Basics","text":"<p>First off, with the index.html file. This is modern web development, so HTML5. This means the main declaration is very easy <code>&lt;!DOCTYPE html&gt;</code>. Referencing the stylesheet and JavaScript file are (for now) the same as we've always done. Things will change later on, but we're starting simple. Everything is in a <code>div</code> with the class \"container\" (a common name). I'm just styling this with a border and border-radius, so it's not so \"box-y\". I've got a <code>header</code> element for my header area with a heading.</p> <p>Above this is a <code>section</code> with a fixed height, and the ID \"status-message\" in which I'll post messages. This takes up space, but is designed to ensure no \"jumping\" UI. The section just includes a div with the ID \"message\". To manage the content, I'll use three functions:</p> <pre><code>const statusMsg = (statusText) =&gt; {\n    document.getElementById(\"message\").innerText = statusText;\n    document.getElementById(\"status-message\").style = \"background-color:#008800\";\n    setTimeout(clearMsg, 3000);\n};\n\nconst statusError = (err) =&gt; {\n    document.getElementById(\"message\").innerText = err;\n    document.getElementById(\"status-message\").style = \"background-color:#880000\";\n    setTimeout(clearMsg, 3000);\n};\n\nconst clearMsg = () =&gt; {\n    document.getElementById('message').innerText = \"\";\n    document.getElementById('status-message').removeAttribute(\"style\");\n}\n</code></pre> <p>Things will get more sophisticated, but this allows us to pass a successful message to <code>statusMsg()</code> and an error message to <code>statusError()</code>. After a few seconds, the message will clear. The styling is basic at this point - this is about getting it working at this point. But we add the style attribute when adding a message, remove the style attribute when clearing the message. This allows us to reuse the same HTML element and change the use.</p> <p>Below the header tag we use a <code>main</code> tag. This is the main area of the application. Beneath the <code>main</code> tag we'll add <code>footer</code> tag, in which we'll have a span with the copyright details.</p> <p>For developers who have primarily used a framework, <code>header</code>, <code>main</code>, <code>section</code>, and <code>footer</code> may be tags they have not come across. But they help semantically group content in a way that <code>div</code> cannot.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#main-area","title":"Main Area","text":"<p>Again, we use <code>section</code> tags for each of the \"pages\" of the single page application. The first section is for the login form. This looks pretty simple:</p> <pre><code>&lt;form id=\"credentials-form\"&gt;\n    &lt;p&gt;\n        &lt;input type=\"text\" id=\"username-input\" placeholder=\"User Name\"\n        aria-label=\"User Name\" autofocus /&gt;\n    &lt;/p&gt;\n    &lt;p&gt;\n        &lt;input type=\"password\" id=\"password-input\" placeholder=\"Password\"\n        aria-label=\"Password\" /&gt;\n        &lt;button id=\"login-btn\" aria-label=\"Login\" type=\"button\"&gt;Login&lt;/button&gt;\n    &lt;/p&gt;\n&lt;/form&gt;\n</code></pre> <p>Here we just want two fields in a single column, so we just use paragraph tags. We'll get more sophisticated with other forms later. And in the second paragraph, we have the login button. Rather than labels, we're just using placeholder text. This is easy for an <code>input</code> HTML element, we'll handle other HTML elements later on. Two other attributes are worth highlighting.</p> <p><code>autofocus</code> is set on the \"User Name\" field. As someone who prefers to use the keyboard, I find it frustrating when a web developer creates a form and does not set a field with initial focus. It's easy, it speeds up use of the page. And for an application primarily used on mobile, speed of input is key here, so autofocus just makes sense. There can only be one element per page with <code>autofocus</code>, it's the element that the cursor is put in when the page is loaded. It can't be used to set the element of focus when an area is displayed, only when the page is loaded. So we'll need a different approach for the other sections. But the login form should be the first one displayed, so it makes sense here.</p> <p>The second is <code>type=\"password\"</code> on the \"Password\" field. This means that whatever is entered is not visible. It just makes sense to do this for a password.</p> <p>Finally we have the login button. But it's just the HTML, there is no onclick event, just the markup. For developers who have not kept up-to-date with the evolution of web development, this will come as a huge shock. So the good news is after the code for the login, we'll be at the end of this blog post and you can digest it all.</p> <p>Inline JavaScript on HTML elements is considered bad practice in web development, it's a security risk for cross-site scripting attacks and depending on the Content Security Policy it might get blocked, as might <code>&lt;script&gt;</code> tags without a nonce or hash. Even some years ago a security review of a web application required that inline JavaScript be removed from an application I had initially developed. You can investigate further or formulate arguments against it. I prefer to spend my time getting things done and learning how to handle the requirement in a modern web development approach, rather than debating or fighting political battles.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#eventlisteners","title":"EventListeners","text":"<p>For the solution, we need to step across to our JavaScript file, index.js. The modern approach is adding an EventListener to the relevant HTML element, in this case the button with id \"login-btn\". But we need to make sure the DOM is loaded, otherwise we will not be able to get a handle on the button. Certain JavaScript developers will immediately jump to using jQuery. But we don't need to, this isn't 2014, JavaScript and browsers have evolved. I'll use a function called <code>bootstrap()</code> to perform all my DOM manipulation. To ensure this triggers once the page has loaded, I'll use this JavaScript in index.js:</p> <pre><code>if (document.readyState != \"loading\") {\n    bootstrap();\n} else {\n    document.addEventListener(\"DOMContentLoaded\", bootstrap);\n}\n</code></pre> <p>This may be overkill, a belt-and-braces approach. But it ensure the <code>bootstrap()</code> function doesn't trigger too early. <code>document.readyState</code> is either \"loading\" or the DOM is ready to interact with. DOMContentLoaded is an event we can hook onto that will trigger after the DOM is accessible. At this point in the development, I also set <code>defer</code> when adding the JavaScript file to the page, so <code>&lt;script defer src=\"index.js\" charset=\"utf-8\"&gt;&lt;/script&gt;</code>. This means the script is loaded at the same time as the page and executed after the page has loaded. This may avoid the need for adding the eventListener to <code>DOMContentLoaded</code>.</p> <p>The bootstrap function then gets the \"login-btn\" element by ID and adds the \"click\" EventListener to call <code>formLogin()</code>:</p> <pre><code>const bootstrap = () =&gt; {\n    let login_button = document.getElementById(\"login-btn\");\n    login_button.addEventListener(\"click\", formLogin);\n    toggleSPA(\"credentials\", \"block\");\n};\n</code></pre> <p>There are additional parameters that can be passed to <code>addEventListener()</code> but we won't be using them (yet!). And, not surprisingly, there is also a <code>removeEventListener()</code> function. toggleSPA()` is a function that will be used to show/hide the relevant section. This is its content:</p> <pre><code>const toggleSPA = (showme, how) =&gt; {\n    spaSections.forEach((s) =&gt; {\n      const display = showme === s ? how : \"none\";\n      document.getElementById(s).style.display = display;\n    });\n  };\n</code></pre> <p><code>spaSections</code> is a JavaScript array of the IDs of the sections. So after adding the eventListener, we set the display of the <code>credentials</code> section to block and all others to none.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#troubleshooting-eventlisteners","title":"Troubleshooting EventListeners","text":"<p>But how do you see this EventListener when you're inspecting the element on the web page? If you view the page source you only see the HTML that was coded in the index.html. And if you open up Developer Tools in Chrome (or your preferred alternative) and inspect the button, on the Elements tab you just see the button, no event.</p> <p>In Chrome Developer Tools, alongside the tabs for Styles, Computed, Layout and DOM Breakpoints, you will see another one - Event Listeners. This shows you all EventListeners registered for the current element and anything above it in the DOM tree, if \"Ancestors\" is checked. Uncheck that and you see just the EventListeners registered for this element.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#wrap-up","title":"Wrap-Up","text":"<p>In the next section we'll come back to the <code>formLogin()</code> function, how we handle logging in, how we set that up to handle mocking and a setting we need to add to DRAPI to allow us to call JavaScript functions against it.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/03/framework-web-5/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/","title":"Framework App to Web App: Part Six - Mocking, DRAPI and CORS","text":"<p>In the last part, we created for login form and added an eventHandler to call <code>formLogin()</code> function. However, we didn't go into the code behind that function. That function is pretty basic, offloading the bulk of the processing:</p> <pre><code>const formLogin = () =&gt; {\n  let username = document.getElementById(\"username-input\").value;\n  let password = document.getElementById(\"password-input\").value;\n  login(username, password);\n};\n</code></pre>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#mocking","title":"Mocking","text":"<p>That function - at least during development - will go down two different routes, depending on the username passed.</p> <pre><code>const login = (user, pwd) =&gt; {\n  // Set as mocking only\n  if (user === \"John Doe\") {\n    window.isMock = true;\n    statusMsg(\"Login successful\");\n    loadSessionData();\n    toggleSPA(\"landing\", \"block\");\n  } else {\n    fetch(baseUrl + urls.login, {\n        method: \"POST\",\n        headers: {\n        \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify({ username: user, password: pwd }),\n    })\n        .then((response) =&gt; response.json())\n        .then((json) =&gt; {\n            if (json.hasOwnProperty(\"status\")) {\n                statusError(json.message);\n            } else {\n                if (extractCredentials(json)) {\n                    loadSessionData();\n                    toggleSPA(\"landing\", \"block\");\n                }\n            }\n        })\n        .catch((err) =&gt; {\n            statusError(err.message);\n        });\n    }\n  };\n</code></pre> <p>If the username is \"John Doe\", we set a global variable <code>isMock</code> to true and continue processing. Otherwise we log into Domino REST API, extract the JWT token and continue processing.</p> <p>The purpose here is that we can develop the application offline and / or with mock data. Even if we have access to Domino REST API, using the mock data means we can update local storage with dummy data and don't need to clean up the database all the time. When we're ready we can test the Domino REST API calls from the app and we can test them from bruno or Postman.</p> <p>Bruno or Postman allows us to perform the relevant REST API calls to retrieve dummy data that we can test with. In this case it allows us to create local JSON files of ships, ports, countries, spots and more.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#fetch","title":"Fetch","text":"<p>REST API calls, whether local or to Domino REST API, are done with fetch. fetch is widely supported by browsers meaning the days of XMLHttpRequest or framework variants like <code>dojo.xhr</code>, jQuery's <code>$.get()</code> or the more recent <code>Axios</code> are no longer required. Even if you're familiar with Axios, it's worth switching to fetch because it can handle chunked responses in a browser, which Axios cannot as Stephan Wissel showed in our Engage session.</p> <p>Fetch is also readily supported in bruno and Postman code generation options, so learning how to use it is very easy. As you can see above, it takes a URL and a JSON object containing options - method, headers and body. If your code does not necessarily return a JSON object, you will need to check the response status code before trying to convert to JSON. The nice thing with Domino REST API is that if some kind of error occurs, the response always contains a \"status\" property.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#cors","title":"CORS","text":"<p>Whenever making browser-based HTTP requests, you need to be aware of Cross-Origin Resource Sharing (CORS). Even when opening a local HTML file, the browser will block CORS requests, which is why you'll need a local HTTP server. In the list of dev tools I mentioned Visual Studio Code's Live Server extension. This is a good way round it if you don't want to install anything additional. Most developers have Node.js installed now and so my preference has become NodeJS's http-server via npx.</p> <p>This works fine for the mock data calls. But as soon as you call Domino REST API, you may also get hit by CORS problems. Here, Domino REST API is only allowing requests from specified hostnames. Domino REST API automatically accepts \"localhost\", but if you're using Live Server extension you'll be connecting from 127.0.0.1. The good news is Domino REST API's documentation covers allows you to configure this. We'll need to create <code>keepconfig.d</code> directory in Domino data directory, if it doesn't exist, and add a config.json file. In the \"CORS\" JSON object, we'll need to add <code>\"127.0.0.1\": true</code> and restart Domino REST API. Now, we can connect from our local app.</p> <p>NodeJS's http-server allows you to use <code>http://localhost</code> rather than <code>http://127.0.0.1</code>. This also gives a URL on your local network (192.168.1.*), but that will not be in DRAPI's CORS setting either, so will fail. Of course this will work for testing the application in a mock session, which may cover most initial scenarios. However, using ngrok will proxy through your local machine, so will work around this problem.</p> <p>The key here is understanding what CORS means, that the server we're connecting to is not accepting the request because it doesn't come from a hostname it has been configured to accept.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#localstorage-and-sessionstorage","title":"LocalStorage and SessionStorage","text":"<p>Whether mocking or calling Domino REST API, I'm storing content locally in the browser. Again with modern browsers this can be done on desktop browsers or mobile browsers. There are two options - localStorage and sessionStorage. Most of the local data I'm storing in sessionStorage, although we'll see a specific use case for localStorage in the next part of the series.</p> <p>The important differences are that sessionStorage is automatically cleared when the user closes the tab and localStorage is shared across tabs for the domain and protocol whereas sessionStorage is only shared if the user uses the browser \"duplicate tab\" functionality.</p> <p>The key thing to bear in mind with both localStorage and sessionStorage is that both the key and the value are always UTF-16 strings. This means booleans need converting or (probably easier) compared as strings, as we'll see in the next part. More typically, the values stored will be JSON, which means calling <code>JSON.stringify()</code> before writing and <code>JSON.parse()</code> after reading.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#wrap-up","title":"Wrap Up","text":"<p>In the next session we'll add some styling to the application, which will demonstrate how far CSS has come in recent years.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/16/framework-web-6/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/09/21/ls-classes-delete/","title":"LotusScript Classes and Delete","text":"<p>A couple of years ago I wrote a number of blog posts about LotusScript / VoltScript classes. The topic is relevant to both languages, we've not made any changes to how classes are managed in VoltScript, even though we discussed adding some things added to Visual Basic since LotusScript was created, things like additional modifiers. Even though classes are still the same, we've used some quite sophisticated aspects of class, as will be apparent to anyone who has looked at VoltScript Testing, its LotusScript port bali-unit, VoltScript JSON Converter, or VoltScript Collections.</p> <p>Three particular aspects I used in those projects are of particular relevance for this blog post.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#base-and-derived-classes","title":"Base and Derived Classes","text":"<p>The first is base and derived classes. We're writing a logging framework, with a session class holding entries. But unlike OpenLog or Enhanced Log, the log entries doesn't handle how it writes out the content or where it writes to. Instead the LogSession class takes one or more LogWriters, which handle where it writes to and what it outputs from the log entries. The BaseLogWriter just prints out the content. But the documentation will show a derived class that writes to a file.</p> <p>This is a concept I've already used in VoltScript JSON Converter and VoltScript Collections, base classes that can be extended by the user and passed into another class, which then calls known functions to perform other functionality.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#delete-method","title":"Delete Method","text":"<p>The second is exploiting the <code>Delete</code> method of a class to always perform functionality at the end of the script. This is used in VoltScript Testing and bali-unit to write out the unit test results. It means developers don't need to remember to call a particular function at the end of their script and ensure it's called if there's an error.</p> <p>In the case of VoltScript Logging, the Delete method of the LogWriter was writing out the logs.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#singletons-accessed-from-a-function","title":"Singletons Accessed from a Function","text":"<p>The third was an approach I blogged about a couple of years ago for creating a singleton, using a property or function to lazy-load a static private variable. This ensures the same instance is used throughout the life of the script, but prevents developers creating a new - or worse, multiple new - instances.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#so-whats-new","title":"So What's New?","text":"<p>All should have worked fine. After all, I'd already done writing stuff from the Delete method in VoltScript Testing. It's been in use for years and runs on every build of various repositories. And it's run on demand to generate the test results we post in our documentation.</p> <p>However, when the code ran none of the logs was written out. Cue troubleshooting!</p> <p>You can see the problem if you use this code as LotusScript or VoltScript. You can trigger it with the following code:</p> <pre><code>    Dim lw as New BaseLogWriter()\n    Call getLogSession().addLogWriter(lw)\n</code></pre> <p>Before it generates an error, you'll get this output:</p> <pre><code>Creating new logSession\nIn LogSession Delete\nTesting logWriter - False\nTesting globalLogSession - True\nCreating new logSession\nWriting out\nIn BaseLogWriter delete\nIn LogSession Delete\nTesting logWriter - True\n</code></pre> <p>The first line is generated by the call the <code>getLogSession()</code>. Then the script starts unloading content. The <code>True</code> / <code>False</code> are check for whether the previous variable is <code>Nothing</code>. As one would expect, in LogSession's <code>Delete()</code> function the LogWriter is correctly still valid.</p> <p>However, at line 4 when we get into the BaseLogWriter's <code>Delete()</code> function, the globalLogSession variable is <code>Nothing</code>. I think this is because <code>LogSession.Delete</code> has already triggered. With the next call for <code>getLogSession()</code> you can see that on line 5 creates a new instance. Then the BaseLogWriter is deleted.</p> <p>If you remove the call to <code>getLogSession()</code> in the <code>writeToLog()</code> and add a check for whether <code>globalLogSession</code> is Nothing in <code>BaseLogWriter.Delete()</code>, and it's still <code>Nothing</code>.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#solutionpart-one","title":"Solution...Part One","text":"<p>There are a few changes we need to make to get this to work.</p> <p>The first is remove the <code>getLogSession()</code> variable. It's now pointless, because if used in the wrong place, it will reinitialize it.</p> <p>The second is we need to create the <code>globalSession</code> variable somewhere. The right place to do this is in the <code>Sub Initialize()</code> of the script containing the classes - the VoltScript Library Module or LotusScript Script Library.</p> <p>The third is there is no point using <code>BaseLogWriter.Delete()</code>. The <code>globalSession</code> has already been set to <code>Nothing</code> by then, regardless of whether it is declared before or after the <code>LogSession</code> class. However, the <code>logWriter</code> is not <code>Nothing</code> during <code>LogSession.Delete()</code>, so we can call the <code>writeToLog()</code> method from there. However, we now need to pass the session across.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#perfectright","title":"Perfect...Right?","text":"<p>But the LogWriter is designed to be extended. And so we created a sample for that, one that writes to a file. This <code>FileLogWriter</code> class and the test script were in a separate script file, as they would be in reality. When we tested the code, it didn't write out. Cue more troubleshooting!</p> <p>First off, a bit of clarification. The actual LogSession iterated through the LogWriters - it's designed to have more than one - and checked if it was <code>Nothing</code> first. A bit of additional logging showed that when we added a FileLogWriter and a BaseLogWriter, the BaseLogWriter was not <code>Nothing</code> (as we already know) but the FileLogWriter was <code>Nothing</code>.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#terminate-method","title":"Terminate Method","text":"<p>We've seen the use of the <code>Sub Initialize</code>, to track this down we added print statements to the <code>FileLogWriter.Delete</code> method and to a <code>Sub Terminate</code> that we added in the scripts. Looking at the output again, the cause became obvious: the script with the main script and the FileLogWriter was unloaded before the script with the BaseLogWriter and LogSession.</p> <p>Just to double-check, we tried moving the FileLogWriter class to a separate script, but this didn't solve the problem.</p> <p>From adding print statements to the <code>Sub Initialize</code> and <code>Sub Terminate</code> of all scripts, it became apparent the cause. The key to understanding is to do with the <code>Use</code> statements and the dependency tree. <code>Initialize</code> methods are fired from trunk to leaf and <code>Terminate</code> methods are fired in reverse. Scripts that have no <code>Use</code> statement load first, then scripts that use those scripts, then scripts that use those scripts, and so on.</p> <p>So by the time <code>LogSession.Delete()</code> triggers, all scripts containing custom LogWriters will already have been unloaded and the variables containing them set to <code>Nothing</code>.</p> <p>So how do we solve this?</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#solution-part-two","title":"Solution Part Two","text":"<p>To solve the problem, we come back to our initial plan: write out from the <code>Sub Delete</code> of the BaseLogWriter class.</p> <p>The reason it didn't work for a start was because it was running from the BaseLogWriter. But this time it won't be: it will be running from the custom LogWriter. Let's work with a basic structure: main script with custom LogWriter class in it (Script A) using a script with the logging classes (Script B). For the print statement in <code>LogSession.Delete</code>, we modify it to print out the <code>TypeName</code> of each LogWriter and whether it's <code>Nothing</code>. With the print statements we've used and initializing the custom LogWriter, this is what we'll see:</p> <pre><code>Initializing logging\nInitializing main\nIn BaseLogWriter delete\nTesting globalSession - False\nTerminating main\nTerminating logging\nIn LogSession Delete\nTesting logWriter - BASELOGWRITER: False\nTesting logWriter -  EMPTYCLASS : True\nIn BaseLogWriter delete\nTesting globalSession - True\n</code></pre> <p>Script B is initialized first, then script A. The custom LogWriter is deleted at the end of script A's <code>Sub Initialize</code> and, because it extends BaseLogWriter and has no <code>Delete</code> method of its own, it calls <code>BaseLogWriter.Delete()</code>. And this time globalSession is not <code>Nothing</code>. That's because then script A is terminated and script B is then terminated. Then the LogSession is deleted. But we see something interesting. The BaseLogWriter is not <code>Nothing</code>, but the custom one is. Moreover, <code>TypeName()</code> returns \"EMPTYCLASS\". This is because the class has already been unloaded, so it can't tell what class it's an instance of. Finally the BaseLogWriter is deleted, at which point <code>globalSession</code> is <code>Nothing</code>.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#full-solution","title":"Full Solution","text":"<p>So we can write out the logs from BaseLogWriter if <code>globalSession</code> is not <code>Nothing</code>. And in <code>LogSession.Delete()</code> we can write out the logs where the LogWriter is not <code>Nothing</code>. It may seem convoluted, but it's the cleanest solution.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/09/21/ls-classes-delete/#see-for-yourself","title":"See For Yourself","text":"<p>VoltScript Logging will be open sourced within the next couple of weeks, as have all our VoltScript Library Modules, so I'm not revealing any state secrets here. And you'll be able to see what it looks like for yourself.</p>","tags":["LotusScript","VoltScript","Domino"]},{"location":"blog/2024/10/07/framework-web-7/","title":"XPages App to Web App: Part Seven - CSS","text":"<p>In the last part we created the login form. In this part we're going to start adding some theming.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#theme-colours","title":"Theme Colours","text":"<p>The power of XPages is in the out-of-the-box themes available. These provide styling for the various XPages components and some developers may even have not add their own CSS. But hopefully developers have.</p> <p>Those who have been developing since the days of Domino 8.5.2 and 8.5.3 may remember the OneUIv2.1 themes which included colour variants. If you have an older Domino server, you may still find them in /Data/domino/html/oneuiv2.1/themes directory. In here are theme directories for blue, gold, green, onyx, orange, pink, purple, red, and silver. Each directory has its own CSS files. The files are virtually identical except for different colours for a handful of elements. To change colour theme for XPages' OneUI2.1 themes, we usually would change the theme of the database and rebuild the application. (If I remember correctly, the XPages demo application provided a dynamic way of switching theme colour, but I don't have an example to check.) <p>This was how different coloured themes was done in those days. But things have changed.</p> <p>There is now a better way to handle theme colouring. Colours are no longer defined in stylesheets. Instead CSS variables are a better way. A separate stylesheet can assign colours to a variable name, and the main stylesheet can then use those variables. For example, we will have a stylesheet called \"blue-theme.css\":</p> <pre><code>:root {\n    --primary-color: #000;\n    --primary-color-dark: #FFF;\n    --button-primary: #005C99;\n    --button-primary-dark: #708DA0;\n}\n</code></pre> <p>By assigning the variables to the root, they are global. And the syntax is \"--\" + variable name. The main stylesheet will not have any colours defined. Instead, the variable names will be used:</p> <pre><code>.btn-primary {\n  background-color: var(--button-primary);\n  color: var(--primary-color-dark);\n}\n</code></pre> <p>With this approach, to change the colour of the application, we just need to load a different CSS stylesheet, switching \"blue-theme.css\" for \"green-theme.css\", for example.</p> <p>The use of CSS variables can be used for more than just colours, for example for font-sizes or fonts. This provides a great amount of flexibility for UI.</p> <p>It's probably not an option XPages developers have used though, even in more recent versions of Domino, because it requires the main stylesheet to use variables. However, when moving beyond XPages, it's an option that can - and should - be embraced. And with variables and native for support for <code>calc()</code>, nesting, <code>color-mix()</code>, and even trigonometric functions, if you're not supporting legacy browsers you may not even need to use SASS or LESS.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#light-dark","title":"light-dark","text":"","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#enabled-with-css","title":"Enabled with CSS","text":"<p>But modern websites and web applications provide something more. Users expect to choose light-mode or dark-mode - or for it to auto-adjust. The good news is that this too can also be handled with standard CSS using light-dark(). If you looked closely at the extract from \"blue-theme.css\" above, you'll have noticed that there is <code>--primary-color</code> and <code>--primary-color-dark</code>. I deliberately omitted something from the main CSS. The actual styling I'm using is:</p> <pre><code>.btn-primary {\n  background-color: light-dark(var(--button-primary), var(--button-primary-dark));\n  color: light-dark(var(--primary-color-dark), var(--primary-color));\n}\n</code></pre> <p>With the use of variables this starts to get a little trickier to read. But the syntax used is to pass two colours to <code>light-dark()</code>. The first is the colour to use when in light-mode, the second is the colour to use when in dark-mode. To actually use this, we need to add something more to the main stylesheet:</p> <pre><code>:root {\n  color-scheme: light dark;\n}\n\n.light-mode {\n  color-scheme: only light;\n}\n\n.dark-mode {\n  color-scheme: only dark;\n}\n</code></pre>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#light-dark-switcher","title":"Light-Dark Switcher","text":"<p>With just this CSS, the application will pick up the light or dark mode of the operating system. It's important to understand that even though you can set light or dark mode on the browser, it's the operating system setting that <code>light-dark</code> uses.</p> <p>The application I'm building is only used by me. I prefer to switch between light and dark mode at the click of an icon. So I'm not auto-switching the theme based on the operating system setting. Instead, I'm adding a switcher icon and storing the setting in localStorage, which we covered in the last blog post.</p> <p>To switch the theme, I'm preferring an icon. There are various icon libraries out there, but I'm using Google's material design icons. This means I can just use a span, just with different text in the span. As in part five I'll be adding the event handler and loading the relevant light/dark setting in the <code>bootstrap()</code> function:</p> <pre><code>    const themeSwitcher = document.getElementById(\"themeToggle\");\n    themeSwitcher.addEventListener(\"click\", function () {\n        switchTheme(true);\n    });\n    switchTheme(false);\n</code></pre> <p>The boolean parameter being passed is whether or not to change the theme. So on the eventHandler, we want to change the theme, whereas in the bootstrap we just want to set the theme. The <code>switchTheme()</code> function itself is this:</p> <pre><code>const switchTheme = (switchTheme) =&gt; {\n    // localStorage is only a string\n    const darkMode = localStorage.getItem(\"shipSpotterLightDark\") === \"true\";\n    if (switchTheme) {\n        darkMode = !darkMode;\n        localStorage.setItem(\"shipSpotterLightDark\", darkMode);\n    }\n    let toggle = document.getElementById(\"themeToggle\");\n    if (darkMode) {\n        document.documentElement.classList.add(\"dark-mode\");\n        document.documentElement.classList.remove(\"light-mode\");\n        toggle.innerText = \"light_mode\";\n    } else {\n        document.documentElement.classList.add(\"light-mode\");\n        document.documentElement.classList.remove(\"dark-mode\");\n        toggle.innerText = \"dark_mode\";\n    }\n};\n</code></pre> <p>The first line of the function checks whether dark mode has been enabled. Remember localStorage values are always strings and may be null, so we need to handle it accordingly. From the eventHandler, we toggle the boolean and update localStorage. Then we set the relevant class (light-mode or dark-mode) on the html tag. We also put the relevant label (light_mode or dark_mode) on the <code>themeToggle</code> span to set the icon accordingly.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#wrap-up","title":"Wrap up","text":"<p>In 2024 CSS is extremely powerful. Frustratingly powerful, at times. But extremely powerful. Because of its prevalence for many years, SASS and LESS will still be used in many applications for some years. But when targeting only modern browsers, in many scenarios CSS alone may be sufficient.</p>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/07/framework-web-7/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Domino REST API","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/","title":"XPages App to Web App: Part Eight - Landing Page Web Component","text":"<p>We've got a login page, we've got theming, we're handling light mode and dark mode. Now we're ready to start adding our landing page.</p> <p>But this application is almost exclusively going to be used from a mobile device, doesn't have a need to show specific data from the database after login, doesn't have a user requirement to navigate directly from one page to another. In addition, this is a single page application, with all HTML, CSS and JS already deployed to the browser before the user logs in. So whereas a server-side rendered application, like XPages, would need to request HTML from the server for every page being rendered, this application's switching of pages will be completely client-side, so zero performance hit. All of this combines to make the better choice for the user experience a landing page with tiles for each subsequent page we want users to select from.</p> <p>We want to make this scalable within the application, being able to easily add new tiles in the future. But we may also want to reuse it in other applications in the future. The other important point is that the CSS is not likely to be something we want to use elsewhere in the application. These are three key aspects that our implementation will leverage.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#xpages-comparisons","title":"XPages Comparisons","text":"<p>In XPages, a developer may instinctively use an <code>xe:navigator</code> control with tree nodes for each of the pages. This is a direct analogy for the Outline control in Notes Client / Nomad development. So it's a familiar paradigm and in use in some web applications beyond XPages.</p> <p>But if they were to resist muscle memory and choose a navigation option like we have here, there would be two options.</p> <p>The first would be a custom control, a reusable component that could be copied into another page and potentially configured by passing properties into it. The alternative would be binding to a property on a viewScoped bean or controller. But this actually makes it harder to reuse across other applications, because the developer who wants to reuse it also needs to use a viewScoped bean or controller and needs to add the same property with the same name. And the custom control may bind to other business logic or property binding that also needs to be reproduced, without cleanly being scoped to the custom control. Don't get me wrong, I've probably done that and put the logic in a base Java class extended by each page. But it undermines the self-contained intention of a custom control, so it's arguably not best practice.</p> <p>The second option, possibly even within the custom control, would be a repeat control. This allows the developer to set the layout and style for each tile and pass a collection in, which may be a collection of custom Java objects. This means if you want to add another tile, you just need to add to the collection. It also makes it easier to reuse - you just change the collection you're passing into the repeat control. However, developers should avoid setting styling directly on each tile, so developers typically put the styling in an application-wide stylesheet, separate from the component.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#the-modern-web-way","title":"The Modern Web Way","text":"<p>If you use a framework like React, Vue, or EmberJS, you're used to the solution to this - components. For some years now the same approach has also been available in standard Web APIs in Web Components. Some of the frameworks can already package their components as native Web Components, and this has been done for many years by the Java framework Vaadin - and I've been a big fan of Vaadin for many years, it's well worth considering if XPages developers wish to choose a different framework but still use Java. But some framework developers see web components as still somewhat deficient to the framework components. And I can understand the problem here. I've included a web component in EmberJS and although it works, it cannot easily interact with the rest of the EmberJS framework like services and its lifecycle may not correspond neatly to the framework's lifecycle. Integrating the two requires a good understanding of both web components and the framework, which is not impossible, but requires a certain type of developer.</p> <p>But this is not a framework application. It's standard HTML, JS and CSS. So web components are a good choice.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#custom-element","title":"Custom Element","text":"<p>A web component comprises two parts, a JavaScript class and a mapping to an HTML tag.</p> <p>As with a Custom Control or a Repeat Control, and as with React components, a web component is a custom HTML element. Just as those two extend a Java class, so the web component is a JavaScript class. Those developers who just use declarative HTML may not realise that the HTML DOM API provides a JavaScript interface, <code>HTMLElement</code>, which is the base for all HTML elements. It is this JavaScript class or another implementation that a web component class will extend. For the Landing class, we just need to extend HTMLElement. Because usually each web component is in its own JavaScript file, the code for the class will export the class too. So the code is:</p> <pre><code>export default class Landing extends HTMLElement {\n    ...\n}\n</code></pre> <p>The second part is the mapping to an HTML tag. In XPages too, the class maps to a tag, in this case an XML tag. For a Custom Control, the tag <code>xc:....</code> and the design element name camel-cased. For a Repeat Control, it's <code>xp:repeat</code>. For web components we use <code>customElements.define()</code>, which takes two arguments - the HTML tag name to use and the class it corresponds to. So the code we need at the end of our class is <code>customElements.define(\"landing-elem\", Landing);</code>.</p> <p>This means in an HTML file, we can import the JavaScript file as a module and use the HTML tag.</p> <pre><code>&lt;head&gt;\n    &lt;script type=\"module\" src=\"/scripts/landing.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    ...\n    &lt;section class=\"landing\" id=\"landing\"&gt;\n        &lt;landing-elem&gt;&lt;/landing-elem&gt;\n    &lt;/section&gt;\n&lt;/body&gt;\n</code></pre>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#template","title":"Template","text":"<p>As with a Custom Control or a Repeat Control, and as with React components, one of the key parts of a web component is the HTML. This could be built up programmatically in JavaScript using DOM APIs. But the easier option - one familiar to XPages or React developers - will be to create an HTML fragment. This is done with the tag <code>&lt;template&gt;</code>, which means it's available for programmatic access but not rendered on the web page.</p> <p>This could be put on the HTML page, in a similar way to how EmberJS uses Handlebars templates corresponding to the JavaScript class. But to keep a web component self-contained, it makes more sense to follow the approach of React and add it into the JavaScript file, but using the JavaScript DOM APIs, <code>createElement()</code> and the <code>innerHTML</code> property.</p> <pre><code>const template = document.createElement(\"template\");\ntemplate.innerHTML = `\n    &lt;style&gt;\n        .landing-container {\n            display: flex;\n            flex-wrap: wrap;\n            align-items: stretch;\n            justify-content: center;\n        }\n        .landing-tile {\n            margin: 5px;\n            font-weight: bold;\n            font-size: 30px;\n            color: light-dark(var(--primary-color-dark), var(--primary-color));\n            background-image: radial-gradient(circle at center, light-dark(var(--landing-tile-start),var(--landing-tile-start-dark)) 15%, light-dark(var(--landing-tile-end),var(--landing-tile-end-dark)) 100%);\n            height: 200px;\n            width: 200px;\n            box-shadow: inset 0 0 2px 2px light-dark(var(--border-color-primary),var(--border-color-primary-dark));\n            border-radius: 10px;\n            flex-grow: 1;\n            text-align: center;\n            align-content: center;\n            cursor: pointer;\n        }\n    &lt;/style&gt;\n    &lt;div id=\"landing-container\" class=\"landing-container\"&gt;\n    &lt;/div&gt;\n`;\n</code></pre>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#the-shadow-dom","title":"The Shadow DOM","text":"<p>The obvious thing that will stand out here is the inclusion of the <code>&lt;style&gt;</code> tag to add inline CSS. This may initially feel like bad practice. Shouldn't the styling be in the application's stylesheet? The answer in this scenario is no, for two reasons.</p> <p>Firstly, one of the reasons for a web component is reusability. Although you may have the same stylesheet everywhere, there will always be the scenario where you realise you're missing something, make a change to the CSS in a particular application and then spend ages trying to work out why it's not appearing as you expect elsewhere. Adding the style in the template means the relevant styling is packaged with the web component.</p> <p>But web components can also be published to the web component community for use by any developers. And if you want the web component to be reusable, you need to package your styling with the component.</p> <p>yAnd in both cases you will probably want to avoid CSS bleeding through from the rest of the application and messing up the look and feel of the web component. In this particular scenario, we only need the CSS here and we're only using the component once in our application, so there's no real benefit in putting the CSS in our core spreadsheet. And we don't need to manipulate the contents of the web component from outside. So we can take advantage of the shadow DOM, which solves all these problems, and we can do so in the JavaScript class's constructor.</p> <pre><code>constructor() {\n    super();\n    this.root = this.attachShadow({ mode: \"closed\" });\n    let clone = template.content.cloneNode(true);\n    this.root.append(clone);\n    this.connected = false;\n}\n</code></pre> <p><code>this.attachShadow()</code> adds the shadow DOM and we need to be able to access it in the future. So we store it in the <code>root</code> property of the class. There are two shadow DOM modes, \"open\" and \"closed\". When set to \"open\", JavaScript can still reach into the web component's DOM, but by <code>getElementById().shadowRoot</code>. When set to \"closed\", JavaScript cannot access anything in the web component. We don't need to access the contents of this component from outside, so we've set it to \"closed\".</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#attributes","title":"Attributes","text":"<p>We will use this because we want the component to be reusable, so we don't want to define the tiles for the landing page inside the web component. To do this we'll define it as an attribute of the web component.  Like any other HTML element attributes - <code>style</code>, <code>value</code>, <code>class</code>, <code>required</code> etc - these can be declaratively defined in the HTML or programmatically.</p> <p>But there's a complication here: attributes are always strings. But our tiles need to be more complex than a single string value. There's another important point to make: attributes are always lower case. The content will be a JavaScript object:</p> <pre><code>const allTiles=[\n    {id: \"new-spot\", label: \"New Spot\", \"focus\": \"spot-ship-name\"},\n    {id: \"spot-search\", label: \"Find Spot\", \"focus\": \"\"},\n    {id: \"ship-search\", label: \"Find Ship\", \"focus\": \"ship-name\"},\n    {id: \"trips\", label: \"Trips\", \"focus\": \"\"}\n];\n</code></pre> <p>That's messy to pass in declarative HTML. We'll see where that's used in the future. But for this component we're going to create the element programmatically.</p> <pre><code>const landing = document.querySelector(\"#landing\");\nconst landingContainer = document.createElement(\"landing-elem\");\nlandingContainer.allTiles = JSON.stringify(allTiles);\nlanding.append(landingContainer);\ntoggleSPA(\"landing\", \"block\");\n</code></pre> <p><code>JSON.stringify()</code> and <code>JSON.parse()</code> are our friends here. As with classes in other languages, we can define getters and setters to interact with the properties. In this scenario, we could just use the getters and setters, but best practice is to map to attributes, so that's what we'll do here. This code is in our JavaScript class:</p> <pre><code>get allTiles() {\n    return JSON.parse(this.getAttribute(\"allTiles\"));\n}\n\nset allTiles(value) {\n    this.setAttribute(\"allTiles\", value);\n}\n</code></pre> <p>Yes, <code>get</code> and <code>set</code> are valid JavaScript, because we're in a class inside a JavaScript file, not just in the JavaScript file. Now we just need to use it.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#web-component-lifecycle-and-rendering-the-component","title":"Web Component Lifecycle and Rendering the Component","text":"<p>You will notice in the constructor that we set <code>this.connected</code> to false. In the context of a web component, \"connected\" means connected to the DOM. Until the component is connected to the DOM, code cannot access any of its attributes. So you want to delay code. But the good news is that web components provide callback functions for various parts of the lifecycle. At this point we'll just use one lifecycle event, <code>connectedCallback()</code>, which is fired when the component has been connected to the DOM.</p> <p>The code for our <code>connectedCallback</code> is:</p> <pre><code>connectedCallback() {\n    console.log(\"landing connected\");\n    this.render();\n    this.connected = true;\n}\n</code></pre> <p>We could just render the component here, but for clarity and convention from elsewhere, we'll use a <code>render()</code> function.</p> <pre><code>render() {\n    console.log(\"Creating tiles\");\n\n    const landing_container = this.root.getElementById(\"landing-container\");\n    this.allTiles.forEach(element =&gt; {\n        const tile = document.createElement(\"div\");\n        tile.id = `tile-${element.id}`;\n        tile.className = \"landing-tile\";\n        const span = document.createElement(\"p\");\n        span.className = \"landing-anchor\";\n        span.innerHTML = element.label;\n        tile.appendChild(span);\n        tile.addEventListener(\"click\", (event) =&gt; {\n            console.log(\"Firing event for \" + element.id);\n            event.preventDefault();\n            this.fireClickEvent(element.id, element.focus);\n        })\n        landing_container.appendChild(tile);\n    });\n}\n</code></pre> <p>Remember that we added a shadow DOM and we need to query its DOM through that, through <code>this.root</code> which was set in the constructor. Then we create a div for each tile and add an event listener. We saw in part three that event listeners are the modern approach. For ease, we call a function that we store in a property of the class.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#custom-events","title":"Custom Events","text":"<p>When we click on the tile, we want to change the view of the SPA, which we've coded in index.js as <code>toggleSPA</code>. That's fine in this application, but what about if we have a more complex application that isn't built as a Single Page Application? Or what if the web component is being published for wider use, we wouldn't want to require the developer to add a function with a specific name.</p> <p>This is where CustomEvents come in. Custom events allow you to trigger events with a specific name, which code can then listen for and map to a named function. They can also pass content across. <code>fireClickEvent</code> creates and triggers a CustomEvent.</p> <pre><code>fireClickEvent = (sectionId, focusField) =&gt; {\n    const event = new CustomEvent(\"changeView\", {\n        bubbles: true,\n        detail: {\n            viewName: sectionId,\n            focusField: focusField,\n            style: \"block\"\n        }\n    })\n    this.dispatchEvent(event);\n}\n</code></pre> <p>We create a new CustomEvent with the name \"changeView\" and passes a JavaScript object of options. <code>bubbles</code> means that the event bubbles up out of the web component up the DOM tree. The <code>detail</code> property allows us to pass information for the event listener to use, in this case the ID of the section to display, the field to pass focus to, and the display style we wish to apply to the section.</p> <p>After creating the CustomEvent, we then need to trigger the event, which is done via <code>this.dispatchEvent()</code>.</p> <p>This is one side of the event, but we need to receive the event. This is done with an event listener, registered from the <code>bootstrap()</code> function of our application with this code: <code>document.addEventListener(\"changeView\", changeView);</code>.</p> <p>This maps to a function stored in a constant:</p> <pre><code>const changeView = (event) =&gt; {\n    toggleSPA(event.detail.viewName, event.detail.style);\n    if (event.detail.focusField != \"\") {\n        const page = document.getElementById(event.detail.viewName);\n        const field = page.querySelector(`#${event.detail.focusField}`);\n        if (field) field.focus();\n    }\n}\n</code></pre> <p>You'll see this function takes the event - a CustomEvent object - that we dispatched. We can access the properties we defined in the <code>detail</code> JavaScript object to toggle display of the various sections and put focus in the relevant field.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#wrap-up","title":"Wrap Up","text":"<p>Web components will be the core of the application we're building. We will do some things differently and use other lifecycle events throughout the rest of the application. And custom events will be the basis of passing functionality around. But this is the basis of the other \"pages\" in the application.</p> <p>Web components are becoming a key aspect of modern web development and well worth getting to grips with. Many are built as NodeJS components, which make it less straightforward to integrate into an application like this, unless you use a tool like Vite to build your web application. And that is well worth looking at, particularly if you want to use TypeScript, instant server start and easy optimized build.</p> <p>But you can still integrate web components into other frameworks, as I've done recently with an EmberJS application.</p> <p>So could you integrate web components into XPages? It's probably not straightforward because of the conflict of server-side and client-side processing. Web components work client-side and manage their properties in the client-side against the DOM HTML element. However, an XPages partial refresh will replace all HTML for its refresh area. This means it will nuke the web component that was there prior to the partial refresh and replace it with a new version of the web component. And that's probably not what you would want with XPages, or any other framework that replaces HTML in the DOM. But if you fully understand XPages, there may be places where it does make sense, places where you just want to handle everything client-side. Or there could be scenarios where you could pass the current component and its attributes to the server side of XPages and generate new HTML with updated attributes.</p>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/21/framework-web-8/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino","Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/","title":"XPages App to Web App: Part Nine - Services","text":"<p>We've got a login and a landing page, we're ready to start building the bulk of the application now. But we need the data. In part six we handled the login, both for a mock session and the actual authentication to Domino REST API. But that format is going to quickly get messy as we build out the rest of the application. We can do better.</p> <p>As with the landing page web component, we're going to write JavaScript classes for the mock services and the actual services. But some of the functionality we need will be common. So we will also create a base class as well. Our three classes will be:</p> <ul> <li>BaseService.</li> <li>MockService, extending BaseService.</li> <li>DominoService, extending BaseService.</li> </ul> <p>Again we'll be doing <code>export default class BaseService</code>, <code>export default class MockService as BaseService</code> and <code>export default class DominoService as BaseService</code>. In each file we'll import the relevant class from the JavaScript file, e.g. <code>import BaseService from \"./baseService.js\";</code></p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#login-process","title":"Login Process","text":"<p>Revisiting the login process in part six, it goes through a few steps:</p> <ol> <li>Check username to verify if we're in a mock session.</li> <li>Log in.</li> <li>Load some session data.</li> </ol> <p>The important point to note here is that the login logic will differ for the two services. So both classes will have a <code>login()</code> function, but with different code. And even though the URLs will differ, the code for loading the session data will be the same. If we architect the code well, we can have single <code>loadSessionData()</code> function in the BaseService class.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#constructor","title":"Constructor","text":"<p>So we will need the URLs available in the BaseService class. In JavaScript, we can just put them in a JavaScript object, e.g. for our mock class it's:</p> <pre><code>const mockUrls = {\n    countries: \"data/countries.json\",\n    ports: \"data/ports.json\",\n    shipTypes: \"data/shipTypes.json\",\n    shipLines: \"data/shipLines.json\",\n    ships: \"data/ships.json\",\n    trips: \"data/trips.json\",\n    spotsByShip: \"data/spotsByShip.json\",\n    spotsByDate: \"data/spotsByDate.json\"\n};\n</code></pre> <p>For those with less experience of JavaScript, note this is a JavaScript object, not a JSON object. JSON object keys must be strings, in JavaScript objects they are properties.</p> <p>For the mock service, we can just use these URLs to retrieve the data, because the files are in a data subdirectory of our web application. But for the Domino REST API we'll have a base URL, the Domino REST API server + \"/api/v1\". We could put this in each URL. Instead, we can have a base URL and just pass an empty string for the mock services.</p> <p>So we'll have a constructor in the BaseService with this code:</p> <pre><code>constructor(baseUrl, urls) {\n    this._baseUrl = baseUrl;\n    this._urls = urls;\n    this._token = \"\";\n}\n</code></pre> <p>This means we'll have the base URL and URLs in the BaseService and both derived classes. We also instantiate the token to an empty string. These could be variables in the class. But my preference is to use getters and a setter for <code>token</code>. This means the base URL and URLs can only be set by passing them to the constructor, but the token can be set from outside the class.</p> <p>The good news is that the constructors of the derived class, MockService and DominoService, don't need arguments. They can just pick up private variables in the relevant classes. So our MockService's constructor is <code>super(\"\", mockUrls);</code> and our DominoService's constructor is <code>super(baseUrl, urls);</code>.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#login","title":"Login","text":"<p>First of, we'll modify the login function in index.js and replace it with this:</p> <pre><code>const login = async (user, pwd) =&gt; {\n    // Set as mocking only\n    if (user === \"John Doe\") {\n        window.dataService = new MockService();\n    } else {\n        window.dataService = new DominoService();\n    }\n    const result = await window.dataService.login(user, pwd);\n    if (result === \"success\") {\n        const landing = document.querySelector(\"#landing\");\n        const landingContainer = document.createElement(\"landing-elem\");\n        landingContainer.allTiles = JSON.stringify(allTiles);\n        landing.append(landingContainer);\n        toggleSPA(\"landing\", \"block\");\n    } else {\n        statusError(result);\n    }\n};\n</code></pre> <p>We create an instance of the relevant class, depending on the username. And we store it in <code>window.dataService</code> property. This means everywhere else, we can just use <code>window.database</code> and always get the relevant service. So for logging in, we can just do <code>window.dataservice.login()</code>.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#mockservice-login","title":"MockService Login","text":"<p>The MockService will run immediately, but the DominoService needs to wait for a response from DRAPI. For ease, we'll just make both <code>login()</code> functions async and await regardless. The MockService login is very straightforward:</p> <pre><code>async login(user, pwd) {\n    window.isMock = true;\n    this.bubbleMessage(\"confirm\", \"Login successful\");\n    super.loadSessionData();\n    return \"success\";\n}\n</code></pre> <p>When the code was just in index.js, we simple called a function in index.js. We can't do that from the service classes. But instead we can leverage the technique we used in the landing page web component - CustomEvents. We'll need to send a lot of messages to the user, so we'll create a method in the BaseService class, <code>bubbleMessage()</code> and call that. It will take two parameters, \"confirm\" or \"error\" for the message type and a message to display. The <code>bubbleMessage()</code> function will be:</p> <pre><code>bubbleMessage(type, message) {\n    const messageObj = {\n        \"type\": type,\n        \"message\": message\n    };\n    document.dispatchEvent(new CustomEvent(\"sendUserMessage\", { bubbles: true, detail: messageObj}));\n\n}\n</code></pre> <p>We pass the type and the message to an object, put that in the CustomEvent's detail. As with the other custom events, an event listener will be registered in index.js to listen for the \"sendUserMessage\" event and call the <code>statusMsg()</code> or <code>statusError()</code> function there, depending on the type.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#dominoservicelogin","title":"DominoServiceLogin","text":"<p>The DominoService login function is more complex, but doesn't change much from what we had in part six. We perform the login, call a local <code>extractCredentials</code> function, then load the session data. The key differences are that the <code>extractCredentials()</code> method is in the DominoService class, so we call it with <code>this.extractCredentials()</code> and the <code>token</code> property is in the BaseService, so we call <code>this.token = bearer</code>.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#loadsessiondata","title":"loadSessionData","text":"<p>We don't want to tie up the application while loading the session data. So we won't await the success or failure of that, but we can again use <code>bubbleMessage()</code> to notify the user of completion. From experience of running the application in production, even on a patchy network, the session data loads very quickly. Ships is the largest at about 700Kb.</p> <p>The good news is that this function can be held just in the BaseService class. First off, we'll set some variables common for all fetch requests:</p> <pre><code>const headers = {\n    Accept: \"application/json\",\n};\nif (this.token != \"\") {\n    headers.Authorization = `Bearer ${this.token}`;\n}\nconst getParams = {\n    method: \"GET\",\n    headers: headers,\n};\n</code></pre> <p>Remember the MockService doesn't need any authorization. Because we instantiate the token property to an empty string in the BaseService constructor, we can check it's still not an empty string. We want to load countries, ports, ship types, ship lines, ships and trips. But we can load them all at the same time, so we create an array of Promises:</p> <pre><code>Promise.all([\n    fetch(this.baseUrl + this.urls.countries, getParams)\n        .then((response) =&gt; response.json())\n        .catch((err) =&gt; this.bubbleMessage(\"error\", err.message)),\n    fetch(this.baseUrl + this.urls.ports, getParams)\n        .then((response) =&gt; response.json())\n        .catch((err) =&gt; this.bubbleMessage(\"error\", err.message)),\n....\n</code></pre> <p>We don't expect any to fail. And if one does, we won't be able to use the application. So this works for our needs. This will return an array of responses in the same order as the requests, so we can process them accordingly. Some will get loaded into sessionStorage or localStorage. But we'll do something a little more sophisticated for others, as we'll see as we get further into the application.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#preventing-double-clicking","title":"Preventing Double-Clicking","text":"<p>As we move further through the application, we'll want to prevent double-clicking by users. Some frameworks have provided helpers to do this. Other applications don't bother. That is even more surprising considering how easy it is to do now, with just CSS. By adding this class to the main div container of the application, we can let the user see that a backend service is running and prevent double-clicking:</p> <pre><code>.container-mask {\n  background-color: #DDDDDD;\n  pointer-events: none;\n}\n</code></pre> <p>When the backend service has completed, or we've hit an error, we can remove the class.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#wrap-up","title":"Wrap Up","text":"<p>We're ready now to start on the first edit form, for ships. This will perform multiple purposes - searching, viewing and editing. But for all of those purposes we will need to interact with data loaded from the services.</p>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/23/framework-web-9/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/","title":"XPages App to Web App: Part Ten - Ship Form Actions","text":"<p>In the last two parts we created our first web component and converted the login function into services we could use for all data interactions, the first use being to load data for any select controls in the application. Now it's time to create the ship form.</p> <p>This will also be built as a web component. This may seem a case of using something new just for the sake of it. But it keeps all the code for our ship self-contained in a single JavaScript file, which I'm sure will make it easier to support later on - after all, knowing quickly where to find the relevant code is key to quickly supporting an application. And we'll also see some big advantages of this choice later on.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#component-declaration-and-html","title":"Component Declaration and HTML","text":"<p>Again, we'll declare the class with <code>export default class Ship extends HTMLElement</code> and add its custom element definition with <code>customElements.define(\"ship-elem\", Ship);</code>.</p> <p>For a start the HTML will be this:</p> <pre><code>&lt;section class=\"ship\" id=\"ship-search\"&gt;\n    &lt;div class=\"actionDiv\"&gt;\n        &lt;span id=\"ship-search-action\" class=\"material-icons material-button md-32\" title=\"\"\n        role=\"img\" aria-label=\"\" style=\"float:right\"&gt;&lt;/span&gt;\n        &lt;span id=\"ship-search-back\" class=\"material-icons material-button md-32\" title=\"Back\"\n        role=\"img\" aria-label=\"Back\" style=\"float:right\"&gt;arrow_back&lt;/span&gt;\n        &lt;h1&gt;Find Ship&lt;/h1&gt;\n    &lt;/div&gt;\n    &lt;form id=\"search-ship-form\"&gt;\n        &lt;ship-elem showspots=\"true\" actionbutton=\"Search\" actionid=\"#ship-search-action\"\n            class=\"grid-container\"&gt;&lt;/ship-elem&gt;\n    &lt;/form&gt;\n&lt;/section&gt;\n</code></pre> <p>Again, we'll add an event listener on the Back button in the bootstrap function. But this won't be the last Back button in our application. So I'm going to reuse the function my friend Stephan Wissel had for our session at Engage 2024. This allows us to pass an HTML element ID and a function to perform. This allows us to do:</p> <pre><code>captureClickEvent(\"ship-search-back\", (event) =&gt; {\n    toggleSPA(\"landing\", \"block\");\n});\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#reusable-action-button","title":"Reusable Action Button","text":"<p>But you'll notice there's another action button here. But it's got no title or aria-label, and we're not going to add an event handler to it in the bootstrap function. Why not? Typically in a Notes Client or XPages application, we add action buttons for all actions, hiding or showing each as required. We have three specific functions - search, edit, and save. But we will only need one at any time. So rather than add all buttons and set the display property on them at an appropriate time, we're going to have a single button and give it the relevant title, icon and event handler as appropriate. And we're going to use an additional piece of functionality in web components and an additional piece of functionality in event handlers too.</p> <p>In the constructor, there will be two differences from the landing web component. The first is we will not attach a shadow DOM, we'll just clone the template node and add it directly to this component. The reason is one I mentioned when covering the landing component, that the shadow DOM creates a barrier to CSS. There are ways to pass a stylesheet in without reloading it. But we don't need to force a specific CSS on our forms, so it's easier to just not attach a shadow DOM.</p> <p>This means there's a significant difference between how we access the elements added to the DOM. When there's a shadow root, we need to do <code>this.root.getElementById()</code> and <code>this.root.querySelector()</code> to access anything, because the shadow DOM creates a barrier. But without the shadow DOM, we need to do <code>this.getElementById()</code> and <code>this.querySelector()</code>.</p> <p>The second difference is that we'll create the component from the template in the constructor. Some may consider it better practice to not create the component in the constructor, but only after the web component is connected to the DOM. And if you're programmatically adding a component, this allows you to lazy load the HTML. But we will always want the component on the page and it's a small application, so there's no real impact on the \"Time to Interactive\". But this choice gives us a big benefit: the component will be created before attributes are observed and we want to leverage that functionality to manipulate our reusable button.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#observing-attributes","title":"Observing Attributes","text":"<p>We will add four key attributes. The first is prefix, because we're going to be setting HTML element IDs but we'll also need the ship form when creating a spot, where we will select a ship that we've spotted. And browsers don't like it when you add two or more elements with the same ID. Having a \"prefix\" attribute allows us to modify the IDs of all elements inside the web component in a logical way that our code can still find the relevant HTML element. We'll add  showspots to determine whether or not to display spots for the relevant ship, because we won't want to do this when we're creating a spot. And we'll add actionid with the ID of the reusable action button we want to manipulate and actionbutton for which button to show by default.</p> <p>Remember the two key points for attributes:</p> <ul> <li>they're always lower case.</li> <li>the values are always strings.</li> </ul> <p>This means when we're checking if <code>showspots</code> is set, we'll need to do <code>this.showspots === \"true\"</code>. To avoid needing to set it when we don't want it, we'll also make the getter optionally return false:</p> <pre><code>get showspots() {\n    return this.getAttribute(\"showspots\") || false;\n}\n</code></pre> <p>We want to update the action button at certain points in the lifecycle. We could just run a function every time. But web components have a different way, observed attributes. This allows you to run code if a certain attribute changes. The attributes to watch for changes on are observed through a static getter with an array of attribute names:</p> <pre><code>static get observedAttributes() {\n    return [\"actionbutton\"];\n}\n</code></pre> <p>This registers the attributes to watch for, but the second part is running the code when they change. This is done in another standard lifecycle callback, attributeChangedCallback. This takes three parameters, the name of the attribute being changed, its old value and its new value. Typically a switch statement will control the logic:</p> <pre><code>attributeChangedCallback(name, oldValue, newValue) {\n    switch (name) {\n        case \"actionbutton\":\n            if (this.defaultActionButton === \"\") this.defaultActionButton = newValue;\n            this.updateButton();\n            break;\n    }\n}\n</code></pre> <p>Here we're putting the first button action type in a <code>defaultActionButton</code> variable, so we know the action to set when we reset the form. Then we call the <code>updateButton()</code> function.</p> <p>If we were rendering the component in a <code>render()</code> function from the <code>connectedCallback()</code> like we did for the landing component, we would need to manually call the <code>updateButton()</code> function at the end of the <code>render()</code> to trigger it. But because we're rendering the component in the constructor, we can let the <code>attributeChangedCallback</code> run it for us. Of course there are times when that would not be what you want. But in this scenario, it's exactly what we want.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#updatebutton","title":"UpdateButton","text":"<p>The first thing we're going to do is clone the HTML and replace the node.</p> <pre><code>const actionButton = document.querySelector(this.actionid);\nconst newActionButton = actionButton.cloneNode(true);\nactionButton.parentNode.replaceChild(newActionButton, actionButton);\n</code></pre> <p>This may seem strange, but there's a challenge we have to overcome. The button will have an event listener registered. And there's not an API to remove all event listeners. You need to remove an event listener by type and listener function. That can get fiddly. But cloning the node and replacing it removes all event listeners. Obviously that could cause problems if another developer is adding an event listener from somewhere else, suddenly their event listener gets removed and they need to handle that. But we don't have to worry about that here.</p> <p>As mentioned, we're going to have three states - search, edit, and save. If the button says \"Search\", we'll only enable ship name and call sign and add event listeners on those fields to perform a search if the user presses enter. If the buttons says \"Edit\", the document is in \"read mode\" so we'll disable all the fields and we won't need the search event listeners. If the button says \"Save\", the document is in edit mode so we'll enable all fields. We'll want the ability to search for a ship. But we'll also want to add an event listener on a help icon to show shipping lines that own ships, because a new shipping line may need adding, but we want to select an pre-existing one.</p> <p>This makes the code pretty straightforward:</p> <pre><code>updateButton() {\n    const actionButton = document.querySelector(this.actionid);\n    const newActionButton = actionButton.cloneNode(true);\n    actionButton.parentNode.replaceChild(newActionButton, actionButton);\n    const shipNameInput = this.querySelector(`#${this.prefix}ship-name`);\n    const callSignInput = this.querySelector(`#${this.prefix}ship-call-sign`);\n    const linesHelp = this.querySelector(`#${this.prefix}ship-lines-help`);\n    switch(this.getAttribute(\"actionbutton\")) {\n        case \"Search\":\n            this.enableDisable(\"search\");\n            newActionButton.title=\"Search\";\n            newActionButton.ariaLabel = \"Search\";\n            newActionButton.innerText = \"search\";\n            newActionButton.addEventListener(\"click\", this.doSearch);\n            shipNameInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\n            callSignInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\n            try {\n                linesHelp.removeEventListener(\"click\", this.showLinesHelpDialog);\n            } catch (error) {\n                // No eventlistener, no action\n            }\n            break;\n        case \"Edit\":\n            this.enableDisable(\"all\");\n            newActionButton.title = \"Edit\";\n            newActionButton.ariaLabel = \"Edit\";\n            newActionButton.innerText = \"edit\";\n            newActionButton.addEventListener(\"click\", this.doEdit, {once: true});\n            try {\n                shipNameInput.removeEventListener(\"keydown\", this.checkEnterDoSearch);\n                callSignInput.removeEventListener(\"keydown\", this.checkEnterDoSearch);\n                linesHelp.removeEventListener(\"click\", this.showLinesHelpDialog);\n            } catch (error) {\n                // No eventlistener, no action\n            }\n            break;\n        case \"Save\":\n            this.enableDisable(\"none\");\n            newActionButton.title = \"Save\";\n            newActionButton.ariaLabel = \"Save\";\n            newActionButton.innerText = \"save\";\n            newActionButton.addEventListener(\"click\", this.doSave, {once: true});\n            try {\n                shipNameInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\n                callSignInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\n                linesHelp.addEventListener(\"click\", this.showLinesHelpDialog);\n                this.lines = this.lines;\n            } catch (error) {\n                // No eventlistener, no action\n            }\n    }\n}\n</code></pre> <p>Comparing when we're adding the event listeners to the <code>newActionButton</code> there is a difference. For \"Edit\" and \"Save\" we pass a third argument, a JSON object. These are the options. There are a number of options that can be passed, but the one we're passing here is <code>once</code>. This means the event handler is triggered once and then removed. If this is added, the button cannot be clicked twice. That makes sense for Edit and Save buttons. Obviously, we don't want that for Search though.</p> <p>This makes the <code>doEdit</code> and <code>doSave</code> functions quite simple. We don't need to call <code>updateButton()</code>, we just need to change the attribute, and the <code>attributeChangedCallback()</code> handles updating the button HTML and adding event listeners:</p> <pre><code>doEdit = () =&gt; {\n    this.setAttribute(\"actionbutton\", \"Save\");\n}\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#wrap-up","title":"Wrap Up","text":"<p>There's still more to do on the ship form, like the search and the save. But that's for another part.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/10/30/framework-web-10/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/11/19/benefits-standard/","title":"Reaping the Benefits of Standard","text":"<p>More than three years ago we began work on VoltScript. A little over a year ago we released our first Early Access code drop. There were certain core principles to which we developed. Two of these were a modern developer experience and adoption of standard development practices.</p> <p>Over recent weeks I received ample evidence of the benefits as I improved and extended Archipelago, the build management engine of VoltScript.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#ide-choice","title":"IDE Choice","text":"<p>We were never going to use Domino Designer as an IDE, because we were never going to be working with design elements in an NSF. And the LotusScript editor doesn't work with files and folders. So we needed to choose a development IDE.</p> <p>The IDE we targeted for developers was Visual Studio Code, with extensions for the language server and build management. On a recent webinar we were asked if we would be building extensions for the JetBrains IDE WebStorm. The extensions require a considerable amount of development. One of the main reasons for choosing VS Code, apart from its widespread use, was that it uses the Monaco editor, which can also be embedded into any web application. After that choice and before we got to integrating with Volt MX Go's Foundry server, work had already and independently been done to integrate the Monaco editor for JavaScript. The choice to target VS Code, a standard and open IDE, was already bearing fruit.</p> <p>More recently, I remembered that VS Code extensions also work in Eclipse Theia, which has its own desktop install and can also be used via a browser. It requires spinning up a separate Docker container for each developer, which is likely to be a barrier to many environments. But with a customised Docker image that included the VoltScript runtime and the VS Code extensions, I had a fully functional browser-based development environment within a few hours.</p> <p>And with multiple AI-first IDEs like Cursor and Windsurf, all of which are using forks of VS Code's IDE and can run VS Code extensions, the options are many. And all because we chose a standard IDE that had open source code.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#ai-integration","title":"AI Integration","text":"<p>Another advantage of choosing a standard IDE was that there is a wide community of developers writing extensions. GitHub and JIRA integration for the workspace folder? Provided before we started the project. Integration with task management for TODOs, FIXMEs and more? Again, already built. A modern terminal to integrate with, which supported console colours? Again, already there.</p> <p>And with the rise of AI and GitHub Copilot, we (and anyone using an early access code drop) had AI integration free with no additional effort. No need for people to wait for a new IDE release from us. No need for additional development by us to integrate AI functions into the IDE. Everything was there with absolute zero effort. Because we used a standard IDE.</p> <p>I've been using GitHub Copilot for VoltScript coding for many months now. Although it understandably doesn't know some APIs - GitHub Copilot's training set is dated from October 2023 - it understands a lot of syntax. (I'm not sure if that's because it's working from LotusScript or Visual Basic.)</p> <p>But it provides some functionality that would otherwise need coding, like code formatting, refactoring code out into its own function, checking case, checking cyclomatic and cognitive complexity.</p> <p>In the most recent development, I had calls to a function that passed a platform name and the platform label to use in a config file. I was refactoring this to a <code>for</code> loop and moving the mapping of platform name to label into a <code>Select Case</code> statement inside the function. I removed the parameter from the function, but did not yet update the function calls. As I typed the <code>Select Case</code> statement, it correctly completed each <code>Case</code> statement.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#json-validation","title":"JSON Validation","text":"<p>Our archipelago build management system requires a configuration file alone the lines of Maven's pom.xml, NodeJS's package.json or XPages' Xsp Properties.</p> <p>In an NSF, the Xsp Properties is a properties file, but with a graphical user interface with checkboxes, drop-downs and input boxes. Because we're using Visual Studio Code, which does not have graphical interfaces, it would be a poor decision to create a graphical user interface. Instead, we chose JSON (well, actually JSONC, JSON with Comments) because it's easy to contribute a schema and samples via the VS Code extension.</p> <p>As one of the improvements, I needed to add validation to prevent \"latest\" as a version for VoltScript Extensions. Again, because we were using JSON, with a JSON schema, it was easy to find a solution...because it was standard.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#coding","title":"Coding","text":"<p>Because we were making changes to the underlying runtime engine, regression testing was important. With the absence of any LotusScript testing framework, creating one was a crucial step early on. This was out first VoltScript Library Module, was open sourced before we did our first Early Access code drop, and has also been ported to LotusScript as bali-unit.</p> <p>Writing unit tests is standard in most coding languages. So obviously Archipelago has unit tests throughout for various functions, as well as integration tests for external REST calls and writing files. So throughout the recent development, I was identifying functions that needed updating, running the unit tests before and after, as well as updating them for additional tests that were required. And because it can be integrated into build pipelines, when I created the pull request for my changes, the unit tests ran as well.</p> <p>And coming back to GitHub Copilot, writing unit tests is something I've found Copilot particularly good at speeding up development.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#json-validation-in-code","title":"JSON Validation in Code","text":"<p>I covered using a JSON schema earlier to show validation errors in VS Code. But that doesn't stop the developer actually saving the invalid file in VS Code - or using another IDE. So when we run dependency management, we need to validate the JSON file as well, before converting it from JSON to a VoltScript object.</p> <p>Unit testing frameworks are pervasive. But when I was writing VoltScript, I found a use for unit testing that I've not seen in any other language: validation. If a test suite runs tests and they pass, the thing being tested is valid; if a test suite runs tests and they fail, it's not valid. So if the output is suppressed, you can use a unit testing framework for validation. So we do. And archipelago was the first place where I did that, for validating the JSON configuration file.</p> <p>So I needed to add validation for a new \"runtimePlatforms\" property and to ensure \"latest\" was not used as the version for a VoltScript Extension. To do this now proved very quick to write - just creating a few additional tests.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#json-conversion","title":"JSON Conversion","text":"<p>But handling JSON objects throughout the code does not make for readable and easily supportable code. So converting JSON to VoltScript objects makes things much clearer. However, that can be verbose if the conversion is manually coded.</p> <p>Again, we have taken a leaf out of other languages. And we have VoltScript JSON Converter, which work in some ways similar to Google GSON with converters and automated mapping to object properties.</p> <p>As a result, the changes needed to handle converting the <code>runtimePlatforms</code> property was literally two lines of changes. First, adding a property to the <code>Atlas</code> object - we don't need getters and setters, an <code>Execute</code> statement sets the property value. Secondly, a mapping to use a StringCollectionConverter for the <code>runtimePlatforms</code> property.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/11/19/benefits-standard/#conclusion","title":"Conclusion","text":"<p>I've always preferred a standard approach where possible. While some Domino developers use custom design element for Java classes and JARs, I only ever used the standard Eclipse functionality, editors and right-click menus. So whereas others have hit problems, I never did. And as an added benefit, when I needed to code normal Java projects, I was already familiar with everything.</p> <p>So when it came to VoltScript, I again wanted to choose standardisation. The benefit came that I was able to complete what had been designated as seven JIRA points (at least two days' work) in less than a day.</p>","tags":["LotusScript","VoltScript","Volt MX Go"]},{"location":"blog/2024/12/08/new-blog/","title":"Introducing My New Blog","text":"<p>Welcome to my new blog. It may seem a lot of effort to switch from one blog to another. But I've been working with Material for MKDocs for some years, thanks to Stephan Wissel, and it's a great framework for documentation. I've been aware of the blog plugin for some time, so it made sense to consider it as a good fit for the future.</p> <p>Don't get me wrong, Jekyll and GitHub Pages was the right choice for me back when I started the blog in 2016. It was still the right choice for me when I joined HCL and started blogging more frequently in 2019. But the right choice changes, depending on how the frameworks evolve and the individual's experience and level of comfort with both the framework and ancillary technologies.</p> <p>My experience of Jekyll has not increased much, even though my level of experience with Liquid did increase when setting up HCL's open source site. On the contrary, my experience with Material for MKDocs has increased massively through Domino REST API documentation and using it extensively for VoltScript documentation. There are places where it's not as flexible - integrating non-static data into a page. But there are places it offers significant usability improvements that I cannot easily solve with Jekyll, with better anchor tags in blog posts, nicer integration with code samples, better annotations, easy integration with Mermaid JS for diagramming. Note, I don't say they can't be solved, it's that they're more easy for me to solve with MKDocs.</p> <p>But the opportunity to migrate a blog is also a good opportunity to leverage other learning to improve the content.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#visual-studio-code","title":"Visual Studio Code","text":"<p>My experience of Visual Studio Code has significantly improved since I started blogging. Alongside it has improved my use of extensions and other quality improvements.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#markdownlint","title":"Markdownlint","text":"<p>All our VoltScript documentation sites use Markdownlint configuration to tailor settings, as does the Domino REST API documentation. There are a variety of settings that are overridden, some for personal preference. But cleaning up the Markdown definitely makes me feel a lot happier with the quality of the content.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#code-spell-checker","title":"Code Spell Checker","text":"<p>One of the de facto plugins I use with all my documentation is Code Spell Checker. As I've gone through updating all the blog posts, I've picked up a number of typos thanks to this plugin. It will almost certainly help minimise the number of typos in future blog posts too.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#problem-pane","title":"Problem Pane","text":"<p>The right-hand gutter of the editor in VS Code is very useful for seeing errors visually - yellow for Markdownlint violations, blue for typos. But the problems pane allows you to see them all quickly and easily navigate to them. The key to using any IDE is learning and leveraging what it offers out-of-the-box - as I found when Domino Designer was rebased into Eclipse.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#material-for-mkdocs-and-docker","title":"Material for MKDocs and Docker","text":"<p>My preference, where possible, is always to use Docker containers for development. It helps keep the host clean and minimises problems with technology conflicts. Material for MKDocs is no different. Stephan Wissel set up a customised Docker image for MKDocs which is used by HCL. But this was missing some plugins I needed and, with the Docker skills I've honed over the years, creating a custom Dockerfile from the main Material for MKDocs image was not difficult.</p> <p>The difficult part was that I had an older image of Material for MKDOcs locally, and the build didn't work. A rookie mistake, solved by pulling down the latest image first. Once I did that, everything built fine.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#professional-bio","title":"Professional Bio","text":"<p>In my old blog, my professional bio pulled from JSON files and added styling to show and hide sections. I could have done the same in MKDocs, with a single page, but it was a good opportunity to split it into different pages. I think this provides a cleaner navigation.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#post-updates","title":"Post Updates","text":"<p>The big difference between Jekyll and MKDocs is the frontmatter that's needed. There are also differences to how relative links are done. All images had been referenced with absolute links, which are discouraged by MKDocs. I was also using post excerpts and validating in the mkdocs.yml to force every blog post to have an excerpt. And doing things like tags and categories is different.</p> <p>So unfortunately that meant modifying all blog posts. That was a few hours work, but gave me the opportunity to fix any of those quality issues I mentioned above. So worth doing.</p> <p>But it also allowed me to add a page linking to all blog series, which will hopefully make it easier to track linked blog posts.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#rss-feeds","title":"RSS Feeds","text":"<p>Unfortunately, the change means the RSS feed URLs are changing. But I realised as I was doing this that there is nothing in the old site to tell users about the RSS feeds. So I added something.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#comments","title":"Comments","text":"<p>GitHub Discussions are something added a few years ago to GitHub repositories. This allows you to store comments in GitHub, and Material for MKDocs has easy-to-follow steps for integrating Giscus, which has a GitHub application that you can integrate with specific repositories. So I've decided to take this opportunity to change my commenting engine to use Giscus.</p> <p>Based on the direction my career has gone over the last five years, as well as the increased usage of GitHub, expecting my audience to have a GitHub account is very reasonable. But there may be other ways added to manage discussions in the future, options which I can't talk about at this time.</p>","tags":["Editorial"]},{"location":"blog/2024/12/08/new-blog/#summary","title":"Summary","text":"<p>Hopefully you will enjoy the new site. But I'd welcome feedback.</p>","tags":["Editorial"]},{"location":"blog/2024/12/12/forall/","title":"ForAll Loops and Type Mismatches","text":"<p>There are always challenges when moving from something familiar to something similar but different. When you've been working with that \"something familiar\" for a very long time, some things become second nature. But when you move to \"something similar but different\", sooner or later you hit an error and become convinced it's not working like it used to. This is even more probable if enhancements mean you more frequently come across a specific paradigm. The rash developer starts throwing accusations, assuming they must be right because they have years of experience on their side. The wise developer double-checks first...and may find out that they were mistaken.</p>","tags":["VoltScript","LotusScript"]},{"location":"blog/2024/12/12/forall/#the-problem","title":"The Problem","text":"<p>When you have a great developer on your team, they improve on the past. Processing collections is one such place where Bob Balaban improved VoltScript. Every Domino database has business logic or data processing agents that loops through collections. Every developer knows the paradigm:</p> <pre><code>Dim coll as NotesDocumentCollection\nDim doc as NotesDocument\n\nSet doc = coll.getFirstDocument()\nWhile Not doc Is Nothing\n    ...\n    Set doc = coll.getNextDocument(doc)\nWend\n</code></pre> <p>Every LotusScript developer, at some time or another, has written this kind of loop and forgotten the highlighted line and caused an infinite loop. The wise developer will type the highlighted line first when they write a loop. Some may even have leveraged IDE functionality to store snippets for the whole <code>While</code> loop, to speed up their development.</p> <p>But when we started with VoltScript, we wanted to prevent this kind of problem. There are ways that's easy to set up. If you have a Variant array - or a function returns a Variant array - you can use a <code>ForAll</code> loop. This can also be done on Lists.</p> <p>The most common data collection we will iterate in VoltScript is probably a JsonObject (which can be a JSON object or a JSON array), using <code>JsonObject.getChildren()</code>. But sooner or later you want to pass the ForAll reference variable (the current element in the container) to a function, something like this:</p> <pre><code>ForAll elem in jsonObj.getChildren()\n    Call testValue(elem)\nEnd ForAll\n</code></pre> <p>When coding this in VoltScript, the compiler frequently complained, giving me a <code>Type Mismatch</code> error for the call to <code>testValue()</code>. The solution was simple:</p> <pre><code>Dim temp as JsonObject\nForAll elem in jsonObj.getChildren()\n    Set temp = elem\n    Call testValue(temp)\nEnd ForAll\n</code></pre>","tags":["VoltScript","LotusScript"]},{"location":"blog/2024/12/12/forall/#comparisons-with-other-languages","title":"Comparisons with Other Languages","text":"<p>This is consistent with what we have to do in Java. In Java, the syntax for casting the variable to a specific Class can be done inline:</p> <pre><code>animals.forEach(animal -&gt; {\n    if (animal instanceof Dog) {\n        ((Dog) animal).woof();\n    }\n})\n</code></pre> <p>But the variable still needs to be cast to the derived class, if the method being called does not take the generic class or the generic class does not have the method to be called.</p>","tags":["VoltScript","LotusScript"]},{"location":"blog/2024/12/12/forall/#lotusscript","title":"LotusScript","text":"<p>But when developing VoltScript, I've been conscious to try to avoid introducing more verbose syntax that is not consistent with LotusScript development. This felt unusual compared to my experience of LotusScript development. But was this perception or reality? I created a ticket to investigate whether this is different to LotusScript.</p> <p>Writing something comparable to the JsonObject.getChildren() ForAll loop proved less straightforward. As mentioned, NotesDocumentCollection and NotesViewEntryCollection cannot be iterated by a ForAll loop. The same is the case for more recent classes like NotesJsonObject. Fortunately, I was able to find one pretty quickly.</p> <pre><code>ForAll itm in doc.Items\n    Call printName(itm)\nEnd ForAll\n\nSub printName(item as NotesItem)\n    Print item.name\nEnd Sub\n</code></pre> <p>This confirmed we had not introduced a regression in VoltScript. LotusScript also gave the <code>Type Mismatch</code> error that VoltScript had given. What others might have called a \"bug\" was merely a mistaken expectation because we challenged the paradigm, broke from the <code>getFirst...</code> and <code>getNext...</code> approach and increased the number of places where ForAll loops could be used in VoltScript.</p>","tags":["VoltScript","LotusScript"]},{"location":"blog/2024/12/12/forall/#conclusion","title":"Conclusion","text":"<p>Is the required syntax ideal? Maybe not.</p> <p>Is there a better, easy-to-code solution? Maybe we could add a global function to JsonVSE called <code>castToJsonObject(incoming as Variant)</code>. But will this be intuitive to people who have not looked at the full API doc for JsonVSE? Probably not. And would it easier than the solution I used? Probably not.</p> <p>Is the <code>Type Mismatch</code> error obvious on what to do to solve it? Maybe not. So I plan on updating the API doc sample code and the How Tos, ready for our next release.</p> <p>But the million-dollar question is, are there better problems for us to fix. Absolutely!</p>","tags":["VoltScript","LotusScript"]},{"location":"blog/2024/12/14/framework-web-11/","title":"XPages App to Web App: Part Eleven - Ship Search and Save","text":"<p>The last part focused on using <code>observedAttributes()</code> to switch what the main action button did, depending on the current document state. In this part we'll cover the functionality behind the search button and the save.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#how-it-used-to-work","title":"How It Used To Work","text":"<p>As mentioned in the previous part, this ship component will also be used for ship details when logging spotting a ship. On the previous XPages application, finding a ship was done using a Value Picker, which used a list of ship names with a \"starts with\" search. This resulted in two problems.</p> <p>Firstly, the Value Picker performs a partial refresh to load the options. One a number of occasions the request for this timed out - the perils of seaside locations!</p> <p>Occasionally - I think a couple of occasions over the three years of use - a ship changed ownership and name. Since using the new application I've found a couple more occasions where two different Ship documents were created under slightly different names. This is a risk of using a picker, and one we can improve on.</p> <p>The new approach will address both.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#logical-flow","title":"Logical Flow","text":"<p>The problem with the picker is that it allows only one way to find the required ship. In Notes Client development, the usual approach to address this problem is a pick list (<code>NotesUIWorkspace.PickListCollection()</code> or <code>NotesUIWorkspace.PickListStrings()</code>). But this still places the onus on the end user to find the match and select it correctly.</p> <p>Instead, we'll build a search. We created a JSON array of all ships when the user first logs in. The JSON array can be filtered to find the results.</p> <pre><code>flowchart TD\nA([Start]) --&gt; B(Get ship name and call sign entered)\nB --&gt; C{Call sign blank?}\nC -- Yes --&gt; D(Filter on call sign)\nD --&gt; F(Check results)\nC -- No --&gt; E(Filter on name)\nE --&gt; F\nF --&gt; G{Results?}\nG -- No --&gt; H(Message user: no ships matching criteria)\nH --&gt; Z([End])\nG -- Yes --&gt; J{Single match?}\nJ -- Yes --&gt; K(Populate ship)\nK --&gt; Z\nJ -- No --&gt; L(Show dialog of matches)\nL --&gt; Z</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#the-code","title":"The Code","text":"<p>The code for the first part is pretty straightforward:</p> <pre><code>this.shipOptions = [];\nconst shipName = this.querySelector(`#${this.prefix}ship-name`).value;\nconst pattern = new RegExp(shipName.toLowerCase());\nconst callSign = this.querySelector(`#${this.prefix}ship-call-sign`).value;\nconst results = this.ships.filter(obj =&gt; (callsign != \"\") ? obj.CallSign === callSign : pattern.test(obj.Ship.toLowerCase()));\nif (results.length === 0) {\n    this.dispatchEvent(new CustomEvent(\"sendUserMessage\", { bubbles: true, detail: {type: \"error\", message: \"No ships found matching criteria\"}}));\n} else {\n    if (results.length === 1) {\n        this.populateShip(results[0]);\n        this.setAttribute(\"search\", false);\n        this.setAttribute(\"actionbutton\", (this.prefix === \"\") ? \"Edit\" : \"Save\");\n    } else {\n        this.shipOptions = results;\n</code></pre> <p>First we clear the array of ships matching the criteria. Then we get the ship name and call sign.Then we filter the ships JSON array according to the logic in the diagram.</p> <p>We can match exactly on call sign - this is usually an alphanumeric string between 5 and 7 characters, so there's no point using regex. But the lower-cased ship name is passed into a RegExp test. Passing whatever is entered will match results anywhere in the ship name - which we also force to lower case.</p> <p>But what if the call sign is in the wrong case? Call signs are always alphanumeric with capital letters. We can fix that, by using the <code>autocapitalize</code> HTML attribute for the input, so <code>&lt;input autocapitalize=\"characters\"/&gt;</code>. It's not something I've used often in the past, and I don't remember it being obviously offered in XPages. But since using it here, it's surprisingly how often I've noticed web applications not using this approach where only capital letters are relevant.</p> <p>As elsewhere in the application, we use a <code>CustomEvent</code> to send the message to the user. If there's a single match, we assume that's correct, populate it and switch from search to read mode for the ship.</p> <p>Warning</p> <p>This choice to populate the ship has an impact when creating a spot. If the ship is not the one expected, we've now set the ship details - including the UNID of the ship chosen. If the user just changes all the details, I think this would change the \"selected\" ship. Instead the user is really intending to ignore the selected ship and create a new ship.</p> <p>There are a couple of ways around that. One is rely on the user to click a \"Reset\" button. Because the only user is me, I can live with that. A second option would be to prompt the user to accept the matching ship. I don't like that because mostly I know the ship already exists, I know I'm matching it correctly, so I don't want to have to click to confirm. The third would be to add a link / button for \"Wrong ship?\", which would also perform the reset. This avoids the extra click but also prompts the user to act if the wrong ship has been shown.</p> <p>The key here is to understand what's happened, understand the user base, and choose the best approach for the current application.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#the-dialog","title":"The Dialog","text":"<p>For the dialog, we're going to create a two-column layout, with links containing the ship and call-sign. The link will close the dialog and, using the relevant ship, perform what we did when there was an exact match - populate the ship and switch to read mode.</p> <pre><code>this.shipOptions = results;\nconst shipOptionsDialog = this.querySelector(`#${this.prefix}ships-options-dialog`);\nconst shipsColLeft = shipOptionsDialog.querySelector(`#${this.prefix}ships-dialog-col-left`);\nshipsColLeft.innerHTML = \"\";\nconst shipsColRight = shipOptionsDialog.querySelector(`#${this.prefix}ships-dialog-col-right`);\nshipsColRight.innerHTML = \"\";\nresults.forEach((ship, index) =&gt; {\n    const col = (index % 2 === 0) ? shipsColLeft : shipsColRight;\n    const a = document.createElement(\"a\");\n    a.href = \"#\";\n    a.innerHTML = ship.Ship + \" - \" + ship.CallSign;\n    a.addEventListener(\"click\", (event) =&gt; {\n        event.preventDefault();\n        this.populateShip(ship);\n        const dialog = event.target.closest(`#${this.prefix}ships-options-dialog`);\n        dialog.open = false;\n        dialog.close();\n        this.setAttribute(\"search\", false);\n        this.setAttribute(\"actionbutton\", (this.prefix === \"\") ? \"Edit\" : \"Save\");\n        this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: false}}));\n    });\n    col.append(a);\n});\nthis.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: true}}));\nshipOptionsDialog.open = true;\nshipOptionsDialog.show();\n</code></pre> <p>As we've seen with other areas, the code to populate the dialog may seem more verbose than, say, passing a collection to an XPages repeat control. But it's effectively the same. Using the index, we identify whether to put the collection in the left or right column. We then create a link with the text being the ship name and call sign. Then we add an event handler to close the dialog and populate the ship.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#html-dialogs","title":"HTML Dialogs","text":"<p>The HTML Dialog was first added in Chromium 37 in 2014. However, it only achieved widespread adoption in 2022 when Safari and Firefox adopted it. It's now standard in web development and massively simplifies web application development.</p> <p>However, there are a couple of key points.</p> <p>The first is the <code>open</code> property. If this is not set, the dialog will not be displayed. The code above could add and remove the attribute, as required. But setting it to true or false works and feels more similar to other paradigms in other languages.</p> <p>The second is <code>.show()</code> displays the dialog non-modally, <code>.showModal()</code> shows it modally. But in this application I'm using the \"mask\" Custom Event to prevent clicking anywhere else on the screen. So I don't need to use <code>.showModal()</code> - I've already achieved the same functionality with the mask.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#save","title":"Save","text":"<p>When we populate the ship, we store the relevant ship's JSON object as a component property. This means:</p> <ul> <li>we know if it's a new ship or an existing one, because we have the Domino metadata (UNID etc).</li> <li>we can copy the values to the input fields.</li> <li>we can modify it. For example, size is in the format \"length x breath\". But if we split that into \"length\" and \"breadth\", we can remove some data entry and make the fields numeric only.</li> <li>we can compare the values before save, to know if changes have been made.</li> </ul> <p>Obviously if we reset the form, we just reset the component property to an empty JSON object.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#dosave","title":"doSave()","text":"<p>The <code>doSave()</code> function is pretty simple:</p> <pre><code>doSave = () =&gt; {\n    this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: true}}));\n    if (!this.shipObj.hasOwnProperty(\"@meta\")) {\n        this.shipObj.Form = \"Ship\";\n    }\n    if (this.checkNoChange()) {\n        this.dispatchEvent(new CustomEvent(\"sendUserMessage\", { bubbles: true, detail: {type: \"error\", message: \"No change to ship, cancelling save\"}}));\n        this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: false}}));\n        return;\n    }\n    this.updateShipObj();\n    this.saveShip = true;\n    this.dispatchEvent(new CustomEvent(\"saveShip\", { bubbles: true, detail: {shipElem: this}}));\n}\n</code></pre> <p>If the <code>shipObj</code> has no metadata, it's a new ship, so we need to pass the Form to DRAPI. We then check if there's a change, update the JSON object and perform the save.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#update-ship-object","title":"Update Ship Object","text":"<p>The functions to check for a change and updating the JSON object are quite similar. One compare the object and values of the inputs, the other pushes the values of the inputs into the object:</p> <pre><code>checkNoChange() {\n    return this.shipObj.Ship === this.querySelector(`#${this.prefix}ship-name`).value &amp;&amp;\n    this.shipObj.CallSign === this.querySelector(`#${this.prefix}ship-call-sign`).value &amp;&amp;\n    this.shipObj.Type === this.querySelector(`#${this.prefix}ship-type`).value &amp;&amp;\n    this.shipObj.Line === this.querySelector(`#${this.prefix}ship-line`).value &amp;&amp;\n    this.shipObj.Flag === this.querySelector(`#${this.prefix}ship-flag`).value &amp;&amp;\n    this.shipObj.YearBuilt === this.querySelector(`#${this.prefix}ship-year-built`).value &amp;&amp;\n    this.shipObj.Size === this.querySelector(`#${this.prefix}ship-length`).value + \" x \"\n    + this.querySelector(`#${this.prefix}ship-breadth`).value;\n}\n\nupdateShipObj() {\n    this.shipObj.Ship = this.querySelector(`#${this.prefix}ship-name`).value;\n    this.shipObj.CallSign = this.querySelector(`#${this.prefix}ship-call-sign`).value;\n    this.shipObj.Type = this.querySelector(`#${this.prefix}ship-type`).value;\n    this.shipObj.Line = this.querySelector(`#${this.prefix}ship-line`).value;\n    this.shipObj.Flag = this.querySelector(`#${this.prefix}ship-flag`).value;\n    this.shipObj.YearBuilt = this.querySelector(`#${this.prefix}ship-year-built`).value;\n    this.shipObj.Size = this.querySelector(`#${this.prefix}ship-length`).value + \" x \"\n        + this.querySelector(`#${this.prefix}ship-breadth`).value;\n}\n</code></pre> <p>Again, the code is pretty simple.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#save-flow","title":"Save Flow","text":"<p>The save flow is where the code may seem more complex than other frameworks. But it's because of the hierarchy:</p> <pre><code>classDiagram\n    index.js &lt;-- ship\n    index.js &lt;-- dominoService\n    class index.js{\n        +saveShip\n        +saveSpotShipObj\n    }\n    class ship[\"scripts/ship.js\"]{\n        +JsonObject shipObj\n        +doSave()\n        +reset()\n        +populate()\n    }\n    class dominoService[\"scripts/services/dominoService\"]{\n        +saveDoc()\n    }</code></pre> <p>We've got a variety of variables, constants for functions in index.js and functions in other files. This seems complex and convoluted. So a sequence diagram helps clarify things. For ease of understanding, I'm omitting the scenario of mock data, and only showing the flow for data being written to Domino via Domino REST API.</p> <pre><code>sequenceDiagram\n    ship-&gt;&gt;index: CustomEvent \"saveShip\", pass web component\n    index-&gt;&gt;index: await saveSpotShipObj()\n    index-&gt;&gt;index: Check if ship name or call sign found\n    index-&gt;&gt;dominoService: await saveDoc(shipObj)\n    dominoService-&gt;&gt;dominoService: POST / PUT to DRAPI\n    dominoService-&gt;&gt;index: Return JSON document\n    index-&gt;&gt;index: Update localStorage\n    index-&gt;&gt;ship: Set actionbutton=\"Edit\" and call reset()</code></pre> <p>By passing the web component as the detail of the <code>saveShip</code> Custom Event, the code in index.js has access to the properties and methods of the web component. This makes a lot of the processing much easier, but will probably feel unusual to developers new to Web Components. Once you understand that this is possible, it brings a lot of power to the developer.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#wrap-up","title":"Wrap Up","text":"<p>This completes the main part of the ship form, although you'll notice we've skipped the actual HTML for the form. We'll come back to that later, and in the next part we'll see why.</p> <p>Note</p> <p>I did not deliberately delay this part of the series until after I had migrated my blog to MKDocs for Material. But because it's come now, I've been able to use Mermaid.js to create the flowchart, the class diagram and the sequence diagram. This certainly makes the blog post easier to understand. It also means if I've got anything wrong, the source code for the diagrams is in source control. So it's easy to see what's changed.</p> <p>Using a good tool and knowing its strengths makes more powerful functionality and a better user experience.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2024/12/14/framework-web-11/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/07/cspell/","title":"Visual Studio Code: Code Spell Checker","text":"<p>Whatever your IDE of choice, there are things you can do to enhance your development experience, improve your productivity and maximise quality. But when your repository contains not only code, but documentation and configuration documents as well, it can be challenging. Then it's a case of diving into the documentation and configuring to the max. This blog post covers some learning I've gained over the last week, specifically for configuring spell checking in Visual Studio Code.</p> <p>Code Spell Checker is a great tool for avoiding typos in your repository. It's a standard extension I've used for some time and one which has prevented mistakes in documentation, as well as typos in code itself.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#how-it-works","title":"How It Works","text":"<p>This extension reviews all opened files and adds \"info\" messages to the Problems view for any spelling problems. It also contributes a Spell Checker view with spelling issues. As well as cross-referencing words in various language and technical dictionaries, it also uses rules for camel case and snake case words.</p> <p>Admittedly, this may not suit some developers' preferences around variable names. If so, you are likely to get a lot of errors notified and it may create too much noise. But I'm happy with those rules for variables, method names and keywords.</p> <p>Even still, it has generated a lot of warnings for me. There are various options for configuration, but there's not a single \"right\" solution. So it makes sense to document my approach to configuration.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#ignoring-words","title":"Ignoring Words","text":"<p>There will always be certain words that are not in usual dictionaries. And it's no surprise that Code Spell Checker provides various ways to do this. The right approach may vary depending on your use case.</p> <p>If the words are specific to the project you're working on, and you're working in a team, a good approach is to add the words to the workspace. They can be added to the normal VS Code workspace settings. But it's important to bear in mind that this means adding the \".vscode/settings.json\" file to your repository, which may also include other settings you don't want. They can also be added to a custom directory in the workspace, but this also needs enabling in the settings.json for the repository.</p> <p>If you're a lone developer or the words are used across all your projects, adding them to your user settings may work for you. This adds them to your global VS Code settings.json, under a section called <code>cSpell.userWords</code>. AGain, you can also add them to a dictionary and load them from there.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#gitignore-challenges","title":"GitIgnore Challenges","text":"<p>In one project, I'm using the VS Code workspace settings.json. But the repository is also used for Mac users, using a devcontainer. This is typically also in .vscode directory of the repository. So we get a conflict: we want to include the settings.json from .vscode directory, but not other files. The good news is that's pretty straightforward:</p> <pre><code>.vscode/*\n!.vscode/settings.json\n</code></pre> <p>This includes the settings.json from that directory, but only that file.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#ignoring-files","title":"Ignoring Files","text":"<p>cSpell ignores certain files for specific languages, like <code>package.json</code>. But with VoltScript and MKDocs, we have files not in that list. These can be ignored in user settings again. The extensions settings has a section under \"Files, Folders, and Workspaces\" called Ignore Paths. When entered as a filename, e.g. <code>atlas.json</code> or <code>mkdocs.yml</code>, they are resolved relative to the workspace, which is easy to manage. But it means the whole team need to standardise on having this set up.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#checking-unopened-files","title":"Checking Unopened Files","text":"<p>It's important to bear in mind something I mentioned earlier, that it only looks at files opened during the current session. There is a command line Node.js tool by the same team who built the VS Code extension. But you will need to convert the VS Code extension settings for custom dictionaries. I've not tried that, but it looks like it can be done from the VS Code extension by using the Command Palette. This could be worth investigating if you want to check the whole repository but don't want to open all the files.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/07/cspell/#ymmv","title":"YMMV","text":"<p>Obviously as a team, you'll need to decide on the approach that works best for your use case. Having a dictionary in a separate central repository may also be an option. But the right approach will depend on the preferences of the developers involved.</p> <p>There are a host of additional configuration options that may or may not be relevant, so it's worth digging into the settings and the documentation.</p>","tags":["Dev Tools"]},{"location":"blog/2025/01/13/framework-web-12/","title":"XPages App to Web App: Part Twelve - Ship Spot Component","text":"<p>In the last two parts we covered the ship form, but we didn't cover the HTML of the form. We'll cover that in the next part, but first we'll cover creating the class. As we do that, we'll see one of the big benefits that Web Components brings to JavaScript application development. Because here we'll start with the Ship Spot form, which not only captures data that will create or display a Ship but also creates a Spot - Location, Port From and Port To.</p> <p>One of the powers of web components is that it gives you an HTML element that can be re-used across your application, with different configurations. And the initial approach I began with development of the Ship Spot form - adding another instance of the Ship form and adding a component for the Spot.</p> <p>But after fighting with a few aspects, I quickly realised the better approach.</p> <p>The Ship component is created as a JavaScript class. And in all languages classes can be extended. The right approach was the make the Spot class extend the Ship class instead of the usual HTMLElement class.</p> <p>The syntax is simple and intuitive. For the Ship, it was:</p> <pre><code>export default class Ship extends HTMLElement {\n</code></pre> <p>For the Spot, it's:</p> <pre><code>export default class Spot extends Ship {\n</code></pre> <p>This will be assigned to the HTML element name \"spot-elem\". This won't include a <code>&lt;ship-elem&gt;</code>. Instead it will include the HTML for the ship and additional HTML for spot fields.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/13/framework-web-12/#the-constructor","title":"The Constructor","text":"<p>The constructor of the Ship class did a few things:</p> <ol> <li>It created the component from the template. As a reminder, it did this so the component would exist before the <code>attributeChangedCallback</code> ran, so we could automatically call <code>updateButton()</code>.</li> <li>If <code>showspots</code> attribute is true, it created a <code>spots-elem</code> element.</li> <li>It added an eventHandler to the reset button.</li> <li>It added a dialog for ship lines.</li> <li>It added a dialog node for the ship options.</li> <li>It modified element IDs if the <code>prefix</code> attribute was set.</li> <li>It added event handlers to close the dialogs.</li> <li>It initialised arrays for ship types, lines and flags.</li> </ol> <p>As well as adding fields, the Ship Spot also needs to do a few other things:</p> <ol> <li>It calls the <code>super()</code> method to run the constructor from the parent class.</li> <li>It needs to add the prefix to the element IDs in the additional HTML.</li> <li>It needs to initialise an array for ports.</li> </ol> <p>We'll come back to the contents of the template and the dialogs in the next two parts.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/13/framework-web-12/#element-html","title":"Element HTML","text":"<p>The HTML for searching for a ship was this:</p> <pre><code>&lt;section class=\"ship\" id=\"ship-search\"&gt;\n    &lt;div class=\"actionDiv\"&gt;\n        &lt;span id=\"ship-search-action\" class=\"material-icons material-button md-32\" title=\"\"\n        role=\"img\" aria-label=\"\" style=\"float:right\"&gt;&lt;/span&gt;\n        &lt;span id=\"ship-search-back\" class=\"material-icons material-button md-32\" title=\"Back\"\n        role=\"img\" aria-label=\"Back\" style=\"float:right\"&gt;arrow_back&lt;/span&gt;\n        &lt;h1&gt;Find Ship&lt;/h1&gt;\n    &lt;/div&gt;\n    &lt;form id=\"search-ship-form\"&gt;\n        &lt;ship-elem actionbutton=\"Search\" actionid=\"#ship-search-action\"\n            class=\"grid-container\"&gt;&lt;/ship-elem&gt;\n    &lt;/form&gt;\n&lt;/section&gt;\n</code></pre> <p>Initially, the HTML for the Ship Spot is this:</p> <pre><code>&lt;section class=\"new-spot\" id=\"new-spot\"&gt;\n    &lt;div class=\"actionDiv\"&gt;\n        &lt;span id=\"spot-search-action\" class=\"material-icons material-button md-32\" title=\"Search\"\n        role=\"img\" aria-label=\"Search\" style=\"float:right\"&gt;search&lt;/span&gt;\n        &lt;span id=\"spot-action\" class=\"material-icons material-button md-32\" title=\"\"\n        role=\"img\" aria-label=\"\" style=\"float:right\"&gt;&lt;/span&gt;\n        &lt;span id=\"spot-back\" class=\"material-icons material-button md-32\" title=\"Back\"\n        role=\"img\" aria-label=\"Back\" style=\"float:right\"&gt;arrow_back&lt;/span&gt;\n        &lt;h1&gt;Spot&lt;/h1&gt;\n    &lt;/div&gt;\n    &lt;form id=\"ship-spot-ship-form\"&gt;\n        &lt;spot-elem actionbutton=\"Save\" searchid=\"#spot-search-action\" actionid=\"#spot-action\" \n        class=\"grid-container\" prefix=\"spot-\" showspots=\"false\"&gt;&lt;/spot-elem&gt;\n    &lt;/form&gt;\n&lt;/section&gt;\n</code></pre> <p>We have some additional attributes set for the Ship Spot:</p> <ul> <li>prefix, because we want to modify the HTML IDs to ensure they're unique.</li> <li>showspots is set to false because we don't want to show spots for the ship. This is the only spot we want to show.</li> <li>searchid, the ID of the action button for searching.</li> </ul> <p>On the ship form, searching was tied to the state of the \"form\" and the single button we re-used. If the user wanted to search, we made that button a search button; if a ship had been selected, we made it an edit button; if they were in edit mode, we made it a save button. But here, we want to search and save the spot. So we have a separate action button for searching.</p> <p>But then we can't use the <code>actionbutton</code> attribute to add the eventHandlers. Instead, we do this in the <code>render()</code> method.</p> <pre><code>const actionButton = document.querySelector(this.searchid);\nactionButton.addEventListener(\"click\", this.doSearch);\nconst shipNameInput = this.querySelector(`#${this.prefix}ship-name`);\nconst callSignInput = this.querySelector(`#${this.prefix}ship-call-sign`);\nshipNameInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\ncallSignInput.addEventListener(\"keydown\", this.checkEnterDoSearch);\n</code></pre> <p>Mapping the search button's click event to <code>this.doSearch</code> will call the <code>doSearch()</code> method in the parent Ship class.</p> <p>The eventHandler for saving the Spot will automatically get added in the Ship class by observing the <code>actionbutton</code> attribute, which is initialised to \"Save\". We just need to override the <code>doSave()</code> function in the Spot class to perform the specific functionality for saving a Ship Spot rather than just a Ship.</p> <p>Note</p> <p>In the source code, you'll see I've also overridden the <code>updateButton()</code> function. But it looks like the only difference is that the <code>switch</code> statement doesn't include \"Search\" as an option. So I don't think I actually needed to override <code>updateButton()</code>.</p> <p>But we also need to enable and disable additional fields. Again, this is pretty straightforward if we break it down into the steps:</p> <pre><code>enableDisable(disableType) {\n    super.enableDisable(disableType);\n    const disableFields = disableType === \"all\";\n    this.querySelector(`#${this.prefix}spot-location`).disabled = disableFields;\n    this.querySelector(`#${this.prefix}spot-port-from`).disabled = disableFields;\n    this.querySelector(`#${this.prefix}spot-port-to`).disabled = disableFields;\n}\n</code></pre> <p>We call the parent method. Here we're either disabling fields or not, depending on whether the disableType is \"all\". So we just set <code>disabled</code> to true or false.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/13/framework-web-12/#wrap-up","title":"Wrap Up","text":"<p>In the next part we'll cover the HTML for the Ship and Spot, as well as validation.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/13/framework-web-12/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Form</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/","title":"XPages App to Web App: Part Thirteen - HTML Layouts","text":"<p>We've covered a lot of the Ship and Ship Spot basic web component JavaScript. But now it's time to dig into the HTML. But first, it's important to recap the landing page web component.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#landing-page-flexbox","title":"Landing Page Flexbox","text":"<p>We covered the CSS for the landing page elements back in part 8. It was:</p> <pre><code>&lt;style&gt;\n    .landing-container {\n        display: flex;\n        flex-wrap: wrap;\n        align-items: stretch;\n        justify-content: center;\n    }\n    .landing-tile {\n        margin: 5px;\n        font-weight: bold;\n        font-size: 30px;\n        color: light-dark(var(--primary-color-dark), var(--primary-color));\n        background-image: radial-gradient(circle at center, light-dark(var(--landing-tile-start),var(--landing-tile-start-dark)) 15%, light-dark(var(--landing-tile-end),var(--landing-tile-end-dark)) 100%);\n        height: 200px;\n        width: 200px;\n        box-shadow: inset 0 0 2px 2px light-dark(var(--border-color-primary),var(--border-color-primary-dark));\n        border-radius: 10px;\n        flex-grow: 1;\n        text-align: center;\n        align-content: center;\n        cursor: pointer;\n    }\n&lt;/style&gt;\n</code></pre> <p>The key parts here are lines 3-6 and line 18. This uses CSS Flexbox to give a responsive layout. The <code>display</code> CSS property has been around for a long time and long-term web developers are very familiar with the values \"hidden\", \"inline\" and \"block\". But here the <code>landing-container</code> class, which is used for the div that contains the landing tiles, has <code>display=\"flex\"</code>. This makes it a flex container.</p> <p>We can then control various settings by applying additional CSS properties to the container. By default, the contents are displayed horizontally left-to-right, so we don't need to set flex-direction. When the tiles take up the full width, we want them to continue onto the next row, so we set <code>flex-wrap=\"wrap\"</code>. There is also a flex-flow property that allows us to set both flex-direction and flex-wrap in a single property.</p> <p>Then there are settings to control the content. justify-content allows us to control whether the items are left-justified (flex-start), centered (as we do here), or right-justified (flex-end). There are also other values that can be used to space the elements out. But what happens if they don't take up the full height? align-items controls this. But we don't explicitly define a height, so this isn't really used. There is also align-content, which controls the alignment of each row, whereas align-items controls how each item within that row is handled.</p> <p>For the items themselves - each tile on the landing page - the key property is <code>flex-grow=\"1\"</code>. This controls the relative size of each box, although we also control that with height and width. flex-basis could be used for the width, but would only give an initial width, which might not be what we want. On our landing page the content on each tile is always quite short with no risk of overflow, so this makes it a fairly simple layout. But flexbox provides a lot of flexibility for a one-dimensional layout.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#ship-and-spot-element-grid-layouts","title":"Ship and Spot Element Grid Layouts","text":"<p>When we come to the Ship and Ship Spot web components, flexbox doesn't provide everything we need. We want more of a form layout. This is where CSS Grid comes in. This allows us to control both rows and columns.</p> <p>CSS Grid is extremely powerful and flexible. It also allows you to make the layout responsive. But browser tools make it easier to work with as well. My preferred browser for development is Chrome. When you inspect an element in developer tools in Chrome, if an element uses CSS Grid you see a button that says \"grid\" beside the HTML tag. Clicking this allows you to inspect the grid in the browser, which can help identify layout issues.</p> <p></p> <p>The screenshot above shows the grid I'm using for the Ship Spot. Similarly to Flexbox, a div uses CSS Grid if it has <code>display: grid</code>. You'll see the second setting is grid-template-columns. This defines the number of columns the grid should have and, should you wish to define a number of rows, there is a corresponding grid-template-rows. The third setting is column-gap, which defines the size of the gap between the columns. There is a corresponding row-gap property to define the gap between rows and a gap property to set row and column gaps in a single property.</p> <p>As with Flexbox, there are align-content, align-items and justify-content properties. But there is also place-content, which can be used to set both align-content and justify-content at the same time. The values you can use are different to Flexbox, but they do the same kind of thing - position within the grid.</p> <p>But we need to come back to the grid-template-columns value. The value can be defined with a basic syntax, defining a fixed number of columns and sizing, e.g. <code>grid-template-columns: 80px 200px auto 40px</code>. This would define four columns, each of which will also be defined as a <code>div</code> HTML element: columns 1, 2, and 4 will have a fixed size, but column 3 will take up the remaining space. You don't specify the widths on the divs themselves, the grid layout automatically handles that. And if you add more than four divs, the grid layout will automatically wrap the subsequent divs onto new rows. And the same syntax can be used for grid-template-rows.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#responsive","title":"Responsive","text":"<p>All this is great if you want a fixed layout. But what if the size of the browser window changes?</p> <p>This is where CSS functions come in, specifically repeat() and minmax(). Here the setting is <code>repeat(auto-fit, minmax(500px, 1fr))</code>. But I also have a media query:</p> <pre><code>@media only screen and (max-width: 768px) {\n    .grid-container {\n        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n    }\n}\n</code></pre> <p>repeat() takes two arguments, the number of times to repeat and what to repeat.</p> <p>For the number of times, we could hard-code the number of times, but this wouldn't make it responsive. There are two keywords that can be used for responsive auto-fit and auto-fill. I would recommend reading this blog post on css-tricks website to properly understand the difference. For this layout, auto-fit is what I want.</p> <p>For what to repeat, we use <code>minmax()</code>, which takes a minimum size and a maximum size. The minimum size is either 500 pixels on a reasonably-sized desktop browser or a mobile device in landscape mode. On a mobile device in portrait, it's 300 pixels. The maximum is <code>1fr</code>, which will probably be unfamiliar. fr means \"flex factor\", a proportion of the remaining space. What this all means is that it will try to fit as many columns as possible into the width available, but will never make a column less than 500px, or less than 300px if the max-width of the screen is 768px (a mobile phone in landscape).</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#grids-within-grids","title":"Grids Within Grids","text":"<p>Now let's use developer tools to display all the grids:</p> <p></p> <p>The salmon-coloured numbers show the grid we've been talking about until now, with four cells. The numbers (1, 2, 3) show the start and end of a grid element, which can be used to map placement. But you'll notice there are also negative numbers on the right-hand size of each column and bottom of each row, which can be used to map placement working backwards from the end of the row or column.</p> <p>But you'll notice there are four other grids with different coloured numbers. If you look at the HTML, these are the four divs inside the Ship Spot (<code>&lt;spot-elem&gt;</code>). The first two (and, incidentally, the two dialogs that follow them) are from the Ship class. The last two are from the Spot class. All of these have the class <code>input-group</code>, which is also a grid. This has a simpler layout.</p> <pre><code>input-group {\n    display: grid;\n    grid-template-rows: auto;\n    align-content: start;\n    row-gap: 5px;\n    margin-top: 5px;\n}\n</code></pre> <p><code>grid-template-rows</code> sets that rows have an automatic height, which works fine because we're using inputs. We set a gap between rows at 5px, but also add a top margin of 5px to ensure it looks like there's a row gap between each div. We also set <code>align-content: start</code>. This becomes useful in the right cell of row 1 and left cell of row 2. It means the inputs there are aligned to the start of the grid cell even though they have less content.</p> <p>But why lay it out like this? It becomes clearer when we reduce the browser window size.</p> <p></p> <p>Compare the two and you now see we get the ship fields first and the spot fields afterwards. But because each inner grid is its own \"row\" now, <code>align-content</code> is effectively irrelevant now.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#testing-alignment","title":"Testing Alignment","text":"<p>If you look at the CSS with <code>display: grid</code>, you'll see an icon displayed after the semi-colon. This isn't in the CSS file, it's added by developer tools and, like the \"grid\" button next the the HTML elements, it's also clickable. It allows you see and change options for align-content, align-items, justify-content, and justify-items. It also gives little images to help understand the differences. It's another example of the browser tools helping.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#subgrid","title":"Subgrid","text":"<p>In my layout, I'm just using input fields with placeholders to identify what they should enter. The Line row is the only one that has any other content, a link to see the lines previously entered. This is a pretty simple layout, and the approach, which doesn't necessarily cover best practices for full accessibility coverage, is one that is both acceptable and preferable for the user base (i.e. me!).</p> <p>But if you need more complex layouts, you should look into Subgrid, which gives a lot of power for controlling the layout of inner grids and passing settings down.</p> <p>But that's beyond the scope of this blog series and this application.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#summary","title":"Summary","text":"<p>For those who have been using tables for the last 20+ years, you may start to see a better option for form and table layouts. There is immense power and flexibility in both Flexbox and CSS Grid, and it may take some playing about with to find the right settings you want. But it's definitely the way forward. In the next part we'll cover some validation and field setting options.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/01/18/framework-web-13/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/","title":"XPages App to Web App: Part Fourteen - Fields and Save","text":"<p>In the last part I covered CSS Grid and its use for the layout of the Ship Spot form. In this part I'm going to cover the additional form functionality and save functionality.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#field-settings","title":"Field Settings","text":"<p>When building the original XPages application, I dragged and dropped fields onto the XPage and thought little about the settings. As a result, I created what on reflection was a suboptimal user experience. When redeveloping the application, I had the experience of having used the application for several years and experience of the data I was dealing with. So I put in a little more effort and investigation to ensure quicker, easier and more accurate data entry.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#date-built-and-ship-size","title":"Date Built and Ship Size","text":"<p>The date the ship built is already being validated in DRAPI to ensure it's a four-digit year after 1800. So we know it needs to be numeric. We also split the size into two numeric fields that will be combined at save. So it makes sense to enforce this on the browser and ensure a numeric keyboard is displayed for the user.</p> <p>Not surprisingly, this is very easy: <code>&lt;input type=\"number\" inputmode=\"numeric\"</code>. The <code>inputmode</code> attribute here ensures the correct keyboard is displayed.</p> <p>For the size, this can be numeric and decimal, to two decimal places. The <code>step</code> attribute can be used to define legal values, in this case we add <code>step=\"0.01\"</code>.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#ship-call-settings","title":"Ship Call Settings","text":"<p>The ship call sign is alphanumeric, but the alpha characters are always upper case. Again, we can make this easier for the user. Adding the attribute setting <code>&lt;input type=\"text\" autocapitalize=\"characters\"</code> ensures the keyboard automatically displays only with upper case letters on mobile devices.</p> <p>When it comes to the Country dialog, the country code is also upper case, so we do the same there.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#summary","title":"Summary","text":"<p>After implementing these usability enhancements and using them for a short period of time, it's surprising and disappointing to encounter websites on mobile that don't take the time to adapt their user experiences.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#validation","title":"Validation","text":"<p>We set up a lot of validation rules in our DRAPI schema. This was one of the driving principles for me when working on Domino REST API in its research days. That's because there's a big difference when moving from XPages to a REST-based web access. If access is via REST services, the web application is just one interface for that REST service. It may be the one the developer creates and the one the developer focuses on. But it's never the only interface, unless access to the REST service is locked down in some way at the server. Domino REST API provides ways to do that by restricting access to a schema to only server-based applications. But where that is not the case, validation needs to be enforced at the REST API.</p> <p>But there are errors that can and should also be caught in the web application. This is less about ensuring data integrity, but more about notifying the user as quickly as possible about errors they might have made.</p>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#required-fields","title":"Required Fields","text":"<p>The first step is ensuring all required fields have been completed. To handle this, we add an array of required fields as a property to the Spot class:</p> <pre><code>requiredFields = [\"spot-location\",\"ship-name\",\"ship-call-sign\",\"ship-type\",\"ship-flag\"]\n</code></pre> <p>And we add an <code>isValid()</code> function to iterate the array and apply a class if they are empty.</p> <p><code>`` js linenums=1 isValid() {     let uncompletedFields = [];     this.requiredFields.forEach(field =&gt; {         const fieldName =</code>#${this.prefix}${field}`;         const input = this.querySelector(fieldName);         input.classList.remove(\"required\");         if (input.value === \"\") {             uncompletedFields.push(field);             input.classList.add(\"required\");         }     });     return uncompletedFields.length === 0; } <pre><code>Line 2 creates an empty array to hold uncompleted fields. We then iterate each of the required fields and get the relevant HTML input, not forgetting that we added a prefix to ensure they were always unique. At line 6 we remove the required class, in case a previous call to the method resulted in failed validation. On lines 7 - 10 we check the field was completed and, if not, add it to the array and add the required class. Finally, we return a boolean for whether or not there are uncompleted fields. This decides whether or not we should progress saving.\n\n### Save Functionality\n\nThe Ship class had a `doSave()` function and we override that function.\n\n``` js\ndoSave = () =&gt; {\n    this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: true}}));\n    if (!this.isValid()) {\n        this.dispatchEvent(new CustomEvent(\"sendUserMessage\", { bubbles: true, detail: {type: \"error\", message: \"The fields marked are required\"}}));\n        this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: false}}));\n        return;\n    }\n    if (!this.shipObj.hasOwnProperty(\"@meta\")) {\n        this.shipObj.Form = \"Ship\";\n    }\n    this.checkNoChange() ? this.saveShip = false : this.saveShip = true;\n    this.updateShipObj();\n    if (!this.spotObj.hasOwnProperty(\"@meta\")) {\n        this.spotObj.Form = \"Spot\";\n    }\n    this.checkNoSpotChange() ? this.saveSpot = false : this.saveSpot = true;\n    // Update JSON object\n    this.spotObj.Location = this.querySelector(`#${this.prefix}spot-location`).value;\n    this.spotObj.PortFrom = this.querySelector(`#${this.prefix}spot-port-from`).value;\n    this.spotObj.PortTo = this.querySelector(`#${this.prefix}spot-port-to`).value;\n    this.dispatchEvent(new CustomEvent(\"saveShipSpot\", { bubbles: true, detail: {spotElem: this}}));\n}\n</code></pre></p> <p>We've already covered the <code>isValid()</code> check which, if it fails, we notify the user and clear the mask using the now familiar approach of CustomEvents. The next part is very similar to the code in the base method in the Ship class: if the <code>shipObj</code> has no metadata, if it's a new ship, we need to pass the Form to DRAPI. But the next part is slightly different: we still check if there's a change, but this time we just update the <code>saveShip</code> property and call <code>updateShipObj()</code> to pass values from the inputs to the JSON object.</p> <p>Then we perform additional processing on the <code>spotObj</code> - checking whether there's a change and updating the <code>spotObj</code>. Finally we trigger the CustomEvent <code>saveShipSpot</code>, which calls <code>saveShipSpotObj()</code>.</p> <p>Back in part 11 we covered the save flow. But we've now added the Spot class to that, so let's expand that diagram further.</p> <pre><code>classDiagram\n    index.js &lt;-- ship\n    index.js &lt;-- spot\n    ship &lt;|-- spot\n    index.js &lt;-- dominoService\n    class index.js{\n        +saveShip\n        +saveSpotShipObj\n    }\n    class ship[\"scripts/ship.js\"]{\n        +JsonObject shipObj\n        +doSave()\n        +checkNoChange()\n        +reset()\n        +populate()\n    }\n    class spot[\"scripts/spot.js\"] {\n        +JsonObject spotObj\n        +isValid()\n        +doSave()\n        +checkNoSpotChange()\n        +reset()\n        +populateSpot()\n    }\n    class dominoService[\"scripts/services/dominoService\"]{\n        +saveDoc()\n    }</code></pre> <p>We're also adding to the business logic in <code>saveSpotShipObj()</code>, so let's also expand on the sequence diagram from the save flow in part 11. The logic is the same as it was before, the only difference is that the object being passed includes a boolean property <code>saveSpot</code>.</p> <pre><code>sequenceDiagram\n    spot-&gt;&gt;index: CustomEvent \"saveShipSpot\",&lt;br/&gt;pass web component\n    index-&gt;&gt;index: await saveSpotShipObj()\n    index-&gt;&gt;index: Check if ship name or call sign found&lt;br/&gt;only if shipSpotObj.saveShip\n    alt if shipSpotObj.saveShip\n      index-&gt;&gt;dominoService: await saveDoc(shipObj)\n      dominoService-&gt;&gt;dominoService: POST / PUT to DRAPI\n      dominoService-&gt;&gt;index: Return JSON document\n      index-&gt;&gt;index: Update localStorage\n      opt if shipSpotObj.saveSpot\n        index-&gt;&gt;dominoService: await saveDoc(spotObj)\n        dominoService-&gt;&gt;dominoService: POST / PUT to DRAPI\n        dominoService-&gt;&gt;index: Return JSON document\n      end\n    else\n      opt if shipSpotObj.saveSpot\n        index-&gt;&gt;dominoService: await saveDoc(spotObj)\n        dominoService-&gt;&gt;dominoService: POST / PUT to DRAPI\n        dominoService-&gt;&gt;index: Return JSON document\n      end\n    end\n    index-&gt;&gt;spot: Set actionbutton=\"Save\" and call reset()</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/07/framework-web-14/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript","Web Components"]},{"location":"blog/2025/02/08/framework-web-15/","title":"XPages App to Web App: Part Fifteen - Dialogs","text":"<p>When it comes to creating Ship Spots, one of the pain points I highlighted with the previous application was when a Ship Spot required creating a new Port or a new Country. This required switching to an \"admin\" area to open a Port form to complete and save; and if the country hasn't been created, it requires additionally switching to a County form to complete and save, before returning to the Port and back to the Ship Spot. Options were cached server-side in <code>viewScope</code>, so launching additional browser windows wasn't an option - the page would still need to be refreshed and entered data lost. We can improve on this.</p> <p>Note</p> <p>It should be stressed that XPages as a framework does not hinder creating multiple types of document from an XPage. However, using the Extension Library dialog component, updating server-side scopes, and updating multiple other areas of the page without losing values is far from straightforward and has been asked many times on forums. Options have to be part of the server-side design, updating them just client-side probably would not work.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/08/framework-web-15/#dialogs-redux","title":"Dialogs Redux","text":"<p>We covered one dialog in part 11. HTML dialogs are now standard in web development. Just to cover a couple of points on usage again: the <code>open</code> property needs to be set to <code>true</code>, and the show()` method is called to open it, because we're adding a mask to prevent clicking elsewhere on the screen.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/08/framework-web-15/#web-component-dialogs","title":"Web Component Dialogs","text":"<p>When we created previous dialogs, the template and the HTML were part of <code>ship.js</code>. But this time we're creating them as web components. As with previous web components, the constructor does little, <code>connectedCallback()</code> calls <code>render()</code>, and the <code>render()</code> method appends the template and adds the event listeners - a Close button and a Save button. The web components also include <code>show()</code> and <code>close()</code> methods, as well as <code>reset()</code> methods to clear the values.</p> <p>With the Ship and Spot web components, we also had objects that correspond to the document in the Domino database. But this time we're not going to bother. This is because there's no intention to edit the Ports or Countries, so we'll just pass the values from the relevant inputs on-the-fly. The structure of the classes is as below:</p> <pre><code>classDiagram\n    index.js &lt;-- Country\n    index.js &lt;-- Port\n    index.js &lt;-- dominoService\n    class index.js {\n        saveCountry()\n        savePort()\n    }\n    class Country[\"scripts/country.js\"] {\n        constructor\n        connectedCallback()\n        render()\n        show()\n        close()\n        reset()\n        addEventListeners()\n    }\n    class Port[\"scripts/port.js\"] {\n        countries : array\n        constructor()\n        connectedCallback()\n        render()\n        show()\n        close()\n        reset()\n        addEventListeners()\n        getAbbreviation()\n    }\n    class dominoService[\"scripts/services/dominoService\"]{\n        +saveDoc()\n    }</code></pre> <p>Notice that we have a <code>countries</code> property in the port. This contains the options available and also allows us to update it when a new country is saved. This is not just an array of country names, it's actually the array of country JSON objects stored in sessionStorage. This allows dual functionality. The setter can load options into a <code>&lt;select&gt;</code>:</p> <pre><code>set countries(value) {\n    this._countries = value;\n    const countrySelect = this.querySelector(\"#port-country\");\n    countrySelect.options.length = 1;\n    value.forEach((value) =&gt; {\n        const option = document.createElement(\"option\");\n        option.value = value.Country;\n        option.innerHTML = value.Country;\n        countrySelect.append(option);\n    });\n}\n</code></pre> <p>But also the <code>getAbbreviation()</code> method can retrieve the correct abbreviation for the selected country:</p> <pre><code>getAbbreviation = (country) =&gt; {\n    const results = this._countries.filter(obj =&gt; obj.Country === country);\n    return results[0].Abbreviation;\n}\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/08/framework-web-15/#loading-the-dialogs","title":"Loading the Dialogs","text":"<p>The Port dialog is only needed from a Spot. But the Country dialog is needed from a Ship and a Spot. This is why we don't add it as part of the ship class - that would create two dialogs instead of just one. Instead of adding it into a specific web component, we add it onto the main web page. The code to add it is in the <code>bootstrap()</code> function which, if you remember, runs when the application has initially loaded:</p> <pre><code>const countryElem = document.createElement(\"country-elem\");\ndocument.querySelector('#header').append(countryElem);\ncaptureClickEvent(\"ship-country-action\", function() {\n    countryElem.show();\n})\ncaptureClickEvent(\"spot-country-action\", function() {\n    countryElem.show();\n})\nconst portElem = document.createElement(\"port-elem\");\ndocument.querySelector(\"#header\").append(portElem);\ncaptureClickEvent(\"spot-port-action\", function() {\n    portElem.show();\n})\n</code></pre> <p>We programmatically create an instance of the country web component, which is defined with the HTML tag <code>country-elem</code>, and add it to the header. We add a click event to the \"Country\" button in both the ship and spot sections to show the country dialog. And we programmatically create an instance of the spot web component, which is defined with the HTML tag <code>port-elem</code> and also add it to the header. And we add a click event to the \"Port\" button in the spot section to show the port dialog.</p> <p>Note</p> <p>For this application I chose a dialog for creating Ports and Countries. Dialogs may not be everyone's choice for this functionality on a mobile device. But as always, it's important to take into account the users (and potential users) of an application. In this case, it's just me and only ever likely to be me.</p> <p>An alternative might have been to have a link to open a section with a wipe animation for creating a Port and/or Country. This could probably be injected below the relevant field and removed after completion. Another thing I've learned from years of development is not to over-complicate for the sake of it. After months of use of the current application, the speed of use is very satisfactory - if a Country doesn't exist (not a common experience now), one click to close the dialog, one click to launch another dialog, fill in two fields, save, then go back to the first dialog. And because it's all client-side, no entered data has been lost. Would the alternative approach have been quicker to use? Possibly, marginally, but slower to implement, and performance is never just about use. Would it give a better user experience? Maybe.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/08/framework-web-15/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/","title":"XPages App to Web App: Part Sixteen: Spots","text":"<p>Now that we've set up the CRUD pages for ships and spots, it's time to put them together. When we search a ship, we'll want to be able to see the spots created for that ship. Now let's set that up.</p> <p>We laid the groundwork for this back in part ten when we added a <code>showspots</code> attribute on the Ship component. This allowed us to determine whether to show spots (when looking at just a ship) or not (when looking at or editing a ship spot).</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#ship-form-amendments","title":"Ship Form Amendments","text":"<p>In the constructor, we add this code:</p> <pre><code>if (this.showspots === \"true\") {\n    const spotsElem = document.createElement(\"spots-elem\");\n    spotsElem.style = \"grid-column: 1 / -1;\"\n    this.appendChild(spotsElem);\n}\n</code></pre> <p>This adds a <code>spots-elem</code> HTML element (no prizes for guessing this will be a web component we'll create), and set it to show full width in the table by making it span from the first grid-column to the last (-1).</p> <p>This handles the display. But when loading the spots we need to consider the lifecycle of the ship form.</p> <pre><code>flowchart RL\nA([Find Ship]) --&gt; B([Search])\nB --&gt; C([Select Ship])\nC --&gt; D([Load Ship])\nD --&gt; E([Load Spots])\nE --&gt; F{Search Again?}\nF -- Yes --&gt; B\nF -- No --&gt; G([Reset])\nG --&gt; H([Clear Spots])</code></pre> <p>Both search and reset call the <code>populateShip</code> function to load the ship details. So we can extend this function to clear them. The <code>populateShip</code> function takes a JSON object that is the selected ship, or null.</p> <pre><code>populateShip(shipObj) {\n    if (shipObj) {\n        ...\n        if (this.showspots) {\n            this.dispatchEvent(new CustomEvent(\"loadSpots\", { bubbles: true, detail: {shipElem: this, shipunid: shipObj[\"@meta\"].unid}}));\n        }\n    } else {\n        ...\n        if (this.showspots) {\n            this.dispatchEvent(new CustomEvent(\"clearSpots\", { bubbles: true, detail: {shipElem: this}}));\n        }\n    }\n}\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#loading-spots","title":"Loading Spots","text":"<p>The two functions corresponding to the custom events are pretty similar, so I'll show the code for both:</p> <pre><code>const loadSpots = async (event) =&gt; {\n    const shipElem = event.detail.shipElem;\n    const spotsObjs = shipElem.getElementsByTagName(\"spots-elem\");\n    if (spotsObjs.length === 1) {\n        const spotsObj = spotsObjs[0];\n        const json = await window.dataService.getSpotsForShip(event.detail.shipunid);\n        spotsObj.populateSpots(json);\n    }\n}\n\nconst clearSpots = (event) =&gt; {\n    const shipElem = event.detail.shipElem;\n    const spotsObjs = shipElem.getElementsByTagName(\"spots-elem\");\n    if (spotsObjs.length === 1) {\n        const spotsObj = spotsObjs[0];\n        spotsObj.populateSpots(\"\");\n    }\n}\n</code></pre> <p>In both cases the web component for the ship was passed, which allows us to query its inner HTML to find the corresponding <code>spots-elem</code> HTML element added in the constructor. It should exist and there should only ever be one, but we do the check anyway. If we're loading spots, we do an async call for spots for the ship, which was also passed into the event, which will return an array containing 0..n elements. For clearing spots, we pass an empty string.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#spots-web-component","title":"Spots Web Component","text":"<p>Let's just quickly cover the Spots web component, which is in a script file called spotsObj.js. The basics for the element are quite basic:</p> <pre><code>export default class Spots extends HTMLElement {\n    /**\n     * Construct and render\n     */\n\n    dateOptions = {\n        dateStyle: 'full',\n        timeStyle: 'long',\n    };\n\n    constructor() {\n        super();\n        this.root = this.attachShadow({ mode: \"closed\" });\n        this.connected = false;\n    }\n\n    connectedCallback() {\n        console.log(\"landing connected\");\n        this.render();\n        this.connected = true;\n    }\n\n    render() {\n        console.log(\"Loading spots\");\n        const clone = template.content.cloneNode(true);\n        this.root.append(clone);\n    }\n\n}\n</code></pre> <p>You should be familiar with this by now and there are just two things to comment upon. The first is we use a shadow DOM set to closed. This prevents JavaScript outside the web component from accessing the HTML nodes within the shadow root: we don't need to, we'll be manipulating the HTML with functions of the component itself. The second point of note is a JSON object <code>dateOptions</code>, which we'll come back to.</p> <p>The HTML template for the component is:</p> <pre><code>const template = document.createElement(\"template\");\ntemplate.innerHTML = `\n    &lt;style&gt;\n        .spots-container {\n            margin-top: 5px;\n            display: grid;\n            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));\n            column-gap: 5px;\n            row-gap: 5px;\n        }\n        .spotsCell {\n            font-size: 16px;\n            padding: 5px;\n            flex-grow: 1;\n            border: 1px solid var(--border-color-primary);\n            border-radius: 5px;\n            box-shadow: 1px 2px var(--border-color-primary);\n        }\n    &lt;/style&gt;\n    &lt;div id=\"spots-container\" class=\"spots-container\"&gt;\n    &lt;/div&gt;\n`;\n</code></pre> <p>This is some styling and a container div, which will display with CSS Grid, 5 pixels gap between the cells and auto-filling the space with a minimum of 200px width. Each \"cell\" in the grid will be the same size (<code>flex-grow: 1</code>) with some colours loaded from the external CSS. We don't have a template for the contents of a cell, they will be created programmatically.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#populating-spots","title":"Populating Spots","text":"<p>The web component also needs a <code>populateSpots()</code> function that is called from the index.js functions:</p> <pre><code>populateSpots(value) {\n    const spots_container = this.root.getElementById(\"spots-container\");\n    spots_container.innerHTML = \"\";\n    if (!value || value.length === 0) {\n        const p = document.createElement(\"p\");\n        p.innerHTML = \"No spots found\";\n        spots_container.appendChild(p)\n    } else {\n        ...\n    }\n}\n</code></pre> <p>We'll come back to the contents of the else block shortly. We start by clearing any HTML previously in the <code>spots-container</code> div. Then, if a blank string or empty array was passed, we add a message saying \"No spots found\". Otherwise it will be an array of JSON objects like:</p> <pre><code>{\n    \"ShipUNID\": \"BAD429566D90A8AB86258751007006F4\",\n    \"Created\": \"2024-08-10T06:20:36-06:00\",\n    \"Location\": \"Southampton\",\n    \"PortFrom\": \"Southampton\",\n    \"PortTo\": \"Cowes\"\n}\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#date-handling","title":"Date Handling","text":"<p>When it comes to dates, Moment.js has become a go-to library to load and manipulate dates. However, for many applications it may be overkill and Moment.js themselves highlight this, stating \"Moment.js was built for the previous era of the JavaScript ecosystem\", noting that Chrome Dev Tools shows recommendations for replacing Moment.js and themselves pointing to a number of posts about alternatives. In our case, we just want to format an ISO date, and JavaScript has a spec for this, ECMA 402. For dates, the part we need is Intl.DateTimeFormat, which has been baseline since 2017.</p> <p>The code we'll use is simple:</p> <pre><code>const localDate = Intl.DateTimeFormat(\"en-GB\", this.dateOptions).format(spot.Created);\n</code></pre> <p>The constructor for <code>Intl.DateTimeFormat()</code> can take up to two arguments, a locale and options. As I've said on numerous occasions, I'm the only user of this application which means \"en-GB\" locale is the only one I'm interested in. The other argument is the web component's <code>dateOptions</code> property we added when we created the web component, a JSON object containing two properties: <code>dateStyle=\"full\"</code> and <code>timeStyle=\"long\"</code>. This returns a value like \"Wednesday 27 November 2024 at 15:41:08 GMT\". There are a wide variety of options available.</p> <p>Note</p> <p>If you need a variety of locales, you'll need to do research on how to get the \"right\" locale. To give you an idea of the complexity involved in a \"default\" locale, I recommend looking at a few GitHub issues from the discussions about the specification:</p> <ul> <li>https://github.com/tc39/proposal-intl-locale/issues/15</li> <li>https://github.com/tc39/proposal-intl-locale/issues/84</li> <li>https://github.com/tc39/ecma402/issues/68</li> <li>https://github.com/tc39/ecma402/issues/883</li> </ul>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#loading-the-spot-html","title":"Loading the Spot HTML","text":"<p>The else block is:</p> <pre><code>value.forEach((spot, index) =&gt; {\n    const div = document.createElement(\"div\");\n    div.id = `spot-div-${index}`;\n    div.classList.add(\"spotsCell\");\n    spots_container.appendChild(div);\n    const createdDate = new Date(spot.Created);\n    const localDate = Intl.DateTimeFormat(\"en-GB\", this.dateOptions).format(createdDate);\n    const created = document.createElement(\"p\");\n    created.id = `spots-created-${index}`;\n    created.innerHTML = `Created: ${localDate}`;\n    div.appendChild(created);\n    const details = document.createElement(\"p\");\n    this.createDetails(details, index, spot)\n    div.appendChild(details);\n});\n</code></pre> <p>We loop through the spots and, for each, create a div with a unique ID. In that div, we add a paragraph for the created date/time and the details. The details varies depending if port details are known, so for simplicity I moved it into its own function:</p> <pre><code>createDetails(details, index, spot) {\n    details.id = `spots-details-${index}`;\n    if (spot.PortFrom != \"\" &amp;&amp; spot.PortTo != \"\") {\n        details.innerHTML = `From ${spot.PortFrom} to ${spot.PortTo}`;\n    } else if (spot.PortFrom != \"\") {\n        details.innerHTML = `From ${spot.PortFrom}, destination port not known`;\n    } else if (spot.PortTo != \"\") {\n        details.innerHTML = `Going to ${spot.PortTo}, origin port not known`;\n    }\n}\n</code></pre> <p>If we have both from and to ports, we list them. If we are missing from or to port, we give an appropriate message.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/02/11/framework-web-16/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/03/14/web-components-ember/","title":"Lessons Learned from Including Web Components in an Ember.js Application","text":"<p>One of the key skills when working in IT research is the ability to work out how something works, either by looking at the code or being able to see the process behind a button or API call on your screen. The inevitable consequence then is that you understand how things work, you see comparisons between technologies or frameworks, and you begin to identify whether or why something will work or fail. You also gain understanding of various possible approaches and which is the right one. That results in a \"lessons learned\" blog post like this one.</p> <p>Over the past few months I've been doing some work on-and-off on the HCL DIgital Solutions Community. But obviously this has included some development, with associated learning. As part of that, I had to dynamically add repeated chunks of HTML to part of a page. It's a good use case for Web Components, as I've talked about in my recent tutorial series. For XPages developers, this use case is like a chunk of XML markup inside a Repeat Control's tag. Think of moving that markup to a Custom Control that's so self-contained you can copy it into any database, and you get an idea of their power.</p> <p>But the framework I was integrating into was Ember.js. A quick search led me to this discussion on using Web Components in Ember.js. But this was research, and I'm able to quickly create a web component, I went that route.</p> <p>First, a bit of background on Ember.js and this application. This is what I've learned and inferred, so if it's not canonically correct, forgive me. It's a Single Page Application, which inserts components on the web page. Modern components in EmberJS use GlimmerJS. So the component is a JavaScript class which, like React, can define the HTML to insert or can map to a Handlebars template. In the HTML, a <code>did-insert</code> attribute is used to map to a JavaScript function in the component whenever it's inserted into the UI. The JavaScript Glimmer component's class has properties, and adding the <code>@tracked</code> annotation to the properties ensures the UI is repainted whenever those properties change. And <code>@service</code> annotations in the Glimmer component map to Ember.js services and routes.</p> <p>There are a lot of similarities here to Web Components, but some additional power. There's a lot of custom HTML attributes for mapping to JavaScript functions. Also Web Components aren't part of a framework, so there's no convention for integrating with other JavaScript files in specific directories like \"services\". A framework has a specific structure to its applications, so builds those conventions. This provides specific challenges:</p> <ol> <li>Web components have a specific lifecycle, Ember.js components have a separate distinct one.</li> <li>You don't have easy direct access to the Ember.js objects.</li> <li>You can't use annotations to access Ember.js services.</li> </ol> <p>The first point meant just adding the HTML markup for the web component didn't work. The lifecycle methods of the component didn't trigger when moving from page to page, because it wasn't recreating the component, it was just re-rendering the HTML with new information for the Ember.js route provided. The solution - easy enough - required taking a programmatic approach. The flow was:</p> <ul> <li>From the <code>did-insert</code> call the JavaScript function to build the UI, passing the current HTML element.</li> <li>Call the Ember.js service to get the JSON either remotely or from local storage.</li> <li>Programmatically create the web component.</li> <li>Append it to the passed HTML element.</li> </ul> <p>Another challenge (although it's no longer needed) was to call the Ember.js router from a <code>click</code> event in HTML elements in the web component. The eventHandler is defined in the web component, but that knows nothing about Ember.js and its router. The solution is one standard in web components - CustomEvents.</p> <ul> <li>The code creating the web component also registered an EventListener on the current HTML element, into which the web component was appended.</li> <li>The <code>click</code> event in the web component created a CustomEvent, passing details.</li> <li>The CustomEvent was dispatched.</li> <li>The EventListener's function (in the Ember.js component) caught the CustomEvent and used the data to pass to the Ember.js router.</li> </ul>","tags":["Web","Web Components"]},{"location":"blog/2025/03/14/web-components-ember/#summary","title":"Summary","text":"<p>Is it easier to create a GlimmerJS component and insert that instead? Possibly, especially if something more sophisticated is needed, where greater access to the Ember.js framework is required. But that's a challenge to be addressed if there's the need, and possibly refactor this code for consistency, again if required.</p>","tags":["Web","Web Components"]},{"location":"blog/2025/04/02/framework-web-17/","title":"XPages App to Web App: Part Seventeen - Lessons Learned","text":"<p>Now the application is built and has been in use for many months. It's time to review experiences and lessons learned.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#domino-rest-api","title":"Domino REST API","text":"","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#mock-data","title":"Mock Data","text":"<p>Right from the start the application was designed to work with mock data and real data. There may be questions over the benefits of this. After all, in this scenario the REST API was being configured by the same developer who was coding the web application. But it provided several benefits:</p> <ol> <li>Bruno requests could be included in the repo. This allowed JavaScript code to be cross-checked from a REST client. Whenever things do not work as expected, the ability to cross-check a process by changing one aspect helps quickly troubleshoot and avoid mistaken assumptions.</li> <li>It allowed the UI to be tested without polluting the database with bad data. This not only avoids false errors, it also speeds up development because I didn't need to keep switching to Notes Client to delete data.</li> <li>Domino REST API calls provided starting data as JSON in a matter of seconds. JSON data is easy to store in a file and load from JavaScript. And the JSON data was the same format as the initial calls. There was no need to business logic or intermediary classes to hold collections of objects, which was done in the original XPages application. If you prefer to just use <code>dominoView</code> and <code>dominoDocument</code> datasources, this is still not much slower.</li> </ol> <p>Yes, there are places where bugs are in non-mock branches of code. But if mock testing doesn't reproduce the error, this means small chunks of code to look at, and problems have been easy to diagnose.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#validation-and-data-quality-management","title":"Validation and Data Quality Management","text":"<p>From the very start of Domino REST API, enforcing data quality by configuration was a core principal. That's because there is no lingua franca for coding REST services on Domino, which means configuration - a no-code approach - was the only option.</p> <p>This means adding the validation is quick to implement, and can be tested before even opening up an IDE to code the web application. It also improves security, because even if a malicious party finds a way around the web application, the validation and data quality is ensured on the server, not in the client.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#drapi-dates-and-times","title":"DRAPI, dates and times","text":"<p>For the first few months of use, the application tracked Trips, which had a start and end date. This functionality had also been available in the XPages application. But this highlighted a problem with the redevelopment of the UI. Because even though trips just had a start date and end date, the XPages interface stored them as a start date-time and end date-time.</p> <p>It's not easy in XPages to store only as a date, you need to handle it with code, create a <code>DateTime</code> object and call <code>.setAnyTime()</code>. But DRAPI makes it easy to ensure something that only needs to be a date is only a date. Early on, the contrast between existing data being a date-time and new data being date only bit me. It's worth being aware of.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#coding","title":"Coding","text":"","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#imperative-vs-reactive","title":"Imperative vs Reactive","text":"<p>There is a major difference between XPages and framework-less web development or even Volt MX: the new application's coding is imperative, whereas XPages is reactive. Consider the scenario where you want to hide a div when a user clicks a button.</p> <p>In XPages, the visibility of the div (<code>xp:div</code> or <code>xp:panel</code>) will be computed on the component itself. It may be tied to a Java bean property, but it will still be managed on the component, and a partial refresh will allow the component to react to the change when the user clicks the button. Getting a component programmatically in XPages is rarely done.</p> <p>There may be web development frameworks that work similarly. EmberJS  tracks properties to know when the UI should be updated - should react to a change. AngularJS uses things like <code>ng-if</code> or <code>ng-unless</code> to react to property changes in a controller.</p> <p>But this framework-less web development is imperative and action-driven, same as Volt MX is. JavaScript code on a button gets HTML elements via <code>getElementById()</code> or queries, and changes attributes of it. This requires a change of mindset. It still requires an ability to think fourth-dimensionally, being aware of what current states are and what you need to change. But it can mean it's easier to work out what happens when you click a button. It's may make coding more verbose if you have a large web pages, but large web pages will create other challenges.</p> <p>But as we've also seen Custom Events can be triggered, so there is some concept of reactive programming. This is different to getting a button and calling its <code>click</code> eventHandler. In this case, you're publishing an event and letting the event listener capture it. Still, the IDE can help track through the code and work out where to add breakpoints.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#learning-curve","title":"Learning Curve","text":"<p>There is definitely a learning curve. XPages encourages creating a UI by dragging and dropping components and filling in property boxes.  As with any framework, developers are likely to stay within the box of what's provided by the framework. But this required a lot of learning - some was done before I started coding, when developing last year's Engage session; some was done \"on-the-job\" and will feed into this year's Engage session. I'm convinced it's increased my skill-set significantly, I've progressed more in one year than more cautious developers would in five or even ten. But it requires a certain mindset to embrace stepping outside of your comfort zone.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#framework-vs-no-frameworks","title":"Framework vs No Frameworks","text":"<p>Timing is key, and the timing is right to code a framework-less vanilla JavaScript application. Modern CSS means there is little need for CSS preprocessors like SCSS, SASS and LESS. CSS variables and <code>light-dark()</code> remove the need for proprietary approaches from frameworks. FlexBox and CSS Grid and their sophisticated layout options remove the need to add Bootstrap as your first dependency. Web components are becoming widespread and are not hard to create. But if you want something off-the-shelf, there are plenty and even the UI components of the Java framework Vaadin have been available as web components for a while. But the are also advances in standard HTML elements that will improve applications.</p> <p>A framework can give that comfort blanket, but still requires you to understand the framework to bend it to your will. Otherwise, simple development is easy, but anything else becomes exponentially trickier. I've witnessed that when I see developers try to combine non-Dojo code into an XPages application. The same occurs when trying to integrate React components into an AngularJS application, or web components into an EmberJS application.</p> <p>But there are similar challenges trying to integrate web components from npm into a vanilla JavaScript application. It's possible, but typically not as well documented. Yet. It will be interesting to see if that changes over the next five years.</p> <p>However, what is certain is that there is much less technical debt in an application that doesn't use a CSS preprocessor, doesn't rely on BootStrap and doesn't use jQuery. The approach is consistent with frugal engineering. It's not about \"not invented here\", it's about choosing standards-based approaches over frameworks.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#coding-performance","title":"Coding Performance","text":"<p>So often when developers talk about performance, they focus only on how the application performs when used. That's never the only measure of performance. If it takes 2 months to make it half a second quicker, is it worth it? Coding performance is equally important.</p> <p>I can't comment on speed to develop. I wasn't comparing. I also can't easily compare number of lines of code written, but the new app is probably about 2400 lines in total (including whitespace). Java provides menu options for generating getters and setters for example, but I would not be surprised if I wrote more custom code for the XPages application. Including pre-written code, it's definitely more.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#testability","title":"Testability","text":"<p>I never included testing in the application. Why? It's one user, who is also the developer, and who is happy to fix bugs when I find them. And it's not had significant development since it \"went live\", so regressions are low risk, low impact.</p> <p>The previous application never had any unit or UI testing. But I think it would be easier to implement for the JavaScript application, and certainly easier to find reference materials on how to implement it.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#deployment","title":"Deployment","text":"<p>This isn't deployed in an NSF. It's deployed to keepweb.d, which for me means I access the Domino server filesystem directly. That may be something some Domino administrators, developers and/or customers are nervous about. But is that a requirement? Certainly not, as you may see before long.</p> <p>An alternate approach would be push it to the filesystem following a successful pull request and unit test running on a build system like Jenkins. That may be unfamiliar to many Domino people, but it's one an approach software houses and IT departments are used to.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#user-experience","title":"User Experience","text":"","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#framework-vs-no-framework","title":"Framework vs No Framework","text":"<p>Framework-less development also ties into user experience. A framework with a predefined set of components encourages you to build something that conforms to those components. But it also means applications often look very similar, and have a similar user journey. When you consider the components provided by XPages Extension Library, you'll see many of them fit the premise of providing a \"Notes Client field for XPages\". The Data View significantly steps beyond the Notes View design element layout, but the other view components in the Extension Library reproduce Notes Client or iNotes view elements. The result is a user experience that prioritises familiar and quick over innovative.</p> <p>In the case of my XPages application, it began from a starter application, which again prioritised familiar to try to fulfil the \"rapid application development\" promise. It was \"development by numbers\" to a large degree, the bulk of coding was Java classes for beans and utility methods.</p> <p>Is it impossible to provide an innovative user experience in XPages? Definitely not. Could I have reproduced an XPages / Notes user experience in this application. Very probably. But a blank page makes it easier to think differently.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#view-and-document-driven-design","title":"View and Document-Driven Design","text":"<p>The XPages application combined two document types on a single \"form\" - ship and spot. It also included details from two different views in a single \"view\" - ship details were dynamically retrieved by performing a lookup on the ship UNID. But the approach was navigation --&gt; view --&gt; document.</p> <p>Keyword-style documents were viewed or edited in their own form. Dialogs were used mainly for pickers, never for creating or editing one of those keyword-style documents. Is that because it's not possible in XPages? No, it's certainly possible. But because of how Dojo handles dialogs, it's an area that generates a lot of questions in the XPages world, especially around data sources in dialogs and refreshing other areas of the XPage after processing the dialog. These are problems that don't exist in vanilla JavaScript and HTML dialogs.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#performance","title":"Performance","text":"<p>Let's talk about performance when using the application. This is the area I'm happiest with.</p> <p>\"Time to live\" is significantly quicker, even though there are things I could do to improve that.</p> <p>Performance for adding a ship spot is significantly quicker:</p> <ul> <li>Checking whether a ship exists doesn't require a round-trip request to the server.</li> <li>It doesn't require scrolling through or searching ships by name. It searches based on part of the name or the call sign.</li> <li>If the ship hasn't been logged before, I've already typed the name.</li> <li>Adding a country and/or flag can be done from the same screen, with at most two calls to the server. Compare that to multiple requests to the server to navigate through and load multiple views and forms.</li> <li>Selecting ports doesn't require round-trip requests to the server. These were two mandatory fields that each required loading a picker and its options from the server. Even though the list of options was the same for them both, because it was a picker, it required two round-trip server calls. The new application requires none.</li> <li>The pickers for ships and ports often timed out loading, because of network performance. This was a frustration, and one that no longer exists.</li> </ul> <p>This all means usability of the new application is much higher than the previous application. Could improvements have been made to the XPages application? Certainly. Could all the problems have been removed. No. That's because XPages requires more communication between device and server.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#summary","title":"Summary","text":"<p>Let me be totally blunt: this application is no less Domino than it was before.</p> <p>Yes, the development was done without launching Domino Designer, because the design already existed.</p> <p>Yes, it's not deployed in an NSF and it's not accessed from Domino's HTTP server. But \"Domino's HTTP server\" is actually \"Domino's HTTP server task\", like DRAPI is \"Domino REST API task\". So is it really that different that it's accessed from DRAPI's HTTP server?</p> <p>It's still Domino. But it's probably more future-proofed than before and has no reliance on third-party enhancements or fixes for a framework. There are definitely modernisations that can be done. And of course there are always improvements that can be made, whether in look and feel or functionality.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/02/framework-web-17/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Search and Save</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/07/css-export/","title":"Avoiding Inline Styles in XPages","text":"<p>Recently there have been discussions about inline CSS in XPages. The absence of anyone mentioning a feature in XPages prompted me to eventually ask about a piece of functionality relevant to this.</p>","tags":["XPages"]},{"location":"blog/2025/04/07/css-export/#to-inline-or-not-inline","title":"To Inline or Not Inline","text":"<p>The trigger for the discussions was a change in how XPages handles inline CSS, because CSP (Content Security Policy) blocks inline styles as well as inline JavaScript. This can be circumvented by setting <code>unsafe-inline</code>, but this is not recommended.</p> <p>I'm not interested in the rights or wrongs of inline content (and there are plenty of opinions on the internet about it) or how the problem was addressed in XPages. I also know there are XPages applications I built which still have inline styles, because XPages allowed me to do it. But there's a way to avoid it and solve it, at just a click of a button. And it appears many XPages developers are not aware of it, even though I was teaching about it in XPages courses over a decade ago.</p>","tags":["XPages"]},{"location":"blog/2025/04/07/css-export/#xpages-and-css","title":"XPages and CSS","text":"<p>Domino has had StyleSheet design elements since Domino 6, over twenty years ago. But when XPages was introduced, many developers were still unfamiliar with CSS. So XPages provided a no-code option for adding CSS, with Font, Background, and Margins tabs with various pickers on \"pretty panels\", as Maureen Leland called them.</p> <p></p> <p>There were two ways to use these. The first was to primarily use the no-code option. The second was to use it to minimise the amount that was new, but once comfortable with the new IDE, learn CSS and progress to code over GUI. This is similar to SSJS: one approach was to only ever use SSJS; the other was to use it while learning the IDE and then move to Java.</p> <p>The difference that CSS pretty panels provided was that it provided a quick and easy way to move the CSS to a stylesheet. It's probably not noticed because it's on the parent <code>Style</code> tab, and only appears when inline styles have been added. It is the Export button, as in this screenshot.</p> <p></p> <p>When you click this, you can export all styles to a stylesheet of your choice or cherry pick which styles to add. It also allows you to replace the inline styles.</p> <p></p> <p>What this means is that manually moving inline styles to comply with CSP settings is very easy - in fact probably easier than in standard web development, if developers wish. The inbuilt Eclipse Search menu functionality in Domino Designer will allow you to search for <code>style=</code> on just XPages and Custom Controls, to easily find where you need to change. Because they're textual design elements, the Eclipse search is well-suited to the task.</p> <p>This really falls into the aspect of \"app modernisation\" - updating dependencies, leveraging new functionality and coding options, removing previously deprecated code, minimising technical debt, leveraging standards to maximise frugal engineering. Investing your time this way means modernising your skills, giving you learning to share with the community and encourage others to get the most out of Domino development.</p>","tags":["XPages"]},{"location":"blog/2025/04/07/css-export/#inline-styles-in-recent-blog-posts","title":"Inline Styles in Recent Blog Posts","text":"<p>I'm also fully aware that in my recent \"XPages to Web App\" series, I was using inline <code>style</code> tags in web components. Firstly, the application is built by me and only used by me, so there's no need for this application to comply with Content Security Policy 2.0 and 3.0. But I've also investigated since how web components solve this problem, and I found a variety of solutions several weeks ago. In due time I'm sure I'll modify the app to remove the inline style - mainly to learn more than just the theory of implementing it, so I know how to do it when I need to.</p>","tags":["XPages"]},{"location":"blog/2025/04/19/framework-web-18/","title":"XPages App to Web App: Part Eighteen: CSP Enhancement","text":"<p>In my last blog post I talked about CSP and inline CSS. I mentioned that I had not addressed these issues with this Ship Spotter app. In this blog post we'll start to fix that.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/19/framework-web-18/#inline-csp-issues","title":"Inline CSP Issues","text":"<p>Firstly, there are two inline violations for CSP that are common: inline JavaScript and inline CSS. Because the application always uses Event Listeners (e.g. <code>element.addEventListener(\"click\", function () {...}</code>), we have no inline JavaScript. So there's no remedial work to do. All we have to deal with - and it's no small task - is inline CSS.</p> <p>Secondly, bad practice happens and needs enhancement work to fix it. \"App modernization\" will constitute its own blog post. As a developer, you should expect it and should manage your customers' expectations accordingly.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/19/framework-web-18/#styledisplaynone","title":"<code>style=display:none</code>","text":"<p>Probably the most common inline CSS is <code>display:none</code>. One of the biggest places this was used in my application was in the <code>toggleSPA()</code> method, used to show / hide sections.</p> <pre><code>const toggleSPA = (showme, how) =&gt; {\nspaSections.forEach((s) =&gt; {\n    const display = showme === s ? how : \"none\";\n    document.getElementById(s).style.display = display;\n});\n</code></pre> <p>Thankfully, this didn't take too long to solve, because I have seen a simple solution in other frameworks and because I used a single function for changing visibility of functions. The approach is to use a class (e.g. <code>.hidden</code>) and add / remove the class as required.</p> <p>Then it's a case of modifying the code. The code here iterates all sections and sets style to <code>display:none</code> or the required style, which is the application was always \"block\", the default display style. We could do something similar here - apply the class <code>hidden</code> to all sections, then remove it from the one we want to show. But I took the opportunity to improve the application.</p> <p>Currently, I call <code>toggleSPA()</code> when the application loads to only show the login section. But this means there's a flash of the other sections when the application first loads, before they are then hidden. So I've changed the index.html to set <code>.hidden</code> class on all sections except the login one by default. So I no longer need to call <code>toggleSPA()</code> on load.</p> <p>In the <code>toggleSPA()</code> function itself, I decided to just add the <code>hidden</code> class to the currently-displayed section and remove <code>hidden</code> class from the section to display. But this means I need to track which section to display. That's done by adding this to the index.js:</p> <pre><code>let currentDisplayedSection = \"credentials\";\n</code></pre> <p>The <code>toggleSPA()</code> function can then be streamlined to:</p> <pre><code>const toggleSPA = (showme) =&gt; {\n    document.getElementById(currentDisplayedSection).classList.add(\"hidden\");\n    document.getElementById(showme).classList.remove(\"hidden\");\n    currentDisplayedSection = showme;\n};\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/19/framework-web-18/#removing-inline-style-from-web-components","title":"Removing Inline Style from Web Components","text":"<p>That leaves all the web components as the next place to remove inline style from. As a reminder, this is the landing.js web component:</p> <pre><code>const template = document.createElement(\"template\");\ntemplate.innerHTML = `\n    &lt;style&gt;\n        .landing-container {\n            display: flex;\n            flex-wrap: wrap;\n            align-items: stretch;\n            justify-content: center;\n        }\n        .landing-tile {\n            margin: 5px;\n            font-weight: bold;\n            font-size: 30px;\n            color: light-dark(var(--primary-color-dark), var(--primary-color));\n            background-image: radial-gradient(circle at center, light-dark(var(--landing-tile-start),var(--landing-tile-start-dark)) 15%, light-dark(var(--landing-tile-end),var(--landing-tile-end-dark)) 100%);\n            height: 200px;\n            width: 200px;\n            box-shadow: inset 0 0 2px 2px light-dark(var(--border-color-primary),var(--border-color-primary-dark));\n            border-radius: 10px;\n            flex-grow: 1;\n            text-align: center;\n            align-content: center;\n            cursor: pointer;\n        }\n    &lt;/style&gt;\n    &lt;div id=\"landing-container\" class=\"landing-container\"&gt;\n    &lt;/div&gt;\n`;\n</code></pre> <p>That style tag needs removing, because it adds inline style to the HTML. Fortunately, there's a great website that covers web components, webcomponents.guide, and it has a page and section covering styling. There are four options:</p> <ul> <li><code>&lt;style&gt;</code> tag, which I'm trying to avoid.</li> <li><code>&lt;link rel-\"stylesheet\" /&gt;</code> using an external stylesheet.</li> <li>Constructable stylesheets.</li> <li>CSS Module scripts to load external stylesheets.</li> </ul> <p>For this application, I'd prefer to avoid external stylesheets. External stylesheets have the benefit of better IDE support, which is well worth considering. But a single file makes it easy to copy and paste around. Many web components are packaged as npm modules, which lets dependency management avoid the problem. Some others use CDNs, which again hides the complexity of loading related files. But I'm not using Node.js or a CDN, so keeping everything self-contained seems a benefit. And the styles are not extensive in these web components and we're already coded as embedded in JavaScript, so constructable stylesheets are my preferred option here.</p> <p>This is similar to the approach used for the HTML templates for the web components: we create a JavaScript object and insert the content. The syntax is different, but not hugely complex:</p> <pre><code>const stylesheet = new CSSStyleSheet();\nstylesheet.replaceSync(`\n    .landing-container {\n        display: flex;\n        flex-wrap: wrap;\n        align-items: stretch;\n        justify-content: center;\n    }\n    .landing-tile {\n        margin: 5px;\n        font-weight: bold;\n        font-size: 30px;\n        color: light-dark(var(--primary-color-dark), var(--primary-color));\n        background-image: radial-gradient(circle at center, light-dark(var(--landing-tile-start),var(--landing-tile-start-dark)) 15%, light-dark(var(--landing-tile-end),var(--landing-tile-end-dark)) 100%);\n        height: 200px;\n        width: 200px;\n        box-shadow: inset 0 0 2px 2px light-dark(var(--border-color-primary),var(--border-color-primary-dark));\n        border-radius: 10px;\n        flex-grow: 1;\n        text-align: center;\n        align-content: center;\n        cursor: pointer;\n    }\n`);\n</code></pre> <p>We create a new <code>CSSStyleSheet</code> and call its <code>replaceSync()</code> method applies the CSS. <code>replace()</code> is the async option, but we're only passing a few rules in. There are methods for adding rules dynamically, but I think this is more readable.</p> <p>Finally, we need to apply the stylesheet to the web component. This is done in the constructor:</p> <pre><code>this.root = this.attachShadow({ mode: \"closed\" });\nthis.root.adoptedStyleSheets = [stylesheet];\n</code></pre> <p>We pass <code>stylesheet</code> JavaScript CSSStyleSheet object to the array of adopted stylesheets.</p> <p>There are still more inline styles to remove, but that will be the usual way: converting them to classes.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/19/framework-web-18/#all-solved","title":"All Solved?","text":"<p>Umm, no. This works for most of the components. But the Ship and Spot components didn't use a shadow DOM. And <code>.adoptedStyleSheets</code> can only be used on a Document or Shadow DOM; it can't be added to just an HTML element. So it won't work for the Ship and Spot components.</p> <p>There are two options:</p> <ol> <li>Move the inline styles to the main stylesheet.</li> <li>Add a shadow DOM.</li> </ol> <p>Option 1 feels wrong, because the inline styles are specific to this web component. But option 2 complicates things for two reasons. Firstly, the styles on the main stylesheets will no longer be available inside the web component. Secondly, we'll be inserting a shadow DOM, which means <code>this.querySelector()</code> and similar APIs will fail, because they now need to be applied to the component's shadow DOM, not the component itself.</p> <p>But we can't just indiscriminately change everything that is <code>this.</code> because the shadow DOM only applies to DOM manipulation methods, not to calls to properties or methods of the web component itself. This is where understanding the code you're writing is critical. It would also have been simplified by setting <code>this.root = this;</code> when originally developing the application, for two reasons. Firstly, the syntax for DOM manipulation would have been identical across all web components. Secondly, we would only have had to change one line of code to insert a shadow DOM.</p> <p>So there are three steps:</p> <ul> <li>Add a shadow DOM with <code>this.root = this.attachShadow({ mode: \"open\" });</code>.</li> <li>Add a stylesheet and add it to the shadow DOM, the same way we did for other components.</li> <li>Update every DOM manipulation call in Ship and Spot. So <code>this.querySelector</code> becomes <code>this.root.querySelector</code> and <code>this.appendChild</code> becomes <code>this.root.appendChild</code>. <code>.getElementById()</code> would be another example of an API we would need to change, if it had been used.</li> </ul> <p>But now we need to get the styles from the main stylesheet in. We could include all stylesheet, but that gets messy. The only styles are for forms, so we'll create a separate stylesheet. Unfortunately the styles are also used on the login form, which was not a web component. So the stylesheet also needs adding to the index.html.</p> <p>For the Ship and Spot, we can just add the stylesheets to the HTML for the Ship, in the same way we would for the index.html:</p> <pre><code>&lt;link href=\"../forms.css\" rel=\"stylesheet\" type=\"text/css\"&gt;\n&lt;link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\"&gt;\n</code></pre> <p>Now it's just a case of going through and cleaning up all the inline CSS throughout the application.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/19/framework-web-18/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/","title":"XPages App to Web App: Part Nineteen: Spots By Date and Stats Pages","text":"<p>Back when the application was on XPages and spots were associated to a trip, there was a summary page that gave stats for the number of new spots, ships, ports, and countries during a trip. Now the requirement for trips has gone. But sometimes it's nice to see details of new documents across a period of time. So over the weekend, I added that functionality. This was very similar to a Spots By Date page, which I had not yet covered. So we'll cover both together.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#getting-the-data","title":"Getting The Data","text":"<p>The first thing to clarify is how the trip statistics were generated previously, and why it had to change. Previously, a Trip document was created and this became the active trip. Then, whenever a document was created, it got stamped with the active trip UNID. This meant any statistical reporting could use a view, categorised on trip UNID, with sub-categories for each form type and totals on them.</p> <p>The method of retrieving the data is noteworthy. Many Domino developers would just instinctively create a ViewEntryCollection and iterate them. But my approach, and one I had used for many years, was to use a ViewNavigator. ViewNavigators in XPages were better optimised than ViewEntryCollections as well as providing cache options and a max level. This meant I could create a ViewNavigator that ignored all actual documents.</p> <p>With DRAPI, there were also optimisations that could be made and this is even more important when returning JSON over HTTP for processing in JavaScript. Hopefully all DRAPI developers are fully conversant with all the options on the \"/lists/{name}\" endpoint. But using category or key options allows restricting to the selected trip UNID. Then scope can be used set to \"categories\", to ignore actual documents. meta can be set to false, to suppress metadata for the view entries.</p> <p>But now, the application no longer has trips. So the reporting needs to be based on dates. So a view won't work. Similarly, for the Spots by Date page, this also needs to be based on a start and end date.To achieve this we need to use DQL.</p> <p>Tip</p> <p>If you make a DQL query trying to retrieve thousands of documents, you're going to create a problem for yourself, same as trying to retrieve thousands of documents from a view. Even though DRAPI chunks data so you can deal without before waiting for the final chunk of data, you're still making it harder for yourself to provide a good user experience. The less data you stream to the browser, the better the user experience will be. And consider your own usage of the biggest database of all - Google: no sane person ever scrolls through hundreds of documents, they want to find the right document quickly, perform whatever action they wish upon it, then move on. So why give users a view of thousands of documents?...other than because that's what you've done for the last twenty years in your Notes Client applications.</p> <p>So what about reporting? If I wanted to get data for more than a small number of days, I would provide selectors for a start month/year and end month/year, and use a categorised view, getting categories and without metadata. I may use one view for the last 12 months, and any further back I would provide either rolling year or calendar year. Thus the \"/lists/{name}\" endpoint would return maximum 12 (months) x 4 (forms) JSON objects.</p> <p>But the key here is that I've thought about the reporting. And I'm not afraid to tell myself or others to think about it and then provide specific reporting. And for historic data, there are things called file servers.</p> <p>Okay, so we've identified it's a DQL query, between two reasonably close dates, not selecting something like start date of 01/01/2021 and end date of 04/04/2025. So this is going to be using the \"/query\" endpoint. This is a POST request that takes a JSON object containing DRAPI details - <code>forms</code> and <code>mode</code> - and standard DQL settings. The <code>getSpotsByDate()</code> query and <code>getStatsByDate()</code> query are virtually identical. The only difference is the array of forms to return, just comprising <code>[\"Spot\"]</code> for <code>getSpotsByDate()</code>, whereas this for the <code>getStatsByDate()</code>:</p> <pre><code>const bodyObj = `{\n    \"forms\": [\n        \"Spot\",\n        \"Ship\",\n        \"Admin_Country\",\n        \"Admin_Port\"\n    ],\n    \"includeFormAlias\": true,\n    \"maxScanDocs\": 500000,\n    \"maxScanEntries\": 200000,\n    \"mode\": \"dql\",\n    \"noViews\": false,\n    \"query\": \"@Created &gt;= @dt('${startDate}') and @Created &lt;= @dt('${endDate}')\",\n    \"timeoutSecs\": 300,\n    \"viewRefresh\": true\n}`\n</code></pre> <p>We're not using aliases, but for ease we set <code>includeFormAlias</code> to true. We should never hit the <code>maxScanDocs</code> or <code>maxScanEntries</code>, but we go with the defaults. The query uses the <code>startDate</code> and <code>endDate</code> JavaScript variables passed into the functions. These are dates only, but can use them to map to <code>@Created</code>, and DQL will return the correct data. The processing of the response is identical to all the other DominoService functions.</p> <pre><code>    Three pro tips here:\n\n    1. Have an agent using LotusScript (or Java if you have to) to perform a DQL query. Use this to cross-reference and make sure your DQL syntax and return are as expected. I also use Postman or the new [**bruno** collection](https://opensource.hcltechsw.com/Domino-rest-api/references/downloads.html#bruno) to cross-reference the DQL data returned with what I get via code, to provide another cross-reference point, this time with DRAPI added into the mix. *Everything* in IT is a process, and if you don't use the skills at your disposal to pinpoint which step in the process is failing, you're going to make your life harder.\n    2. DRAPI filters the DQL results based on the form mode (`mode`) passed. If the mode doesn't exist for that Form, you don't get the document. If there is a mode, but the current user doesn't map to the read access for that mode, you don't get the document.\n    3. You can create a specific mode that returns the minimum possible fields you need. This can also ensure the JSON returned is as small as possible.\n\nNot surprisingly, if you remember where this tutorial started. I generated some data via bruno as mock data for me to develop against using my \"John Doe\" user. This was loaded from a `getStatsByDate()` function in the MockService class, paralleling the `getStatsByDate()` function in the DominoService class. Because it was a while since I last built that sample data, I also needed to re-run the Ship data. But now I'm ready to start developing the new page.\n\n!!! tip\n    If you're doing reporting across multiple categories, \"/listspivot/{name}\" is well worth getting familiar with. It's used against a flat view to pivot (or group) the data on a specific column and provides minimum, maximum, count and sum for all other columns. Of course minimum, maximum, and sum are not so helpful for alpha columns. But this provides powerful and succinct data for reporting. And it can also be refined based on keys.\n\n## Adding the New Page\n\nOn the index.html, we need to add new section for the web component, with back buttons and titles. We'll call the new HTML elements `&lt;ship-spots-elem&gt;` and `&lt;stats-elem&gt;`.\n\nOn the index.js file, there are six steps:\n\n- load the JavaScript classes we're going to create, in the same way we loaded all the other web components.\n- add to the `spaSections` array, for the new sections.\n- add to the `tiles` array.\n- add an event handler on the new back button.\n- register custom event handlers for `loadSpotsByDate` and `loadStatsByDate`.\n- add functions for `loadSpotsByDate` and `loadStatsByDate` to call the Domino / Mock service functions and process the response.\n\n`loadSpotsByDate()` takes the `shipSpotElem` object that will have a start date, end date, and a `populateSpots()` method.\n\n```js\nconst loadSpotsByDate = async (event) =&gt; {\n    const shipSpotElem = event.detail.shipSpotElem;\n    console.log(\"Getting spots between \" + shipSpotElem.startDate + \" and \" + shipSpotElem.endDate);\n    const endDate = shipSpotElem.endDate;\n    const json = await window.dataService.getSpotsByDate(shipSpotElem.startDate, endDate);\n    if (typeof(json) === \"object\") {\n        json.sort((a,b) =&gt; {\n            if (a.Created &lt; b.Created) return -1;\n            if (a.Created &gt; b.Created) return 1;\n            return 0;\n        });\n        shipSpotElem.populateSpots(json);\n    } else {\n        statusError(\"Error getting spots: \" + json);\n        removeMask();\n    }\n}\n</code></pre> <p>The key difference here is that if we return a JSON object, we sort the data on created date. This gracefully does nothing if an empty array was returned. If no JSON object is returned, it will be because we returned a string error message from DRAPI.</p> <p>For <code>loadStatsByDate()</code> the if statement is slightly different.</p> <pre><code>    if (typeof(json) === \"object\") {\n        if (json.length === 0) {\n            statsElem.populateStats(json);\n        } else {\n            json.sort((a,b) =&gt; {\n                if (a.Created &lt; b.Created) return -1;\n                if (a.Created &gt; b.Created) return 1;\n                return 0;\n            });\n            const groupedJson = Object.groupBy(json, ({ Form }) =&gt; Form);\n            statsElem.populateStats(groupedJson);\n        }\n    }\n</code></pre> <p>We want to sort on date first, but then group on Form. <code>Object.groupBy()</code> has been baseline supported in browsers for some time, so it makes sense to use that. To avoid any potential problems, if we got an empty array back for the query, we just pass that to <code>statsElem.populateStats</code>.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#statsobj-web-component","title":"statsObj Web Component","text":"<p>The HTML template is similar for both web components. This is the bulk of it for the stats web component:</p> <pre><code>&lt;link href=\"./forms.css\" rel=\"stylesheet\" type=\"text/css\"&gt;\n&lt;form&gt;\n    &lt;div class=\"col\"&gt;\n        &lt;div&gt;\n            &lt;label for=\"stat-start\"&gt;Start Date&lt;/label&gt;\n            &lt;input type=\"date\" class=\"form-control\" placeholder=\"Start Date\" aria-label=\"Start Date\" id=\"stat-start\" /&gt;\n        &lt;/div&gt;\n        &lt;div&gt;\n            &lt;label for=\"stat-end\"&gt;End Date&lt;/label&gt;\n            &lt;input type=\"date\" class=\"form-control\" placeholder=\"End Date\" aria-label=\"End Date\" id=\"stat-end\" /&gt;\n        &lt;/div&gt;\n        &lt;div class=\"actionBtn\"&gt;\n            &lt;button id=\"stat-search-btn\" aria-label=\"Search\" type=\"button\" class=\"btn-primary\"&gt;Search&lt;/button&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/form&gt;\n</code></pre> <p>We're adding a shadow DOM, so we load the forms stylesheet. Colour variables allocated to <code>root</code> in the theme stylesheet are available even to the shadow DOM, so we don't need to load that. Then we have a Start and end date, and a search button. We could add min and max values to the inputs. But as I'm the only user, I won't bother.</p> <p>Below the form, we add a div to hold the results. For the spots, that's sufficient, because like the list of spots on a ship, this list gives us everything we need. However, on the stats form, the div to hold the results will give summary figures only. So we'll add a second \"details\" div.</p> <p>Following the CSP enhancements in the last part we'll add a stylesheet. I'll just cover the CSS for the selection form for now:</p> <pre><code>.col {\n    display: grid;\n    grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));\n    gap: 0.5rem;\n\n    div {\n        display: grid;\n        grid-template-columns: 100px 1fr;\n        gap: 0.5rem;\n        align-items: baseline;\n    }\n\n    .actionBtn {\n        grid-template-columns: 1fr;\n        justify-items: end;\n    }\n\n}\nbutton {\n    padding: 0 0.5rem;\n    height: 1.75rem;\n    cursor: pointer;\n}\n</code></pre> <p>For a desktop browser, we put the two fields and the button on the same line. Because we're using a repeat on the <code>col</code> grid, on a mobile device they will display on separate lines. The date selection divs are also grids, with two columns. But the <code>actionBtn</code> div is a single column, with its contents justified at the end of the column. This ensures the button appears at the end of the line, which looks better.</p> <p>The <code>ShipSpot</code> component for the spots by date extends the <code>Spots</code> component created in part 16, with a <code>dateOptions</code> array to handle consistent date format. During the <code>render()</code> function we add an event handler onto the button to call the <code>doSearch()</code> function. Again, that's very similar between the two web components, and just validates the start date, defaulting end date to the same if not set, and dispatches an event for the index.js to catch:</p> <pre><code>doSearch = () =&gt; {\n    this.startDate = this.root.querySelector(\"#stat-start\").value;\n    this.endDate = this.root.querySelector(\"#stat-end\").value;\n    if (this.startDate === \"\") {\n        this.dispatchEvent(new CustomEvent(\"sendUserMessage\", { bubbles: true, detail: {type: \"error\", message: \"Start Date is required\"}}));\n        return;\n    }\n    if (this.endDate === \"\") {\n        this.root.querySelector(\"#stat-end\").value = this.startDate;\n        this.endDate = this.startDate;\n    }\n    this.dispatchEvent(new CustomEvent(\"mask\", { bubbles: true, detail: {show: true}}));\n    this.dispatchEvent(new CustomEvent(\"loadStatsByDate\", { bubbles: true, detail: {statsElem: this}}));\n}\n</code></pre>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#displaying-results","title":"Displaying Results","text":"<p>Displaying the results for the spots by date is similar to the code in part 16 for spots by ship. The main difference is we also need the ship details. So we just need to get the ships JSON from localStorage (to ensure it's up-to-date) and filter where UNID is the same as the spot's ShipUNID. The code itself should be easy enough to follow, so I won't go into details.</p> <p>For the stats, it's slightly different. Here we're going to do two things:</p> <ol> <li>Add a graphical summary of numbers of new documents for each form type.</li> <li>On click of each form type, display details.</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#graphical-summary","title":"Graphical Summary","text":"<p>Just to remind you, the JSON object we got back was details, grouped on form type. We'll load that into a property of the web component, so we can reference it if the user clicks on each label. And we'll also pull the ships from local storage as well. It won't change between the user clicking each label, only (potentially) if the user does another search. If the array is empty, we'll display a message saying \"No stats found\", otherwise we'll load the summary.</p> <p>Spots will always have the highest number: there's no reason to create any of the other form types unless also creating a spot. So we'll set that as our max for all bars. We only have a specific set of form types, so our code will just look for each:</p> <pre><code>const max = value.Spot.length;\nconst spotProg = this.createProgressBar(\"Spots\", \"Spot\", max, max);\nspots_container.appendChild(spotProg);\nif (value.Ship) {\n    const shipProg = this.createProgressBar(\"Ships\", \"Ship\", max, value.Ship.length);\n    spots_container.appendChild(shipProg);\n}\n...\n</code></pre> <p>We call a <code>createProgressBar()</code> function passing label, form name (which is the key we grouped on), the max for the bar and the actual number of documents for this form type. This is the code for each \"row\".</p> <pre><code>createProgressBar(label, key, max, value) {\n    const div = document.createElement(\"div\");\n    div.className = \"summary\";\n    const labelElem = document.createElement(\"label\");\n    labelElem.setAttribute(\"for\", label);\n    labelElem.innerHTML = label;\n    labelElem.addEventListener(\"click\", (event) =&gt; {\n        event.preventDefault();\n        this.loadDetails(key);\n    });\n    div.appendChild(labelElem);\n    const progress = document.createElement(\"progress\");\n    progress.setAttribute(\"max\", max);\n    progress.setAttribute(\"value\", value);\n    progress.setAttribute(\"aria-label\", label);\n    progress.setAttribute(\"id\", label);\n    div.appendChild(progress);\n    const span = document.createElement(\"span\");\n    span.innerHTML = value;\n    div.appendChild(span);\n    return div;\n}\n</code></pre> <p>We create a clickable label which will load details for the key in the JSON object. We then create a <code>&lt;progress&gt;</code> HTML element, which has been baseline for a decade. We finally add a span for the number, so the user can easily see the actual number of new documents at a glance. We could choose a charting library, pull it in, and try to load numbers and labels. But all we really want is a label, a bar that gives comparative sizing between each form type, and a number. Purists might prefer a proper chart. But this provides the information I want in a satisfactory UI, quickly, easily, without additional dependencies. I'll let others fight with charting libraries, while I'm spotting ships at the beach with a beer!</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#css","title":"CSS","text":"<p>Let's also look at the CSS for this graphical display:</p> <pre><code>.stats-container {\n    display: grid;\n    grid-template-columns: auto 1fr auto;\n    gap: 5px;\n    align-items: center;\n    background-color: light-dark(var(--spot-background), var(--spot-background-dark));\n\n    .summary {\n        display: contents;\n\n        label {\n            padding: 5px;\n            cursor: pointer;\n            color: light-dark(var(--button-primary), var(--button-primary-dark));\n        }\n\n        progress {\n            width: 100%;\n            block-size: 1.5rem;\n        }\n\n        span {\n            padding: 5px;\n            justify-self: flex-end;\n        }\n    }\n}\n</code></pre> <p>The main container has three columns, the ones for the label and the number of new documents set to <code>auto</code> and the one for the progress bar taking up the remaining space. <code>align-items: center</code> means they all line up nicely vertically in the middle of each bar. We add a background colour, as always using <code>light-dark()</code>.</p> <p>The summary div, which contains all elements for each form type, is set to <code>display: contents</code>, so it's in the DOM but ignored for CSS Grid. That means we can't add any styling to it, but it makes it easier to see the elements with developer tools.</p> <p>The label is styled like a link with padding so it doesn't appear flush against the left, top, and bottom of the grid. The progress bar is set with full width, so it takes up all available space in the grid cell, and <code>block-size: 1.5rem</code> to make it a bit thicker. The span for the number of documents is also padded, for the same reason as the label, and with <code>justify-items: flex-end</code> so it is right-aligned.</p> <p>One points on browser coverage: I've noticed that the progress bar thickness isn't modified by <code>block-size</code> on iPad, whether in Safari or Chrome. I've no idea why, but that's rarely what I use. So I haven't bothered troubleshooting or trying to resolve.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#details","title":"Details","text":"<p>The <code>loadDetails()</code> function clears the <code>detailsContainer</code> and loads data from the relevant document type for each document. For ships, it also provides a link to open the relevant Ship. For ports and countries, it just displays summary information - there is no expectation to edit those documents. Spots is the most complex and modelled on the display on the \"Spots by Date\" page, so I'll show that in full:</p> <p><code>``js linenums loadSpot(spot, index, div) {     const shipUnid = spot.ShipUNID;     const filtered = this.ships.filter(obj =&gt; obj[\"@meta\"].unid === shipUnid);     const shipDiv = document.createElement(\"div\");     shipDiv.id =</code>spot-${index}<code>;     const a = document.createElement(\"a\");     a.href = \"#\";     if (filtered.length === 0) {         console.error(</code>Could not find matching ship ${shipUnid}<code>);         a.innerText = \"No matching ship\";     } else {         if (filtered.length === 1) {             a.innerHTML = filtered[0].Ship;         } else {             console.error(</code>${filtered.length} ships match ${shipUnid}`);             a.innerHTML = filtered[0].Ship;         }         a.addEventListener(\"click\", (event) =&gt; {             this.dispatchEvent(new CustomEvent(\"showShip\", { bubbles: true, detail: {shipObj: filtered[0]}}));         })     }     shipDiv.append(a);</p> <pre><code>const span = document.createElement(\"span\");\nspan.innerHTML = \"&amp;nbsp;spotted at&amp;nbsp;\"\nshipDiv.append(span);\nconst locLink = document.createElement(\"a\");\nlocLink.href = \"#\";\nlocLink.innerText = spot.Location;\nlocLink.addEventListener(\"click\", (event) =&gt; {\n    if (filtered.length &gt; 0) {\n        this.dispatchEvent(new CustomEvent(\"showSpot\", { bubbles: true, detail: {shipObj: filtered[0], spotObj: spot}}));\n    } else {\n        this.dispatchEvent(new CustomEvent(\"showSpot\", { bubbles: true, detail: {shipObj: null, spotObj: spot}}));\n    }\n})\nshipDiv.append(locLink);\ndiv.append(shipDiv);\n\nconst createdDiv = document.createElement(\"div\");\nconst createdDate = new Date(spot.Created);\nconst localDate = Intl.DateTimeFormat(undefined, this.dateOptions).format(createdDate);\nconst created = document.createElement(\"div\");\ncreated.id = `created-${index}`;\ncreated.innerText = `${localDate}`;\ncreatedDiv.append(created)\ndiv.appendChild(createdDiv);\n\nconst details = document.createElement(\"div\");\ndetails.id = `spots-details-${index}`;\nif (spot.PortFrom != \"\" &amp;&amp; spot.PortTo != \"\") {\n    details.innerHTML = `From ${spot.PortFrom} to ${spot.PortTo}`;\n} else if (spot.PortFrom != \"\") {\n    details.innerHTML = `From ${spot.PortFrom}, destination port not known`;\n} else if (spot.PortTo != \"\") {\n    details.innerHTML = `Going to ${spot.PortTo}, origin port not known`;\n}\ndiv.appendChild(details);\n</code></pre> <p>} ```</p> <p>We find the relevant ship on lines 2-3 and create a div to hold the ship details. We then create a link displaying the ship name, triggering a custom event to show the ship. From line 25 we create a span saying \" spotted at \". Then from line 28 we create a link displaying the spot location, this time triggering a custom event to show the spot. This needs to pass both the ship and the spot. Next from line 43 we create a div for the created date, formatting using <code>Intl.DateTimeFormat</code>, as we covered in part 16. Finally, from line 52 we add a div with the port from and to details.</p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#summary","title":"Summary","text":"<p>This provides quick, simple reporting with the ability to drill down and navigate around, as needed. You can see an example of the output below:</p> <p></p>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/22/framework-web-19/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Web","Domino","HTML","CSS","JavaScript"]},{"location":"blog/2025/04/26/using-ai/","title":"Using AI","text":"<p>As a researcher, I'm always looking to learn, to expand the toolbag I have, and find innovative ways to improve outcomes. Even when AI is not at the heart of the project I'm working on, I'm constantly looking for ways it can make my life easier and life easier for developers using what I create. And the fact that we targeted a standard IDE means the effort required to integrate AI is reduced. But as with any new tool, it's important to learn what it can do and what it can't. And work this week has shown me that this requires a diligent approach.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#how-we-learn","title":"How We Learn","text":"<p>First, it's important to review how we learn new technologies, as developers. Because the way we will learn to use AI is very different. It's also much more sophisticated than any technology we've ever used before. And unless you're using it in a very basic way, the way you need to interact with it is much more black and white than any language, framework, or IDE we're used to working with.</p> <p>Let's think about how we learned languages or frameworks. We look for a tutorial, follow the tutorial, and get a predefined, pre-canned outcome. The same is true of an IDE. Of course any tutorial will not cover anything, so we use cheatsheets of menu options, keyboard shortcuts, settings etc. And with a framework, we look for linters and validators to build best practice.</p> <p>The bottom line is that the language, framework, or IDE has been designed for a specific use, a specific set of outcomes. And someone has usually come before us, there's someone who's used it the way we want to. And someone understands how it works at a microscopic level, even if the language, framework, or IDE is not open source and allows us to look at how it works.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#a-whole-new-world","title":"A Whole New World","text":"<p>Hopefully now I've made this explicit, you realise that AI doesn't work that way. If it's designed for narrow specific use cases, it's just not scalable. The whole point is to scale quickly and effectively, in what AI can understand and the outcomes it can generate. It's not coded to work with Java, or JavaScript, or Python code. It's not taught individual frameworks like React, Vue.js, Dojo, or others. So there's no developer who can inspect code to see the answers it would give for a specific question on a specific framework.</p> <p>Agentic AI looks to be designed to provide that kind of narrow answer to a specific question. But not in the way chatbots used to work, where you loaded a variety of ways of asking a question, so it could cross-reference and give a static response - or a static response with variables inserted. If my understanding is correct, the AI model will parse the natural language of the question, formulate content for an AI agent in a specific way (e.g. one of multiple APIs to get a customer or a location), and work out how to use the response - either to pass directly to the user, or to other agents.</p> <p>But whereas these AI agents can be inspected to understand what they do, the developer of the agent can't inspect how the rest of the AI process flow acts around the specific call to the agent.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#basic-usage","title":"Basic Usage","text":"<p>Of course there's a very simplistic usage of AI: asking a question against a knowledgebase of information, whether it be a set of APIs or a dataset. This has already become available in search engines. When I ask something in Google, it gives me an AI response, with a handful of links from which it got the answer. If it's API - or combinations of APIs to generate code - it either compiles or it doesn't, it either runs or it doesn't, and it's usually simplistic enough for you to understand that it will do what you want or won't. It's how Copilot's code completion works, filling in subsequent lines of code. It's black and white, right or wrong, useful or not.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#more-sophisticated-usage","title":"More Sophisticated Usage","text":"<p>But if you go beyond the basic usage, there can be more sophisticated ways that AI can be used, more extensive processing of code. The answers may still be black or white, but the whole reason for using AI may be to speed up a process that would be too time-intensive if done manually. That was the way I used Copilot's chat recently, and it demonstrated to me some lessons.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#the-troubleshooting-approach","title":"The Troubleshooting Approach","text":"<p>Some years ago I wrote a blog post on troubleshooting support. It's probably one of the most important blog posts I've written, and it's also very relevant to using AI for more sophisticated purposes.</p> <p>In that blog post I talked about having hypotheses (plural), some idea of the likelihood of each, and identifying ways to prove and disprove them. Likewise, with AI, it's important to have an idea of the answer you're going to get. Even more important, you need a baseline, a way to cross-reference that the answer is correct. This can be modifying the dataset and asking the same question, to see if you get the correct answer.</p> <p>But there's a problem.</p> <p>You know you've modified the data. You saw yourself do it. And so you assume your AI also knows you modified the data. But does it?</p> <p>It's important to explicitly tell Copilot when you need it to re-read the dataset.</p> <p>You may find out that it doesn't give the answer you expect from your baseline. At this point, you can just ignore AI. Or you can try to work out why it's getting the wrong answer. In my case, I was asking it to compare method signatures in two files, and I tried to get a baseline by modifying a method signature. But the code had comments as well.</p> <p>It became apparent that we were looking at the files differently.</p> <p>I was focusing on the code, Copilot was looking at the comments. It's important to have hypotheses about how AI is generating answers. There are questions you can ask to verify this. But again, it's down to making intelligent hypotheses (plural), avoiding assumptions, and gathering the information needed to verify the answers you're getting.</p> <p>It requires an intelligent user to perform sophisticated processing with accurate outcomes.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/04/26/using-ai/#some-key-thoughts","title":"Some Key Thoughts","text":"<p>Passing multiple files to a Copilot chat is possible. But if you want to pass a number of files, it may be easier to pass a folder using <code>#folderName</code>, even it means temporarily moving files into the same folder. According to Copilot's answer, you may need to pass a file again if you need to clarify something specific.</p> <p>By default, it also uses what it can see in VS Code's active editor. You may not want it to do that, so it's important to exclude it.</p> <p>There is also useful information in the results, Copilot will include a twistie with the references it used as the basis for its response. It's worth looking at that to make sure it's using the right content. And it may be useful to start a new chat, to clear the context.</p> <p>And remember, you can always ask Copilot to help verify your assumptions of how it's coming to conclusions. That is something that is very easy to forget.</p>","tags":["AI","GitHub Copilot"]},{"location":"blog/2025/05/01/drapi-cors-regex/","title":"Domino REST API, CORS and Regex","text":"<p>Release 1.1.3.1  of Domino REST API introduces a breaking change in CORS handling. This makes configuration less straightforward, but as the documentation states, it increases the flexibility and probably makes things a lot easier for larger environments. And though regex is not something Domino developers work with regularly, there are tools close to home that can help.</p>","tags":["Domino REST API","XPages","Web"]},{"location":"blog/2025/05/01/drapi-cors-regex/#testing-regex","title":"Testing Regex","text":"<p>The Domino REST API documentation offers one option for testing regular expressions, regex101.com. But there's another option...within Domino Designer, as I've blogged about in the past. To do this:</p> <ul> <li>drag or add the XML for a Dojo Validation Text Box onto an XPages.</li> <li>go to its properties in the All Properties panel.</li> <li>scroll down to the data category and click into the <code>regExp</code> property.</li> <li>click on the icon that appears at the right side of the row with the hover help \"Launch external property editor\".</li> </ul> <p>The dialog that appears can help you construct a regular expression, but more importantly can help you test it.</p> <p>However, there are reasons to prefer the regex101 website instead:</p> <ul> <li>The quick reference on the website is comprehensive.</li> <li>The XPages dialog doesn't provide any support for start of string and end of string characters. You'll need to know those or find the help elsewhere - like the regex101 website.</li> <li>many of the radio buttons won't be relevant for constructing a regex for a url.</li> <li>\"current selection\" is what is highlighted in the regex box at the bottom, which may not be obvious.</li> <li>the Test Regular Expression page of the wizard just tells you whether it matches or not. The website provides more information.</li> <li>the help for DRAPI provides good examples that are easier to follow than manually construct.</li> <li>the XPages dialog requires you to always use Domino Designer. The website provides a standard website whatever your development framework or platform.</li> <li>the chat icon in the left-hand gutter of the website provides access to a Discord server.</li> <li>you will need to switch between <code>\\\\</code> for the JSON in your DRAPI configuration file and <code>\\</code> for both testers, so there's no advantage to either.</li> </ul> <p>The last point is important to point out. If you're copying and pasting from the code samples in the documentation to a tester, you'll need to change <code>\\\\</code> to <code>\\</code>. And when you paste back into your JSON file, you'll need to remember to change <code>\\</code> back to <code>\\\\</code>. That's because you're using one escape character (for regex) inside a language that also uses an escape character (JSON).</p> <p>Note</p> <p>The CORS settings apply to any application hosted by Domino REST API, including the admin UI. Domino REST API documentation calls out the need to add the server or domain in the CORS settings. It's important to understand the regular expression you're building, and the documentation does a great job of explaining the various parts.</p>","tags":["Domino REST API","XPages","Web"]},{"location":"blog/2025/05/15/shu-ha-ri/","title":"Shu-Ha-Ri","text":"<p>Some months ago Stephan Wissel pointed me to an article on Shu-Ha-Ri, which refers to Alistair Cockburn's Agile Software Development. It's well worth software developers being aware of the concept, because it is crucial to the developer's life journey.</p> <p>The concept of Shu-Ha-Ri, which dates back to Japanese Noh theatre, is used to demonstrate how you need to tailor your teaching to the level where the student is at:</p> <p>Shu: learning how to do a task and not worrying about the underlying theory. Ha: starting to learn the underlying theory and principles, and learning different ways to achieve an outcome. Ri: learning from their own practice, creating their own approaches, and adapting what they've learned to their particular circumstances.</p> <p>It's an interesting concept, one very relevant when working in a software community.</p>","tags":["Editorial","Coding"]},{"location":"blog/2025/05/15/shu-ha-ri/#a-padawan-amongst-masters","title":"A Padawan Amongst Masters","text":"<p>When I started becoming active in the Domino community around 2010, I very much surrounded myself with developers at the Ri stage. And anyone who has been around that community for many years will know the names I mean. But I was already blogging and speaking at conferences within months of starting blogging. Although my area of expertise when I was blogging was quite narrow at the time - mainly XPages - I believe I was already deep into the Ha stage myself.</p> <p>My first session at Engage 2010 was encouraging developers to move beyond the Shu stage of displaying view data in XPages, namely using View Panels, and to use the more advanced different ways to achieve that outcomes, Repeat Controls. And this was done by teaching about the underlying principles of displaying collections of data in JSF, namely Data Tables. My next sessions covered another underlying principle of XPages, Dojo.</p> <p>Looking back, it's very apparent I was soaking up their knowledge, learning deeper on the underlying principles of Domino, XPages, Java, and open source. Never was this more apparent than my sessions on the JSF lifecycle behind Java - the first time I gave the session I was due to deliver it with Tim Tripcony, but at least we finally did manage to deliver it together. Taking the opportunities to get involved in the XPages Extensions Library book and Domino REST API reinforced that. I was always going to progress to the Ri stage.</p>","tags":["Editorial","Coding"]},{"location":"blog/2025/05/15/shu-ha-ri/#ri-search","title":"Ri-search","text":"<p>After more than five years at HCL in the Labs organisation, it's become very apparent to me that being at the Ri stage - not just in one technology but many - is crucial to being able to be an effective researcher. Even with a new technology, I think I'm barely in the Shu stage. Almost from the start I'm trying to understand how the technology or product works under the hood, its strengths, its weaknesses. I'm already bringing in learning from adjacent technologies, and willing to try innovative approaches to achieve an outcome. It's key to getting quick, innovative outcomes. Just following the documentation and \"developing by numbers\" will leave you floundering before long.</p> <p>The Ri mindset also means flexibility, making different choices depending on different circumstances. Decisions can be made so quickly, taking into account such a variety of factors, that explaining why a decision is made is often difficult. Depending on the mindset, some Ri people can find it frustrating to explain their reasons - they may find it difficult to understand why others cannot see those reasons. The converse can happen, as Alistair Cockburn's gives an anecdote in his book, where the author was talking to a conference organizer who operated in Ri mode and didn't need the level of detail provided.</p> <p>The Ri mindset is also relevant to troubleshooting. I wrote what I still believe to be a very important blog post years ago on troubleshooting. The steps I covered there are ones I do instinctively now and have proven particularly useful recently. Working alongside a fellow Ri on a complex problem is very satisfying, when they can instinctively see where you're going and have similar levels of what they need and don't need to investigate or question.</p>","tags":["Editorial","Coding"]},{"location":"blog/2025/05/15/shu-ha-ri/#personality","title":"Personality","text":"<p>The Shu-Ha-Ri journey is not related to age or experience. A lot also comes down to personality and, probably, imagination. Some individuals prefer following the rules rigidly. Of course there are others who prefer to break the rules, but without the level of understanding from the earlier stages, resulting in regular mistakes in judgement. These avoid the Shu stage, but will not gain the understanding to reach Ri. Attaining Ri, it seems to be me, requires a level of discipline to learn the basics, but also the confidence to try new ways and review effectively. It takes a logical mind but also a creative one - a mixture, it seems, between the scientific and the artistic. Maybe it's something that can probably be learned and practised, maybe not. But it's key to achieving great things.</p>","tags":["Editorial","Coding"]},{"location":"blog/2025/05/18/engage-2025/","title":"2025 05 18 engage 2025","text":"<p>This week, once again, I will be speaking at Engage in Den Haag. As usual, it promises to be a great conference. And as usual, I will be busy.</p>","tags":["Conferences","Volt MX Go","Community","CSS","HTML","Web"]},{"location":"blog/2025/05/18/engage-2025/#wo4-volt-mx-and-volt-mx-go-workshop-monday-19th-may-1330-room-110w","title":"Wo4 Volt MX and Volt MX Go Workshop, Monday 19th May 13:30, Room 1.10w","text":"<p>Once again we have a workshop on Volt MX and Volt MX Go. Later this year, VoltScript will GA in Volt MX Go. The stats surrounding the work are not inconsiderable: </p> <ul> <li>four Early Access Releases across 18 months</li> <li>comprising development runtimes for three different platforms (Windows, Linux, and now Mac Silicon)</li> <li>eleven different VoltScript Extensions on Volt MX Marketplace</li> <li>five open-sourced VoltScript Library Modules</li> <li>an open-sourced reository of samples</li> <li>VoltScript Interface Designer, a tool to help architect VSEs and VSLMs</li> <li>extensive documentation which will also be open-sourced at GA</li> <li>plus videos coming soon</li> </ul> <p>One of the key messages I'll be highlighting this year is that VoltScript is just one tool in an extensive toolbag of micro-functions. The skill is understanding what's available and avoiding the instinctive approach of doing everything in script. We will also show how much can be achieved to create a delightful user experience.</p>","tags":["Conferences","Volt MX Go","Community","CSS","HTML","Web"]},{"location":"blog/2025/05/18/engage-2025/#de10-get-to-know-your-developer-community-wednesday-21st-may-1015-room-110-f","title":"De10 Get To Know Your Developer Community, Wednesday 21st May 10:15, Room 1.10 F","text":"<p>On Wednesday Maria Nordin and I will deliver a session diving deep into the HCLSoftware Digital Solutions Community. With only 45 minutes, there's no time to cover all the powerful functionality provided by the product and the customisations / extensions (both from the Discourse community and self-developed). But we hope to give enough information to enable you to orient yourself, maximise your personalisation of the site, and know where to go for more.</p> <p>And look out for content to help promote the site.</p>","tags":["Conferences","Volt MX Go","Community","CSS","HTML","Web"]},{"location":"blog/2025/05/18/engage-2025/#em03-engage-supreme-pro-code-mode-more-web-awesomeness-without-frameworks-wednesday-21st-may-1330-room-109-e","title":"Em03 Engage Supreme-Pro-Code-Mode: More Web Awesomeness Without Frameworks, Wednesday 21st May 13:30, Room 1.09 E","text":"<p>Later that day, Stephan Wissel and I will be delivering another session on what can be done with web standards alone. The interesting aspect is that last year, I was very much guided by Stephan on what we should cover. But this year, thanks to the personal investigation I've undertaken over the last year, I've been able to suggest much more of the content. The web development landscape has evolved significantly and continues to do so. Learning standards has always been a good thing, not only for future-proofing your skills but also for understanding how frameworks achieve what they achieve, and to know if a given framework is looking to stay modern or use out-dated workarounds.</p> <p>Of course I will also be around throughout the conference and I look forward to catching up with old friends and new.</p>","tags":["Conferences","Volt MX Go","Community","CSS","HTML","Web"]},{"location":"blog/2025/06/15/rust/","title":"Adventures in Rust","text":"<p>One of my core principles for IT research development can be summed up by a saying more than 2500 years old, attributed to Solon and appearing twice in Plutarch\u2019s Life of Solon: \u201cI grow old always learning many things\u201d. In many ways, to stop learning is to stop living.</p> <p>So when I need to build something I will not choose a technology or framework based upon what!s comfortable. But neither will I choose something new for the sake of choosing something new. I will consider what offers the most benefits but is also achievable. If that means stepping outside my comfort zone, if it means learning a new framework or even a new language, I\u2019m not afraid to do so.</p> <p>But I\u2019m also a realist and results-focused. If I\u2019m building a proof of concept and I don\u2019t have something to show in days, it\u2019s not an effective use of my time. The key to software development research is quick results that give a clear idea of the potential, that help identify problems or limitations, and demonstrate an approach that either can be used for a full deliverable or should be avoided. And that must never take weeks.</p> <p>So having the confidence to try something new is key, but only combined with delivering rapid progress. And that\u2019s what\u2019s happened with a recent project. Of course I\u2019m not going to go into minute details. This is about the \u201chow\u201d not the \u201cwhat\u201d, and there are still a lot of lessons worth recording.</p>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/15/rust/#eating-the-elephant-part-one","title":"Eating the Elephant Part One","text":"<p>In problem solving there\u2019s a well-documented approach of chunking the elephant. This means breaking a problem down into smaller pieces. But the key here goes back to a blog post from last month on Shu-Ha-Ri, namely understanding how something works rather than just using it. If you understand how technologies and adjacent technologies work, if you see something as a set of steps in a process rather than a single action, you are more likely to be able to construct a similar process yourself.</p> <p>And when it comes to learning a new framework or technology, you\u2019re able to identify the parts you need to build. That was my approach and before I had even written a line of code I had a set of tasks written down for four reasons.</p> <ol> <li>So I didn\u2019t forget. This was estimated as several weeks of work, providing multiple features.</li> <li>To allow me to place estimates against each and track.</li> <li>To identify how each part might potentially be achieved before a line of code was written, but also to identify what I didn\u2019t know how to achieve.</li> <li>To demonstrate to others the features I expected to deliver and the approach I was taking, to manage their expectations on actual working code.</li> </ol>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/15/rust/#why-rust","title":"Why Rust","text":"<p>The proof of concept was not being built on anything else, so there was no language or framework that needed to be used. There were similar solutions that might provide a template, but none would provide sufficient code to be a starting point.</p> <p>In terms of technology choices, there were two: Java and Rust. JavaScript might have achieved it and might have been chosen by someone with a strong affinity for it, but offered no obvious advantages. The choice of Rust came down to five reasons:</p> <ol> <li>External influences, and I won\u2019t say more on that.</li> <li>Lower-level, so better performing, potentially.</li> <li>A colleague recommended an approach for one aspect, and that approach was most consistent with Rust.</li> <li>I was able to easily identify libraries that helped achieve specific requirements.</li> <li>If the proof of concept was replaced with anything, it would be something more adjacent to Rust than Java. There are several reasons it might stay in Rust and some it may change. But Rust was the correct choice in order for me to be productive.</li> </ol>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/15/rust/#background-and-assistants","title":"Background and Assistants","text":"<p>So Rust was a language I have been aware of for some years, I\u2019ve lurked in Discord chats, and did a self-paced course a couple of years ago. But no opportunity presented itself to build anything useful with Rust.</p> <p>When I first learned Java in XPages, my progress was significantly improved through the help of a colleague in the open source community, Nathan T. Freeman. I had no \u201csmartest person in the room\u201d for Rust, but I had found GitHub Copilot helpful in code generation, explaining and fixing.</p> <p>But it became apparent very quickly that I needed to undertake a refresher course. Even then, Copilot has been a considerable use. For example, the course covered Results, traits, and <code>match</code>. But I have not been too proud to use Copilot to fix my code to get the syntax right, and I\u2019m not ashamed of my use of Copilot. It\u2019s undoubtedly and significantly improved my productivity. But it\u2019s been combined with a mindset that means I understand the code I have, I know what it\u2019s doing, I identified where better error management needed adding and was capable of writing it. And as the project progressed I was more and more able to write running code without needing to ask Copilot.</p>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/15/rust/#eating-the-elephant-part-two","title":"Eating the Elephant Part Two","text":"<p>When it came to creating the POC, this is also where chunking the elephant proved the right approach for maximum productivity. Of course I needed configuration files, and a degree of flexibility in the functionality that increased complexity, I needed cache management across threads, and of course I need logging and unit testing. Plus there\u2019s a layer in from of the Rust application which potentially added - and in reality did indeed add - scope for error beyond my code. But if I had tried to add all that in at the start, it would have been weeks before I had a single demo. That\u2019s not my way and I firmly believe it would have produced messy and over-complicated code, with countless failings.</p> <p>The first step was running a hard-coded piece of Rust to provide a very basic starting point. And I had that built as a demo on day one. Next was calling that from the layer in front, which again I had as a demo on day one. Functionality was added piece by piece, with the focus being multiple demos every day.</p> <p>This provided two additional benefits. Firstly, regular and quick wins. These boost confidence and a feeling of progression. But they also make development fun. Secondly, if you\u2019re adding one piece at a time or replacing one bit of hard-coded functionality with a bit of flexibility, you come up against fewer scenarios where you\u2019re fighting with code for hours. These are the times when you become mentally exhausted and it affects not only that piece of functionality, but the ability to think clearly and effectively for the rest of the day. This results in bad code and stop development being fun.</p> <p>Even though this has been a new language, and about two-and-a-half weeks of development, I\u2019ve probably had no more than two occasions when I\u2019ve been fighting with code and become tired. It means I have achieved immense progress which has impressed others as well as me, and achieved what I initially estimated would take twice as long. And it\u2019s opening up wider potential than I had considered.</p>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/15/rust/#conclusions","title":"Conclusions","text":"<p>But this is a proof of concept only. It\u2019s not guaranteed to go into a product, but nothing in the code will prevent it going into a product, if it should need to. And if it gets thrown away or put on a shelf and never used again, it\u2019s less than three weeks work and provides a reference point for anything else where I need to use Rust. Return on investment is important to me, I value my time, and this has been another good use of my time whatever happens. It\u2019s further boosted my confidence to achieve and it\u2019s expanded my toolset.</p>","tags":["Rust","Coding","Web","REST API"]},{"location":"blog/2025/06/30/rapid-development/","title":"Developing at Speed","text":"<p>One of the main outputs of research development is the proof of concept. An early lesson I picked up when I joined HCL Labs was to deliver working code, not slides. And the key when building a proof of concept is speed. In some cases, it may end up proving why an approach won't work. In many scenarios, it may end up being put on a shelf indefinitely. Even if the concept proves appealing, the implementation choices may not be the preferred option for the final solution. So speed is of the essence: spending a couple of weeks building something that goes nowhere is acceptable; spending a couple of months is not. So the ability to get maximum results in the minimum time is key.</p> <p>But how do you do that?</p> <p>Recent experiences have reinforced confidence in my ability to achieve this, to such an extent that the speed of my development surprised even me. It's led me to reflect on my approaches and draw some conclusions. The conclusions are not just relevant to building proof of concept applications, but are relevant to all development. Indeed many of the lessons are ones I've been using and talking about for many years.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#planning","title":"Planning","text":"<p>If you fail to plan, you are planning to fail.</p> <p>This is a well-known quote often attributed to Benjamin Franklin, but there is a lot of truth in it. The planning phase is the first and an important step in developing at speed. Is it the most crucial? Not necessarily. But a bad start is never a good omen.</p> <p>Does it need to be done on paper? Not necessarily. It depends on the complexity of what you're building and your ability to understand it at a macro and micro level. An individual with a Ri mindset may already have grasped all aspects we'll cover shortly. But in can be helpful to document all the requirements, both to ensure something's not forgotten during the development process and ensure it's documented in case something else takes precedence.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#structure","title":"Structure","text":"<p>Firstly, you're not building something. You're building a bunch of somethings that come together to comprise a bigger something. You need to understand the separate pieces in what you're building and you should break everything into small bite-size pieces. You need to identify those early on.</p> <p>In addition, you should also consider what additional functionality might be needed, considering its importance and urgency. These are importantly not things you're going to build, but things that might end up needing to be built.</p> <p>Most importantly, you're looking at functionality here, not technology. You're defining the \"what\", not the \"how\"; the functions that need to be delivered, not the technology or framework, not necessarily even the language you'll use. And everything may not even use the same technology. You need to focus just on what the proof of concept needs to do, not how it will do it. Until you know all the moving parts, you should not commit to a particular implementation.</p> <p>This last point is particularly important, and a mistake I often see in software enhancement requests. Too often they focus on a specific technology choice or implementation approach. And sometimes those particular choices are not suited to the requirement or will prohibit key functionality that has not been identified. That's because the individual was focusing on a specific technical solution for a requirement that was too narrowly defined, focusing on the how instead of the what.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#understand-the-options","title":"Understand the Options","text":"<p>PLURAL. In my blog post about troubleshooting I also focused on the importance of considering multiple potential causes. The same is true here. Once you've identified all the \"whats\" you need to build, it's important to consider multiple \"hows\" for each. I regularly describe development research as problem solving, as throwing a lot of balls into the air and looking for the best balls to use to get from A to B.</p> <p>And there are multiple factors I take into account when I'm looking at potential options:</p> <ul> <li>ease to build</li> <li>off-the-shelf options</li> <li>ease to deploy</li> <li>skill-sets of those involved now or in the future</li> <li>standards and best practice approaches in other implementations</li> <li>vibrancy of relevant communities</li> <li>scalability, but only if relevant</li> <li>future-proofing / technical debt</li> <li>loose or tight coupling</li> <li>flexibility</li> <li>external factors</li> </ul> <p>I want to go into more details on a few of the aspects here.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#ease-to-build","title":"Ease to Build","text":"<p>It's important to get an idea of the complexity to build certain functionality. This isn't about giving firm timescales, although when doing customer-specific work there might be that requirement. It's about identifying where you might need to do a bit more investigation up-front, or where you need to do additional investigation later. It's also about identifying the order you'll work on pieces of functionality. A specific piece of functionality may be very important, but it's easy to build a quick throwaway alternative which can have advantages when it comes to the development phase.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#off-the-shelf","title":"Off the Shelf","text":"<p>Firstly, in all aspects of development, writing something from scratch will give greatest flexibility. But it will often be slower to build and harder for others to work with. Moreover, the more complex the functionality, the greater the likelihood of reinventing mistakes others have already learned.</p> <p>Of course this can be mitigated if you understand how alternatives work, whether they be in the same or other languages. But personally, the \"not invented here\" attitude is not one I've chosen or considered productive. If something does the job, has decent documentation, is well-considered, is structured well-enough for me to understand, and gets the job done quickly, I'm happy to use it and move on.</p> <p>Will it be perfect? Probably not.</p> <p>Will my version be perfect? Certainly not. And I guarantee, neither will yours first time.</p> <p>If I'm looking at open source, I'm not necessarily looking at number of downloads or git stars. Number of downloads is different to number of production uses. Git stars doesn't equate to relevance in the current climate. I'll be impressed if the documentation is decent, gets me productive quickly, and answers my related questions. If there's a vibrant community around the solution or technology, that also is important. But I'm also looking at level of technical debt and future-proofing. Are there techniques, approaches, or frameworks that are becoming out-dated? Can I understand the code well enough to support or extend it myself. Is it something that I think will need replacing in a few years? If it's not the ideal solution, can it be loosely-coupled so that a better alternative or a custom solution can be switched in later?</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#flexibility","title":"Flexibility","text":"<p>It's important to stress what flexibility is not.</p> <p>I don't mean soft-coding everything, making everything configurable, or using configuration files for everything.</p> <p>I'm talking about flexibility of use. A proof of concept may sit on a shelf indefinitely or may be picked up for something completely different a few years later. Or part of the solution may be relevant to a different project or product. The key here is not enabling flexibility, but not preventing flexibility: leaving as many avenues as possible open for the future.</p> <p>There will be decisions you make that will preclude certain future opportunities. The key is being aware of where lack of flexibility is being included, identifying the level of risk, and minimising those that will have the worst impact.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#others","title":"Others","text":"<p>Technical debt and future-proofing most definitely fit into this flexibility. This is not just using modern frameworks or standards. It's also being aware of other projects happening, other products where the proof of concept may be relevant, and business and technical trends. Using an outdated technical approach because you're familiar with it may get quick outcomes in the short term. But adopting something more modern gives experience and sample code that will be relevant for other projects, as long you're able to be productive.</p> <p>Understanding trends is something that's more difficult and something that may not be easy for detail-focused developers. It requires a broader focus, the ability to look at consistent themes across different areas or timescales, and an awareness of what's not said as well as what is said. But if you can identify trends, you're more likely to make decisions that have greater flexibility than a narrow use case.</p> <p>And there will also be external factors that need to be borne in mind. This might be skill-sets of people available, technological preferences from interested parties etc.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#dont-be-afraid-to-not-know","title":"Don't Be Afraid To Not Know","text":"<p>You don't have to have all the answers. There may be particular parts of the process for which you don't have a perfect answer - or maybe even no answer at all. It can be useful to have excluded certain options for a specific piece of functionality. If you need a certain prerequisite before a solution is possible, that is useful information that can be factored in. Maybe the overall understanding can still be conveyed by using mocking or hard-coding. Or you may identify a viable solution later in the process.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>With the various options for each part in the overall solution identified, you can assess the advantages, disadvantages, and limitations of each. The right option for a proof of concept may be different to the right option for a final product. Limitations may be acceptable or not. Increased flexibility may make an option for one part more preferable. The more loosely-coupled each piece in the puzzle is, the easier it will be to adapt in the future. And it may be quicker to develop as a result, because mock inputs / outputs can be used by different teams or developers.</p> <p>The important point here is that this is a first pass. THe crucial part is to keep reviewing throughout the development process and not to be afraid to change your mind. Having multiple options makes this easier and quicker. The earlier you're able to change, the shorter the distance you've travelled down a dead end, the less mental effort will have been expended on the development and the less mental effort you'll need to expend on deciding on an alternate approach.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#development","title":"Development","text":"<p>Now it's time to start the development. There are some key lessons I've learned over the years and ones I used very recently that significantly reduced development time.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#build-incrementally","title":"Build Incrementally","text":"<p>As I mentioned at the start of this post, one of the important lessons I learned early in HCL Labs was to show running code, not slides. Rarely does a week go by when I don't demo something and I'm not happy if I don't have something to demo after two weeks. I'm keen to highlight also where this demo fits into the process of the overall development.</p> <p>So at the heart of my development, and at the heart of developing at speed, is building piece by piece. Of course this comes from the planning phase, where I've broken it down into smaller pieces. I'll add more functionality bit by bit. An added side-effect of this is it gives regular touchpoints to assess progress against expectations, assess what's left, and review existing and additional limitations or opportunities.</p> <p>The timeframe is significant here. I'm not aiming to complete a milestone each week. I'm looking at adding a new piece of functionality every day or two at most. That is the level at which I've broken the development down at the planning phase, and the level of velocity I'm aiming at.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#hard-code-then-add-configurability","title":"Hard-Code, Then Add Configurability","text":"<p>Hard-coding is bad, right? Wrong! For a proof of concept, it's perfectly acceptable. Even for a bigger development, hard-coding enables you to get more complex functionality working successfully and, most importantly, with greater simplicity. If it doesn't work, there are fewer places where things can go wrong. And even if you assume the soft-coding and configurability to built in is not the problem, you will be wrong sooner or later. And while you're wasting hours and getting exhausted and probably asking colleagues to help you, I'll be demoing the final solution.</p> <p>Minimise the new code, minimise the places mistakes can happen, and progress will be quicker.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#measure-twice-cut-once","title":"Measure Twice, Cut Once","text":"<p>Ensure your code can be tested as granularly as possible. Some will prefer pure functions to minimise the risk, I'm personally not a purist (forgive the pun!). But if something doesn't work as expected, I will ensure I can identify with certainty where it's going wrong as quickly as possible. This may be through debugging hitting a specific breakpoint, this may be by cross-checking from multiple directions. For example, with a REST service, I'll test from bruno as well as the application I'm building.</p> <p>Source control can help identify regressions and for that reason I ensure each commit tries to include all files affecting a particular piece of functionality, and ancillary cleanup is in its own commit. Unit or integrations tests can help identify regressions as well. These might be coded tests or might be bruno requests.</p> <p>But the key throughout is speed. If it takes a long time to identify the cause of a problem, that increases mental exhaustion which increases the risk of more mistakes which slows down development. Looking longer term, it also means more time spent on support, which increases the risk of unfixed bugs and lower ROI.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#increase-complexity","title":"Increase Complexity","text":"<p>At your planning phase, you should also have got an idea of what was easy and what was difficult. Get your quick wins in, demonstrate progress, build the easier parts and add the more complex functionality later.</p> <p>It's not just about minimising new code at each new step, it's also about minimising complexity. Your planning phase may have identified options that were simpler but no use for a final solution. That doesn't mean you shouldn't code them.</p> <p>In the case of a proof of concept, a less feature-rich option may have no perceptible difference to the audience. If it takes a fraction of the time, even if it needs to be replaced later, the proof of concept has still achieved its aim.</p> <p>Even where it's not fit for final purpose, choosing the simpler option can still make a lot of sense. You may have identified a two inter-related parts that are both complex. Trying to get both right at the same time will be challenging, fraught with risk, and likely to slow you down. In the planning phase you may have identified an alternative option for part B that is simpler, but of no use for the final solution. I've found real benefit from developing the whole things with the production-ready option for part A and that simple option for part B. It can allow you to focus on getting a working outcome quickly, and minimising the changes you make to then integrate the more complex option for part B.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#standalone-first","title":"Standalone First","text":"<p>At some point, the codebase will grow and you have another piece of complex functionality to add, something you expect to get wrong first. Using hard-coding and less complex functionality may not be enough to avoid risk. At this point, I've found considerable benefit in building the functionality standalone first, working through all the complexities separately, then trying to integrate it into the full codebase.</p> <p>This builds on everything we've seen so far: increasing complexity, giving a way to test standalone and cross-reference with the full codebase, and minimising new code.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#fail-early","title":"Fail Early","text":"<p>Failure is not a bad thing. But failure after days of struggling is bad. At that point, you're mentally exhausted, you may have introduced bugs into the code, or made changes that you've forgotten and which stop alternatives working.</p> <p>Again, this comes back to identifying multiple options in the planning phase. It comes down to having alternative ways to test. It comes down to minimising complexity.</p> <p>The sooner you can identify a failure to achieve your aim on a particular part, the better. The solution may vary. In the case of a proof of concept, some hard-coding or a less optimal option may be imperceptible when it's demoed. So go with that and move onto something more important. An alternative that seemed harder may end up being easier to implement. Or you may now see it as more preferable, when you review it some time later. A less optimal alternative may still be an acceptable stop-gap until the preferred option can be made to work. It may be that you can get it working just a day or two later, it may be that someone else can succeed where you failed. The key is progress.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#summary","title":"Summary","text":"<p>All of this comes down to minimising risk and focusing on progress.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#review","title":"Review","text":"<p>My job title has usually used the term \"developer\". That's because I've never just coded to a technical spec, I've always been developing the application. That means reviewing the spec and reviewing the options while I do the development. This is crucial and it's why I specifically stated the planning phase was a first pass.</p> <p>We've already seen it addressed in failing early. But it's a common case where I identify additional requirements, limitations, or flexibility as I'm developing. What seemed to be the right or wrong option initially may change as development progresses. Or it might be affected by external factors.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#understand-limitations","title":"Understand Limitations","text":"<p>It's also important to review, understand, and accept limitations. There is a difference between a proof of concept, a minimal viable product, and a complete solution. A limitation may cease to be a limitation because of changes elsewhere. Or something that seemed fine may subsequently become a limitation. And if it's a proof of concept that just gets put on the shelf and never used again, or functionality that gets delivered but never used, limitations are moot.</p> <p>Knowing when to stop is important. Because there should always be another project.</p>","tags":["Coding","Editorial"]},{"location":"blog/2025/06/30/rapid-development/#overall-summary","title":"Overall Summary","text":"<p>There is a lot here. But there are a number of key takeaways I always bear in mind:</p> <ul> <li>Planning is crucial, whether it's in your mind, bullet points in your notepad, or a few slides.</li> <li>Developers are paid to write code, so have running code and demos.</li> <li>Having all problems solved is not critical. Solutions may come later if you review regularly, just as more problems will crop up.</li> <li>Ensure you can test parts of a process in more than one way.</li> <li>Minimise complexity, minimise what's new.</li> <li>If a problem is not overcome in a couple of hours, and you don't understand the problem, try an alternative.</li> <li>Constantly review.</li> </ul>","tags":["Coding","Editorial"]},{"location":"blog/2025/08/05/drapi-agents/","title":"Supercharging Input to Domino REST API Agents","text":"<p>One of the things I learned when building HCL Volt MX LotusScript Toolkit was that calling a web agent with <code>?OpenAgent</code> URL populates the <code>NotesSession.DocumentContext</code> with various fields containing useful information from the request. So when I was building agent processing functionality into the POC that became Domino REST API, I utilised the same approach to provide opportunities to pass contextual information across to an agent.</p>","tags":["Domino REST API","LotusScript"]},{"location":"blog/2025/08/05/drapi-agents/#tester-agent","title":"Tester Agent","text":"<p>Maybe someone has tried checking what's in <code>NotesSession.DocumentContext</code>, but I've not seen anyone from the community mention it. But a simple agent will allow you to access the content passed into the agent. Just put this into a LotusScript agent:</p> <pre><code>Option Public\nOption Declare\nSub Initialize\n\n    Dim s As New NotesSession\n    Dim doc As NotesDocument\n    Dim item As NotesItem\n    Dim resp As String, itmVal As String\n    Dim firstDone As Boolean\n\n    Set doc = s.DocumentContext\n    resp = \"{\"\n    ForAll itm In doc.Items\n        If firstDone Then\n            resp = resp &amp; |, |\n        Else\n            firstDone = True\n        End If\n        Set item = itm\n        resp = resp &amp; |\"| &amp; item.Name &amp; |\": | \n        itmVal = CStr(item.Values(0))\n        If (itmVal Like \"{*}\") Then\n            resp = resp &amp; itmVal\n        Else\n            resp = resp &amp; |\"| &amp; itmVal &amp; |\"|\n        End If\n    End ForAll\n    resp = resp &amp; \"}\"\n    Print resp\n\nEnd Sub\n</code></pre> <p>This will basically get <code>NotesSession.DocumentContext</code> and iterate the items and construct a JSON object containing all items and their values.</p>","tags":["Domino REST API","LotusScript"]},{"location":"blog/2025/08/05/drapi-agents/#testing","title":"Testing","text":"<p>You can access this agent by enabling the agent in the schema and ensuring a valid trigger - certain triggers can only be used within the Notes Client, so the easiest is \"Agent list selection\" as the Trigger and \"None\" as the Target.</p> <p>If the agent is called \"TestAgent\", you can access it with this curl command:</p> <pre><code>curl --request POST \\\n  --url 'https://YOUR.SERVER.COM/api/v1/run/agent?dataSource=demo' \\\n  --header 'authorization: Bearer YOUR-TOKEN' \\\n  --header 'content-type: application/json' \\\n  --data '{\n  \"agentName\": \"(TestAgent)\"\n}'\n</code></pre> <p>Alternatively, you can go to the \"OpenAPI v3\" tab on your Domino REST API dashboard, by selecting \"basis\" and then the relevant scope from the drop-down. You will then be able to test it from the Code - /run/agent request (after authenticating, of course).</p>","tags":["Domino REST API","LotusScript"]},{"location":"blog/2025/08/05/drapi-agents/#parameterisation","title":"Parameterisation","text":"<p>Amongst other content, you will see \"KEEP\" in the \"REQUEST_METHOD\" field. This can be used to code agents or LotusScript Script Library business logic differently depending on whether it comes from Domino REST API or other clients.</p> <p>You could add a check to prevent these agents being called via the <code>?OpenAgent</code> URL command, but agents are still accessible to Domino REST API even if they are hidden from the web. So you can just hide the agent from the web, unless you want it to be accessible from the web or unless the whole application is prohibited from URL open.</p> <p>You will also see all sorts of fields passed into the DocumentContext that are prefixed with \"HTTP_\". It will be apparent that many of these are the HTTP headers passed into the request. So of course you can pass custom headers as a way of passing additional information into the agent.</p> <p>But there is also another way to pass additional information into the agent. In the body, you have to pass a JSON object with the property \"agentName\". But you can also pass a property \"payload\", which needs to be a JSON object. An example JSOn body would be:</p> <pre><code>{\n  \"agentName\": \"(TestAgent)\",\n  \"payload\": {\n    \"requestId\": \"test\",\n    \"requestNo\": 1234\n  }\n}\n</code></pre> <p>The contents of payload would then be available in the \"REQUEST_CONTENT\" field as a string, for you to use as you see fit.</p> <p>So there are two ways to pass content to use within your agent, is you wish to do so.</p>","tags":["Domino REST API","LotusScript"]},{"location":"blog/2025/08/16/ai-lessons/","title":"More AI Lessons","text":"<p>A little while ago I blogged on developing at speed. The obvious omission from all aspects was AI. But AI \u2013 like an IDE \u2013 is just a tool. Unless you understand what it can and can't provide, unless you use it intelligently, you will not reap the benefits. But unlike an IDE, AI doesn't come with a set of menus that hint at what it can and can't do. AI doesn't come with a marketplace of extensions that provide functionality shared by the community. And it's so new that we're all working it out. So what are my thoughts?</p> <p>Matt White blogged about this recently as well, and I agree with many of his points. Most of all, I agree that it will not replace either junior or highly skilled engineers. It's just another tool, like an IDE or build systems: in the hands of the right people it can provide significant performance boosts and avoid the mind-numbing aspects like poring through documentation or hunting through a host of technical posts; in the hands of the wrong people, it can provide even more significant productivity losses and leave your skilled engineers wanting to throttle those who have forced them to spend hours trying to understand and undo the deleterious impacts of dumb code!</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#contextual-background","title":"Contextual Background","text":"<p>To give a bit of contextual background to my experience with AI, my use has predominantly (almost exclusively) been in Visual Studio Code, using a couple of models (GPT-4.1 and Claude), initially in \"Ask\" mode, now mostly in \"Agent\" mode. My use of Copilot began nearly 18 months ago and has been used in a variety of projects, a variety of languages, and multiple frameworks. I covered some of the scenarios I use it for last year. That work continued, including investigating how suggestion results could be improved. But it's been complemented more recently with coding in technologies and with concepts I've been less familiar with. As I've done that, my use of AI has become more sophisticated.</p> <p>That's what I want to focus on here.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#understand-limitations","title":"Understand Limitations","text":"<p>My initial use of Copilot with Rust was very positive. Subsequent experiences were less positive. But there was a simple reason: the framework and more specifically the APIs from that framework that I was initially using were well-established and had not changed for some time. The later experiences were on APIs that had changed significantly and frameworks that were still evolving. This resulted in AI hallucination.</p> <p>The AI-phobes will immediately use this as an excuse for not using AI. The skilled engineer will adapt their usage of AI to maximise performance. There are a variety of techniques that can be employed to improve outcomes, and the right combination will vary.</p> <p>\"Agent mode\" has been helpful because unlike \"Ask\" mode, it doesn't stop after the initial response. This can provide code, check compilation, and adjust accordingly. One of the key aspects is informing AI of dependency versions in use, which it can identify from config files \u2013 although it may not proactively do it. I've found Claude much better than GPT-4.1, but sometimes still not right.</p> <p>The key here is tolerance \u2013 anticipate where it might struggle, if you can, or at best accept it. It's not API completion, it's not designed to do that or even use it the way a developer does. AI is not a tool designed for software developers like API docs and API introspection in language servers are. Wake up and smell the coffee \u2013 you don't pay sufficient money to justify an AI specific for your framework or language or even software development in general. The only reason it expects is that investment justifies a jack-of-all-trades, which will never be the absolute expert for everything you or any other individual wants. If it fails, try to give it the information it needs to succeed. If it still fails, try to understand why it fails, fall back to \"traditional\" software development approaches (reading API docs and coding on your own!), but be willing to use it where it's more likely to succeed.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#making-introductions","title":"Making Introductions","text":"<p>We call it AI, but it's really natural language processing backed by an LLM (Large Language Model) and, potentially, tools and resources. The big strength of AI is ingesting a large corpus of information previously gathered and providing a succinct analysis. And this is where it can provide big benefits.</p> <p>I've gained benefits from asking AI for a high-level explanation of concepts I'm not familiar with. My traditional approach would have been to search the internet and read a variety of articles. AI can give a more succinct summary which is \"probably\" right. The key then is to approach it with intelligence and healthy scepticism. What \"sounds\" correct? Use that with the caveat that you might need to verify it later. For the rest, be willing to challenge, ask for and investigate sources, and cross-reference it with web searches. This is not rocket science, it's standard for higher education academic research.</p> <p>It's also important to think about what's omitted. Are there aspects where you need more detail, more information.</p> <p>I've also used AI to choose what libraries are available to provide certain functionality, or what the pros and cons are for various options. This provides quick contextual information to help make a more informed decision. It's important to highlight that AI doesn't know you, so asking for \"the best\" doesn't provide the key information to decide the right choice for your particular requirements. Asking for pros and cons can help you make the right decision, and might help you ask additional questions with additional contextual information.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#getting-it-right-first-time","title":"Getting It Right First Time","text":"<p>Code generation is a given for AI usage. But that's not the only benefit when writing code. I've found AI very useful to explain unfamiliar syntax or verifying assumptions about what APIs do. \"If I use this API in this way, with these inputs, what will be the outcome?\" Or if you want to \"lead the witness\", \"will I get this outcome and, if not, how do I get this outcome?\"</p> <p>Historically I would have written code, tested it, and adjusted accordingly. Sometimes it may have required lots of additional coding to get to a position where the scenario could have been tested, and I would need to remember to test when I got to that point later. Other developers just post the question on a community chat or forum somewhere, hoping for an answer to a query that may not even have provided all key information. Asking AI can be a more effective solution, not least because the AI can actually see all the related code you've written unless those community chats.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#improving-code","title":"Improving Code","text":"<p>Matt White talked about his experiences of using AI and I've found significant performance benefits on writing unit tests even for VoltScript, a language for which there are limited resources available. Matt also documented the challenges in trying to improve code by a more complex refactoring. But there are more granular examples where I've used AI to refactor code.</p> <p>Way back last year I pointed Copilot to code and asked summary of Voltscript code complexity, both cognitive complexity and cyclomatic complexity. More mature languages have integrations with code quality tooling, but obviously that's not possible with VoltScript in Visual Studio Code, which will not even be GA until later this year. We could build that, but it's not a quick task. AI fills that gap and can identify places where code can be improved. Maybe it could even be used to rewrite it to minimise complexity.</p> <p>More recently with Rust, I've posed questions like \"is this the correct way to...\", \"what is the best practice for doing...\", \"is there a more elegant way to...\". This can also be used to help introduce more advanced techniques into your code, educating you on how to use them correctly and build knowledge to use them first time next time.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#garbage-in-garbage-out","title":"Garbage In, Garbage Out","text":"<p>A key aspect is one Matt learned quickly and highlighted: specificity. If questions are too open-ended or not specific enough, you get bad output. Using an AI chat in-IDE can minimise some of that risk of not providing key information. It's one I've seen very often on chats and other forums - not stating which version of software is in use, not referencing key environmental or adjacent factors that are crucial to understanding your outcomes. But it's important to be able to provide the relevant information in a succinct manner, ask targeted questions, not distract the AI from what you need. This is a skill.</p> <p>I'm constantly surprised that some IT professionals, whose day job is providing support, can be so bad at providing the information needed to support them. But it's more true with AI than anywhere else, that the quality of input directly correlates to the quality of output. That does not mean good input guarantees good output, but bad input definitely increases the chances of bad output.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/08/16/ai-lessons/#summary","title":"Summary","text":"<p>Hopefully I've provided some ideas about how to maximise your success with AI. There's a follow-up article due, at a higher level, because in many ways we've been here before. And some potential mistakes and lessons learned are ones we can identify from comparable technologies and IT approaches of the past.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/","title":"Effective AI Usage Part One - What is AI?","text":"<p>In my last blog post I talked about some lessons I've learned from using AI. I talked about a follow-up article talking about AI use at a higher level. Recent experience has reinforced my thinking on this. In this blog post we're going to focus on what AI is, the initial interaction, and training.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/#what-is-ai","title":"What Is AI?","text":"<p>First off, \"AI\" is a misnomer. This isn't the first time that I've talked about the importance of understanding the processes behind how technology works. It's the same with AI, we have multiple aspects in play, and it's evolving. We have the interface, but we need to understand what it allows and doesn't allow access to.</p> <p>First off, there's the LLM (Large Language Model). GitHub Copilot offers a limited number of LLMs to choose from, the most common ones I've used are GPT-4.1 and Claude Sonnet 3.5. Later in the year Copilot will allow BYOM - Bring Your Own Model, but that's not available yet. And different models are suited for different things. Alternative extensions like Continue and other IDEs allow you to integrate with different LLMs, local or cloud.</p> <p>Local LLMs depend on suitable hardware, usually including a GPU. This isn't something your average end user is going to have access to, although ARM-based Macs and Windows laptops with a very recent Intel chip have started to include GPUs. However, the LLM and something like Ollama or LM Studio need to be downloaded locally. And the more powerful the large language model, the bigger it is. Specific LLMs are good for specific tasks, e.g. coding. So one LLM may not even be enough.</p> <p>There have been attempts to distribute the work of LLMs, for example clustering hardware to create an AI hub. The problem here is the performance of communication amongst the cluster.</p> <p>Then there's RAG, Retrieval Augmented Generation, which can add additional specialist information into the mix. RAG uses a vector database, which takes inputs and \"vectorises\" the content using an embedding model. The choice of embedding model will affect how the data is stored, but the same embedding model is needed for both loading and reading. When retrieving content, the LLM takes the input and retrieves a subset of documents from the vector database based on a similarity search. The number of documents retrieved is a variable passed to the retriever - the higher the number of documents, the more potential matches but also the more contextual information the LLM needs to take into account. And depending on what's in the vector database, too much may include a lot of useful information.</p> <p>There is also CAG, Cache Augmented Generation, which can be used to avoid calling the LLM by identifying questions already asked and answers already available. However, it depends on the quality of those answers.</p> <p>Then there is MCP (Model Context Protocol), which also has two parts: an MCP Client, which the interface needs, otherwise you can't use MCP in that interface; and an MCP Server which can provide prompts, tools, and resources. These can be included in the process to bring additional specific information into the mix.</p> <p>Then there are AI agents, which can add automation into the process. Copilot can run in agent mode, performing iterative code updates.</p> <p>There are also IDE integrations, like how Copilot can read code in editors and files, run terminal commands and read terminal responses, and update code in the editors. These could be considered tools.</p> <p>It's important to understand these moving parts, in order to understand where effective use of AI needs to be optimised and where it can go wrong. But there's another moving part involved: humans. Unfortunately humans are involved in every parts of the process...as well as another part I've omitted to mention - using AI. And that's where I'll start.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/#weve-been-here-before-part-one","title":"We've Been Here Before Part One","text":"<p>My most recent usage of AI did not go well. What I was trying to ask may actually be impossible, but it's not a priority to confirm or deny at this point. As I covered in my last blog post, being specific is key. And I thought I had been specific enough. The ask was for configuration that:</p> <ol> <li>Created a new file when the program started.</li> <li>Rolled over when file size exceeded.</li> <li>Retained old logs.</li> </ol> <p>But there are two major problems with this. There is a difference in English between \"when\" and \"every time\" or \"whenever\". And when does the current log become old? In precise English terms, what AI did was what I asked for - create a new file only the first time the program started, and then retaining foo_1.log and foo_2.log but not the current foo.log file. The problem was what I asked for and what I wanted were not the same.</p> <p>Because whether you accept it or not, the English I used was imprecise and I didn't include sufficient examples to illustrate my requirements.</p> <p>We've been here before.</p> <p>Remember the days of IT off-shoring, with business analysts creating spec for cheaper IT \"code monkeys\" to do the work? AI is just an extension of IT off-shoring, and the same problem is rife - poor communication skills and lack of clarity. It results in a solution that does what's asked for and no more, even if that results in something that makes no sense.</p> <p>I've seen the same in enhancement request systems, where an empty text box is given without even a hint at the amount of content required. Sometimes you get a one-liner, sometimes something that's so imprecise it becomes a Schr\u00f6dinger's requirement - simultaneously shipped and not shipped, because both sides can argue what it could mean and no one knows what it should mean!</p> <p>South Park's recent episode, season 27 episode 3 \"Sickofancy\", wittily riffs on this idea of ChatGPT, poorly thought out requirements, and poor review of the answers. It's a good commentary on the use of AI by those not qualified to use it effectively.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/#weve-been-here-before-part-two","title":"We've Been Here Before Part Two","text":"<p>If you have something more than one-shot AI, there's a whole process in the middle where a conversation takes place between the human and the LLM. This constitutes a whole blog post on its own, so I'll come back to it.</p> <p>But the RAG depends on the quality of the data that goes into it. And we've been here before too, most notably for AI in the era of machine learning which came to prominence around the turn of the century. It didn't really take off in enterprises, often failing in pilot stages, for a simple reason: businesses were not willing to invest sufficient time to train ML systems on data. I've also seen the same with OCR tools in the pre-AI era. It takes time to find good-quality samples, train the systems effectively, avoid issues caused because of bad data. And it's time that needs to be spent in addition to the day job.</p> <p>And the initial training isn't the end of the story. There needs to be ongoing training. That still seems to be a nascent aspect of AI, training beyond the initial conversation to ensure better and quicker results next time. Again, this needs to be done in addition to the day job - and with no immediate benefit: the benefit comes from planning for the future. And fail to plan, then plan to fail. But whether it's hubris or laziness, human nature so often prefers procrastination.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/#why-bother-with-rag","title":"Why Bother With RAG","text":"<p>AI has become pervasive in our personal lives, with NPUs on our phones helping avoid typing, learn about the world around us, or modify photos we take (for improved quality or humorous outcomes). The NPUs or Neural Processing Units are the chips that perform the AI functions, which seem to be quite lightweight. But we never use RAG in our personal lives, and the fact is we don't really need to. The answers to the personal questions we have are typically in the public domain.</p> <p>That's typically not the case in business AI usage.</p> <p>There are two approaches. One is MCP to make API calls to systems to include a subset of data gathered via traditional means. That may work where the information is readily available from a database with a simple query. But where you need to review a broad corpus of data or search via vectorised similarities, the better option is RAG. And that's not necessarily just your internal data. Manuals for systems that have been bought may not be publicly available and may not have been indexed by the LLM in use. In-app AI may leverage RAG to contribute specific content to their AI integrations.</p> <p>Of course it's more true in the context of open source software. And that raises the question of whether AI will make developing in the open more attractive, because starting from closed source may mean you're already on the back foot in terms of customer productivity.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/08/effective-ai-1/#summary","title":"Summary","text":"<p>There are a lot of moving parts to AI and while generic and cloud offerings will work for simple tasks and many personal requirements, enterprise AI is a much more complicated challenge. So understanding the moving parts is even more important.</p> <p>The ability to optimise the various parts of AI offered to the business will be a major requirement of IT departments and third parties. Proper understanding by both sales executives and procurement is crucial to moving beyond the sale and basic POC to implementation.</p> <p>But even then, AI-fu has a major role to play.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/14/framework-web-20/","title":"XPages App to Web App: Part 20: Custom CSP Settings","text":"<p>A good web server will enforce Content Security Policy settings. If you are using something like Express as the web server, the endpoints will set that Content Security Policy.  In the case of Single Page Applications hosted on Domino REST API, since release 1.15 by default a strict CSP is applied. But it is possible to change the CSP settings per application.</p>","tags":["Domino REST API","Domino"]},{"location":"blog/2025/09/14/framework-web-20/#manifest","title":"Manifest","text":"<p>The web manifest is required for Progressive Web Applications, telling the browser the information it needs about your application in order to install it, and so it's not uncommon for other single page applications.</p> <p>Although it can is registered to have a .webmanifest extension, it's more typical for the file to be called \"manifest.json\". It has its own specification. Many of the properties are standard, but there are also some experimental settings.</p> <p>But Domino REST API specifically uses an additional property, \"csp\", for a custom Content Security Policy to use. In the case of my web application, I'm loading icons from Google's material design icons, as I mentioned in part 7. So I need to allow the browser to load the stylesheets, otherwise the requests get blocked and I just get the text instead of the icons.</p> <p>A quick look at the headers will show that the URL is \"https://fonts.googleapis.com\". But this is only part of the story. They're loaded as fonts, so I also need to allow fonts loaded from \"https://fonts.gstatic.com\". The full CSP setting in the manifest,json is <code>\"csp\": \"default-src 'self'; script-src 'self'; style-src 'self' https://fonts.googleapis.com; font-src 'self' https://fonts.gstatic.com; img-src 'self' data:; connect-src 'self';\"</code></p> <p>You may need to restart Domino REST API after making the change, I can't remember.</p>","tags":["Domino REST API","Domino"]},{"location":"blog/2025/09/14/framework-web-20/#summary","title":"Summary","text":"<p>If you're deploying a Java application on Domino REST API, custom CORS is set differently, in a config file. You can look at the open-sourced Admin client for an example.</p> <p>As ever, you can see the full application on github</p>","tags":["Domino REST API","Domino"]},{"location":"blog/2025/09/14/framework-web-20/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Dev Tools</li> <li>Frameworks</li> <li>DRAPI</li> <li>Home Page</li> <li>Mocking, Fetch, DRAPI and CORS</li> <li>CSS</li> <li>Landing Page Web Component</li> <li>Services</li> <li>Ship Form Actions</li> <li>Ship Spot Component</li> <li>HTML Layouts</li> <li>Fields and Save</li> <li>Dialogs</li> <li>Spots</li> <li>Lessons Learned</li> <li>CSP Enhancement</li> <li>Spots By Date and Stats Pages</li> <li>Custom CSP Settings</li> </ol>","tags":["Domino REST API","Domino"]},{"location":"blog/2025/09/15/effective-ai-2/","title":"Effective AI Usage: AI-fu","text":"<p>In my last blog post I talked about the many aspects of modern AI and the importance of understanding them all. But even more important than this is what I am terming \"AI-fu\". So what is AI-fu?</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#ai-fu","title":"AI-fu","text":"<p>When I started working in IT, the internet was in a nascent form. Database searches were full text with fuzzy matches. Google was not even the predominant search engine. Over the first quarter of this century, the ability to refine a problem into the right search criteria has been a differentiator - google-fu. Those who have been able to employ good google-fu have solved problems more quickly than those who could not. I often see questions on chat forums that I can easily answer with an online search - if the question is accurately reflecting the requirements.</p> <p>The trend just starting is what can be called AI-fu, expertise in interacting with AI interfaces to retrieve accurate outcomes effectively.</p> <p>Google-fu had some impact. But AI-fu will become a major differentiator, because its transformative power is so much greater. Those who can master effective use of AI will create more, better, faster. Those who can't will be stuck in the slow lane.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#hypotheses","title":"Hypotheses","text":"<p>Using AI is not like using your coffee maker or TV: there's not a manual of fixed functions with clearly-defined options, and there will never be. If you use AI to create \"smart buttons\", you will not get the most out of AI, and it will be like taking a bazooka to a knife-fight (to use an analogy a friend once used for something else).</p> <p>So using AI needs to be approached with the same mindset I discussed in my blog post on troubleshooting. Typically you're asking AI because you don't know the answer. But you need to approach it with possible hypotheses, and think about how your can test and verify the answers you receive. I'm reminded of when I suspected a use of Apache HTTP Client was not sending as UTF-8: rather than asking specifically about UTF-8, I asked what the default character set it would send would be. This verified my hypothesis without asking directly about UTF-8 and also gave me additional useful information. With that additional information, I could quickly verify with internet searches, removing the need for assumptions.</p> <p>You also need to gather the right information beforehand and provide the right information to the AI. Garbage in, garbage out. And if you don't provide important pertinent information, AI may give an answer that is correct based on the inputs but incorrect based on the actual problem being solved. I'm not talking at this point about how the question is being asked, but what information is provided.</p> <p>Based on what I see from some online chat platforms, there are even some IT people whose ability to provide pertinent information is lacking. IT people are often on the receiving end of support queries where pertinent information is crucial to effective processing. So they should be more capable of identifying and providing pertinent information than others. However, this requires approaching AI use with deliberate thinking.</p> <p>One should not assume that will happen.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#valuable-information","title":"Valuable Information","text":"<p>How do we know what we know?</p> <p>It's a difficult question to answer, and one most people rarely ask themselves. But the simple fact is that we pick up valuable information from everywhere. And more importantly from lots of places that AI does not have access to. We may look at documents outside the IDE where we're using AI. Or we may perform internet searches. Or we may be using information from our past experiences or face-to-face discussions with colleagues. Or the right answer may vary depending on external factors or even change depending on the time of year. We identify chunks of that information as important almost subconsciously, using them in our decision-making process.</p> <p>But if we want AI to help in that decision-making process, it also needs to be aware of that same information.</p> <p>Including AI in the planning phase is one way to maximise information AI is aware of. But this only works if the AI user interface supports conversational interactions with the LLM. Certain AI interfaces also allow mechanisms to increase the contextual information automatically - or explicitly - included in an AI. You can get very different information depending on what contextual information is provided. And interfaces with GitHub Copilot in Visual Studio Code allow you to contribute prompt files that can include additional relevant information. Prompt files can also request that AI perform certain roles, which can change the kind of response gained. RAG can allow the AI to pull in additional pertinent information, but you need to take into consideration whether or not the particular AI interface is re-using previously collected information or using new information from RAG. Agentic processes can allow the AI interface to pull in additional relevant resources. And if you're deep into AI, different LLMs have different skills.</p> <p>But this all requires an AI implementation that empowers you to control and maximise the inputs, and not all AI implementations do.</p> <p>Where they do, you need to know what information has been taken into account when answering your questions. Or you need to be able to anticipate what information may have been used. Or you need to confirm what information has been used.</p> <p>Super-powered AI-fu users will choose an AI implementation that empowers them and will understand what information it's using to ensure they can understand what AI has used to provide its responses.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#awareness-of-content","title":"Awareness of Content","text":"<p>If the last decade has proven anything, it is that what you ask and how you ask it can drastically alter the outcomes received. Politicians have increasingly used language, provided a subset of facts, or repeated incorrect statements all for their own benefit. Interviewers have done the same as have mainstream media. That been the case in mainstream media news outlets for some time, but it has been increased by lower sales, cost-cutting, and increasing focus on clicks. Click-baiting is pervasive and headlines are deliberately worded to be sensational, appeal to specific demographics, and entice the reader with deliberately incomplete information.</p> <p>There are already horror stories about how specific inputs have skewed answers from AI, whether it's encouraging a specific political viewpoint or reinforcing human biases reflected in the training material. The quality of inputs is key to the quality of outcomes. \"Why\" is always an important question to ask, in order to get the best from AI.</p> <p>Note, this article is focusing on effective use of AI, not ethical use of AI. Anyone who has been involved in school college, or university debating societies know that at - just like any source of information - can be used to create misleading narratives.</p> <p>But the good news is that it can also be used to challenge narratives, get alternative viewpoints, and get additional information to help validate the basis of opinions. Where you are asking for recommendations, asking about pros and cons can provide more information to help you make a better decision. But it's important to be aware that what you want is the best decision for you, which might be very different to the best decision in general. Again, this comes back to providing pertinent information.</p> <p>Where you are asking for factual answers, it's important to identify how to verify them. If you choose to take what you receive as gospel, you deserve whatever bad outcomes you inevitably will receive! You're the one paid to make the right decisions not AI.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#language-skills","title":"Language Skills","text":"<p>Underpinning all of this is the importance of good language skills. Human nature is to be careless with language, particularly when rushed and especially when using your first language. Among certain groups, colloquialisms and acronyms are thrown around without thought.</p> <p>When interacting with AI, we must be aware of ambiguities in our language, as I demonstrated in my last blog post. We need clarity in our requests to avoid muddled thoughts from AI. There are a host of presentations and YouTube videos about quality of phrasing, using numbering, reducing the number of things you're asking in a single request, being specific about the kind of response you want.</p> <p>But that's only part of the job.</p> <p>When you receive the response, you need good comprehension skills. Responses are often detailed, so the ability to quickly comprehend the response is important. Core to this is being able to quickly identify if there has been a misunderstanding. A human colleague will tell you if they haven't fully understood or if there's some ambiguity in what you were asking. AI does not.</p> <p>Moreover, AI will only tell you what you ask for. Those with good AI-fu will identify where additional information is needed. They will be able to identify what's missing in a response and ask follow-up questions. This requires an  awareness of your actual requirements, to ensure all have been met and, if not, why not.</p> <p>Another key factor to effective AI usage here is how you communicate. For example, my typing skills are very good. I can type fast enough and accurate enough to be very productive when conversing with AI through typed interfaces. Others may not be, and an interface that supports speech may be required for optimal effectiveness (regardless of transcription accuracy). It's a factor that needs to be borne in mind.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#when-response-go-wrong","title":"When Response Go Wrong","text":"<p>AI will inevitably provide an incorrect answer. There are two key aspects for AI-fu here. First is providing effective feedback to ensure correct information. Saying \"that's wrong\" will rarely get a better answer. And at this point it comes to employing everything I've already covered:</p> <ul> <li>reviewing what you asked to identify failings.</li> <li>reviewing the response to identify misunderstandings or confusion.</li> <li>identifying any pertinent information omitted first time round.</li> <li>reconciling what you asked for vs what you wanted.</li> <li>identifying incorrect assumptions.</li> <li>identify where unclear but crucial information was just ignored.</li> <li>knowing how to check the information received to ensure it's accurate.</li> </ul> <p>But equally inevitably, there will be occasions when AI is incapable of giving the right answer - because you're trying to get it to do something that's not possible or is not a recommended approach. The key here is quickly identifying when what you're asking for is impossible or not the right approach. And if there are alternative approaches, finding them and identifying the right approach.</p> <p>There are also some areas where it's not likely to have the right answer. And that is another blog post of its own. The key is anticipating where it is not likely to be useful, and either knowing not to use it or approaching it with a high degree of scepticism.</p> <p>There are some mechanisms with certain AI interfaces where you can improve the outcomes next time. GitHub Copilot in VS Code allows you to change what AI can see or add prompt files to increase the corpus of information it uses. Then there are more complex training improvement approaches. If you're using a RAG, you may need to improve the quality of the information, change chunking, choose a different embedding model. You may need to change the LLM in use.</p> <p>This takes AI-fu to a whole new level.</p> <p>If you can.</p> <p>AI implementations in some AI systems and certain IT departments will place barriers between the skilled AI user and effective use. Dumb down AI interactions and you get dumber outcomes. Companies will do this, and they will lose their best talent or you'll get \"shadow AI\" - skilled individuals using AI outside the constraints placed upon them by systems or IT, to get the job done.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/09/15/effective-ai-2/#summary","title":"Summary","text":"<p>As I stated at the start, the difference between those with good google-fu and those with poor google-fu has not had a huge impact on the effectiveness of those individuals. That skill is a minor part of the difference between effective and ineffective employees.</p> <p>But the potential for AI to speed up work is much more significant. Good AI-fu will maximise both the speed and quality of outcomes, optimised by effective cross-checking. The transformative power will vary depending on what you're trying to use it for. But I'm already seeing big productivity boosts in my daily development work, depending on language and framework choices. And AI is a key tool for me in accelerating my research. AI is often a first-choice go-to for me instead of Google. That does not mean it always gives me the right answer. But it's definitely having a transformative effect on my productivity that I will easily be able to demonstrate when appraisals come round.</p> <p>That primarily comes from approaching AI with a good awareness of language, good comprehension skills, a healthy awareness that both parties can be wrong, and a desire to understand why AI might be giving the answers it gives. I'm using all the skills in this blog post, as often as possible, and always looking to improve my AI-fu.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/","title":"Effective AI Usage: Understanding Brains","text":"<p>I've talked about the (current) moving parts of AI and AI-fu. But a fundamental aspect of AI-fu is being aware of how we think and how that's different to how LLMs \"think\". It's probably true that most people are not consciously aware of how they think or aware of how colleagues thinking works differently. So it's well worth raising that topic, because it's crucial to the quality of results.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#ai-program-vs-llm","title":"AI Program vs LLM","text":"<p>I will use the term \"AI program\" quite a bit in this blog post. This is because, as we covered in the first blog post, the AI the \"thinking\" is done in multiple places:</p> <ul> <li>interpretation and natural language processing by the LLM</li> <li>traditional database searching</li> <li>chunking of large data when populating a vector store</li> <li>choice of metadata put into the vector store</li> <li>ANN (approximate nearest neighbour) or hybrid search in a vector store</li> <li>specifying which tools and context stores are available to the AI program</li> <li>coded steps in tools</li> </ul> <p>Of course the LLM is just one part of the process. The AI program integrates an LLM with various other pieces of functionality, as well as a system prompt and user prompt.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#expertise","title":"Expertise","text":"<p>If Ferrari have a problem with their formula one car, they don't take it to the local garage. But if you need an MOT for a moderately-priced pre-owned car, you probably don't take it to a specialist for that make and model. The level of knowledge comes from two main places - the model (LLM) and context (RAG or traditional search). It's best done in the model, but few will have the skills or ability to create their own model. Context is limited by the quality of the search and the size of the context window.</p> <p>The quality of the search depends on what you're looking for. Humans may type a full question into a search engine or refine it to specific keywords. Sometimes those will be sufficient (e.g. what oil to use for a specific car make and model - \"oil\" + make + model). Sometimes you'll need to try multiple attempts, reviewing the results and trying again. A traditional database search will give you answers for the first, assuming the database holds the information. A vector search will be better for the second.</p> <p>The additional contextual information needs to be prepared beforehand, and the LLM needs to be pointed to the right store before the question is asked. A business will need multiple such stores, and which one (if any) is needed will depend on what the user wants to know or do.</p> <p>As a human being we have access to many areas of expertise and we choose the right \"datastore\" to use without thinking. Sometimes it's driven by what we're doing at the time. A mechanic will point their mind to a specific datastore of knowledge at work, and a completely different datastore of knowledge when cooking at home. This may seem obvious, and this is something that software developers can do when providing specific AI systems: if the legal department are creating legal agreements, it's not rocket science to point it to a RAG of existing agreements. But sometimes it's harder to anticipate what contextual information needs providing and how it's best to prepare it.</p> <p>But other times it's less obvious which datastore or datastores we're going to need. As human beings we work it out. But how? Are the rules defined? Because those rules need to be defined for the AI program to make the right choice. Because if the right RAG store is not provided, it will not be used. And if too many RAG stores are provided, results that fit the search criteria but are - to a human being - totally irrelevant will be used. The result will be hallucination, because the LLM is designed to just use what context is provided without question.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#tools","title":"Tools","text":"<p>At the other end of the process are tools. The use of tools is similar to datastores: we have access to a large number of tools, both physical (e.g. calculators) and conceptual (e.g. mental arithmetic skills). And as with datastores of knowledge, we have access to all of them all the time, and we just know which to use. But an AI program will use tools based on the descriptions provided. So, providing too many tools with imprecise descriptions is more dangerous than providing not enough. And the order of tools may also impact which tool gets used.</p> <p>An example was an AI program I built following a Python course. It provided a tool for simple arithmetic calculations, with a description to use them if the result of a previous step was \"Calculate...\". However, the LLM provided an observation \"Calculate Jupiter + Mars\", which caused the program to error. As a human being we have learned that we only calculate numeric values, but the description didn't specify that, so an error occurred because the tool was used at the wrong step in the process.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#context-window","title":"Context Window","text":"<p>The obvious solution for this is to put the human in control. Let the human decide what gets passed to the LLM, make the human manually use the right tool. But the benefits of using AI in this way are minimal, e.g. summarising a large document, as long as the size doesn't exceed the context window. One-shot AI use has the additional limitation that it is only aware of what's currently being asked, it doesn't have memory. So you have to constantly summarise the history or pass that large document each time.</p> <p>And this is where things can go wrong very quickly. Because even though an LLM is designed to have a large pre-loaded datastore, it is limited on what content can be passed in a single request. And if the amount of content being passed in the request exceeds the context window, it will just get ignored - typically without informing the user that trimming has occurred.</p> <p>Furthermore, different LLMs can support different context window sizes. Obviously cloud models are designed to scale to a greater context window than local models, but the bigger the context window, the more powerful the server needs to be to run it. And then there's also the interface to the context window, which may further refine the size of the context it passes to the LLM. And context is typically in tokens, which varies depending on the tokenization process. So it's hard to map to a number of characters. And anyone who deals with text at a platform level also knows single-byte and multi-byte character sets have an impact here as well.</p> <p>As humans we also have a limited context window and it will also vary from person to person. But our action when the context window is exceeded is more extreme: we will tell the person to restrict what they're asking for. Or if they're asking about larger documents we don't have a ready summary of, we will ask them to provide the summary. Few if any will just ignore excessive content. So it's not an approach we're used to, and it's one we need to be aware of. (By the way, I've not tried asking the LLM to tell me if I'm providing too much context, but I wouldn't be hopeful that it would work, because the trimming would occur before the LLM assesses the contextual data.)</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#conversational-ai","title":"Conversational AI","text":"<p>So the standard AI program implements conversational AI: it \"remembers\" and passes the history of the conversation with each request for you. Of course, after a certain amount of content, the conversation needs summarizing, in order to not exceed the context window or give the LLM less text to trawl through.</p> <p>As human beings we're used to conversations, but we also subconsciously deal with changing the subject of a conversation. We can typically tell when someone has moved onto a new subject and wants us to ignore what was said previously. Sometimes confusions appear, and on occasion we may start to suspect the conversation has switched topics, and clarify. But in human interactions, we're used to switching topics in the same conversation or at best we use micro-cues, saying things like \"on a different topic...\", or \"changing the subject...\".</p> <p>However, an LLM needs more explicit conversational splitting in order to know when to ignore past conversational history. It's possible that as AI evolves, models may become more aware of change of topic. But for now, the onus is on the human to create new conversations whenever the topic changes.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#the-customer-is-always-right","title":"The Customer Is Always Right","text":"<p>One of the biggest problems with LLMs is that its starting point is that the customer - in this case, the user - is always right. That means it will try to do what it's asked to do. I mentioned in part one that this has been encountered before, with outsourcing. The problem is that the customer is never always right. Except at very low levels in business, employees are expected to be smarter. Employees are paid more so that the instructions they are given are briefer and not explicitly specific. Experts are paid because they know better, they can do additional research and think around a problem, they can provide better alternatives, and a good consultant will tell the customer when they're asking for something that's just not right.</p> <p>But that's not how LLMs work, and I don't expect that to be achievable any time soon. With LLMs the onus is on the person to provide all suitable information, to ask the questions that are hidden from end users.</p> <p>The other problem is that the default assumption from an LLM is that the human is asking for something that's possible. That may not be the case. The LLM needs to be advised if that's a possibility. It also assumes the information it has is correct. If you wish it to verify the information, you need to explicitly ask that. Some humans are capable of pre-assessing the likelihood of a correct answer, or the likelihood of needing to verify information, based on the topic or the source of the information. LLMs do not currently have that ability, and we need to bear that in mind.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#iterative-approaches","title":"Iterative Approaches","text":"<p>So far we've only been dealing with single-response interactions. But particularly in the case of coding, we often use AI to achieve a specific outcome in code, which requires a cycle of writing code, checking compilation, fixing, checking compilation, and repeating until it works. It's very easy to assume approaches based on the level of knowledge. But in my experience, what we get is the approach of a junior developer with the knowledge of a senior developer: code is written, checked if it works, then modified, checked if it works, modified again etc. But what if problems are introduced during modification, what if there were multiple options and the wrong one was chosen? From my use of GitHub Copilot, it's not been good at going back and trying a different approach. It's also not great at recovering when code gets totally broken.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/05/effective-ai-3/#summary","title":"Summary","text":"<p>Consumers of AI programs need to be aware that they will \"think\" in a certain way. LLMs will assume high quality of the information and requests it is provided with. If history is available, conversations need carefully separating to ensure only relevant information is used. AI programs can accumulate additional contextual information, but it's important to only include datastores and other contextual information that is relevant to the query in question. And if there is insufficient information available, that needs to be carefully managed. AI programs can leverage tools to perform additional functions, but again you will cause problems if there are too many tools and poor quality descriptions of when they can or should be used. These are problems humans have learned to adapt to. Maybe future generations of LLMs will improve on these, but today it's down to the user to be aware of these limitations and adapt accordingly.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/","title":"Effective AI Usage: Managing Hallucination","text":"<p>One of the biggest challenges when working with AI is hallucination. I've encountered and fought against it. It's worth discussing what can and can't be done to solve the problem.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#know-when-youre-dumb","title":"Know When You're Dumb","text":"<p>One problem of hallucination you can control is being aware when you might be asking the AI program to do something impossible. The AI program will assume that what you're asking is possible and sometimes it's not. It can also get caught in a loop of trying alternative approaches. Unless you're very conservative in your use or AI and IT, you will try to achieve something that's not possible.</p> <p>Including AI in the planning process and asking targeted questions about potential options early on may identify before you start coding whether something is possible or not. But there may be times you need to just dive into code. Ensuring you've got a good source code commit prior to trying something impossible is important, to create a clean rollback position. And if you're using AI in an agentic manner, when you're unsure if something's possible, you'll want to keep a closer eye on what it's doing.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#changed-apis","title":"Changed APIs","text":"<p>If you're working with a more unstable code base, APIs may change. I've encountered that recently and it can get frustrating trying to use AI to peer code when it's constantly using the wrong APIs or out-dated syntaxes. Specific prompts can help, telling the AI program to check APIs before writing code, or pointing it towards dependency files that define the version being used. But even in conversational chats, you might need to repeat the instructions.</p> <p>Using an up-to-date model is important. But this may also be where paid-for models provide better results. It's not obvious why, whether they use more up-to-date sources or don't use out-dated sources. But I have definitely found times when the choice of model makes big differences in whether the AI hallucinates about APIs.</p> <p>Having existing code available that uses the right APIs can definitely help. I've used GitHub Copilot a lot on VoltScript Testing Framework which it doesn't know. It starts off guessing incorrect APIs. But as I write more tests, it knows the correct syntax based if APIs have been used already.</p> <p>Similarly, if source code is available and accessible, it can avoid some hallucination.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#consumer-responsibility","title":"Consumer Responsibility","text":"<p>Of course changed APIs becomes a problem when, like me, you're working at the bleeding edge. However, there's the same problem for consumers and customers that are ultra-conservative or use edge-case functionality that gets broken. AI gets trained on what's latest at the time of training, not what most of the customer base uses. So you may get incorrect results if you're working on out-of-date versions of software, languages, or frameworks. Where languages widely publish a Long Term Support version, these may get preference. But if you're on an older LTS, you might get bad results.</p> <p>It will be interesting if this encouraged customers to stay more up-to-date with non-cloud software. Or whether IT partners or consultants will start charging customers more if they choose not to remain on outdated versions. Because I believe it will have a bigger impact than ever before, as I'll cover in a future blog post.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#tailored-content","title":"Tailored Content","text":"<p>AI programs have also evolved to add ways to provide specific information to help. GitHub Copilot uses prompt files and I've been able to generate prompt files that, at least with brief results, improve the outcomes. That's fine if you want to generate those prompt files yourself, and are willing to invest the time. But what about proprietary content?</p> <p>Another option that might be worth experimenting with is MCP. This was not an option for me at a time when I needed it. But it might be a good option to allow the AI program to read API docs.</p> <p>Whether prompt files, MCP, or something else, the format is probably important for the LLM to process it in the most efficient manner and minimise problems with context window sizes.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#quantity-and-quality-of-content","title":"Quantity and Quality of Content","text":"<p>Beyond API hallucination, a major cause of hallucination is not enough quantity and quality of content. This is where open source with a vibrant community has an advantage. Not only is documentation available. But also the code, tests, issues, and examples are available to be cross-referenced, as well as any community content - tutorials, blog posts, and open source projects that use the content.</p> <p>Proprietary products and frameworks will have documentation available for LLMs, which may include some examples. But the code and tests are not available. A custom RAG database may be an option. With a proprietary IDE, there is the advantage that can easily be integrated into the tooling. And it will be interesting if that becomes only available with paid support. But it's not immediately obvious how that would integrate with standard AI programs like GitHub Copilot or Claude.</p> <p>However, a vibrant community is key to providing additional information, with blog posts, samples, tutorials, and integrations. Vendor-only content is unlikely to be sufficient. Documentation deliberately tries to avoid duplicating content, and for a very good reason. If a topic is covered in more than one place and changes are required, you (or whoever takes over) needs to know that it needs updating in multiple places. Ensuring content exists in as few places as possible in documentation minimises the risk of it becoming out-of-date. Whether it's RAG or model-trained content, if there is a paucity of content and that content does not include a definitive answer to the question you're asking, LLMs will try to extrapolate from what is standard elsewhere. And when it makes those guesses, the LLM does not typically inform you that it has guessed because it has not found sufficient answers. Community content, especially open source code, is key to increasing the content and validating correct approaches.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#consistency","title":"Consistency","text":"<p>Ensuring APIs are consistent with other, more pervasive languages is also a good step to take. It's why many IDEs copy keyboard shortcuts from other IDEs, and why developers customise an IDE's keyboard shortcuts. It's the same with languages and frameworks. Where there's consistency, it's easier to move from one to another.</p> <p>But what if the consistent APIs and approaches came after your technology decided on its route? There is a trade-off to make here: the pain of requiring consumers to change their learning and code vs the benefit of making it easier for AI. Before AI existed, that quandary had a simple answer. Now there may be incentives for standardising. It depends if AI program tooling - like Copilot prompt files, or skills or projects in Claude - are a more effective solution.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#tooling","title":"Tooling","text":"<p>Tooling can also play a big part in avoiding hallucinations. A lot of my Copilot usage over recent months has been with Rust. Rust's CLI means it's easy for an AI to run terminal commands and read the response. And Rust's higher quality output works particularly well for AI programs.</p> <p>In some compilers, error messages are a particular error message at a line number. The problem is that LLMs may not read the line numbers the same. Indeed there are some technologies I've used where the editor hides content from the actual stored content. Rust's error messages include the actual code block that caused the problem, which means the LLM has some actual code to cross-reference against the file it's reading, avoiding mistakes. Rust also tries to advise on how the problem can be resolved, which the AI program can use to try to fix the code.</p> <p>And this is where documentation alone and vendor content alone will also have a problem. Documentation lists error codes and error messages. But it doesn't explain how to best resolve the problem. Community content can help here, but not just forum posts. Forum posts are often poor on giving sufficient information for a human to work out the cause of the problem, let alone an AI that is not designed to fill in the gaps. And I've often come across forum posts that ask to solve a problem, where that problem is not the root cause that actually needs addressing.</p> <p>Another piece of tooling Rust provides that AI can leverage is cargo, the dependency management solution. This means an AI program can be asked to find what dependency could perform a particular function, e.g. reading JSON or integrating with a particular third-party technology. Because cargo is a command line interface, the AI program can call it, read the output, and send it to the LLM. Not all languages and frameworks provide this kind of interaction.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/10/26/effective-ai-4/#humans-vs-ai","title":"Humans vs AI","text":"<p>At the root of a lot of the points I've made is one very simple fact: humans and AI do not work the same. And historical content, historical tooling is often aimed at humans, not computers. Products, languages, and frameworks that have embraced automation have an advantage in that some of their tooling is designed for automated processing and to give a binary response - success or failure, continue or abort the build.</p> <p>But documentation is specifically aimed at humans, who look at a screen, use keyboard shortcuts, select menu options, and click buttons. Documentation may not be in an appropriate format for AI to use. And if you address this, you have the added challenge of maintaining two different documentation bases - and more importantly, identifying when you've updated one but not both. That could be a lot harder than it initially sounds.</p> <p>The same is true for compiler errors and community extensions. And the knowledge for filling in the gaps often comes from experience and asking in chat rooms. None of this is available to an AI program. The quality of work by AI programs comes from filling in the gaps we intuitively and subconsciously ignore on a daily basis. And the problem with subconscious action is it's hard to bring back to our conscious mind in order to educate the AI.</p> <p>And the knock-on effect is potentially huge and may significantly transform the software development landscape over the coming years.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/","title":"AI Coding - Thoughts About The Future of Development","text":"<p>Vibe coding is probably the term of the year. Since being coined by Andrej Karpathy in a tweet in February the term has gained widespread adoption. My job is research, so I'm not one to accept code without review. But I absolutely need to be aware of new technologies and approaches, and evaluate their usefulness. And all technologies improve over time. So AI-assisted coding has been a regular part of my work for 18 months. Over recent weeks I've used it more and more, for a wide variety of purposes. And research is not just about trying things, it's about extrapolating and anticipating future usage.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#recent-experiences","title":"Recent Experiences","text":"<p>Research means using different languages, frameworks, and technologies. And at the heart of research work is creating prototypes, quickly. So being able to generate what you need quickly is important. Traditionally, this would be done by poring over documentation, looking for examples, trying to get it working, and asking questions in chats or forums. Now, the key to speed and learning in leveraging AI. And it's impressive what can be achieved.</p> <p>Within a few hours, via chat AI generated a Python web UI leveraging the Dart framework. This was a fully working web UI, calling backend services, built up iteratively adding additional UI elements. The impressive thing is I never once looked at Dart documentation. I never looked at a Dart example. But AI gave me everything I needed for a powerful demo. If someone wanted an additional input box or combo box, if the combo box needed to pull from options via a REST service, I now had all the information I needed. By asking the right questions of AI, I was able to convert it into a server-side REST service and a framework-less web UI using web components.</p> <p>I also had an opportunity to try developing with Go, another language I've not done before. The traditional approach is following a training course, which I did for more than a few hours, still learning the basics of the language. I could have continued, probably for several more days before I started coding a prototype for what I needed. But I decided the better approach was to use AI. GitHub Copilot and VS Code has evolved so much during the year that custom agents are now available, which allows you generate instructions for specific needs. With a custom coding agent, within an hour I had generated an initial sample in Go and, using a separate documentation sub-agent, I had generated comprehensive documentation which I augmented after going through the code line by line and asking AI questions to understand it better. And less than an hour later, with additional AI attempts and follow-up research, I had learned enough to decide that Go was not the right language to choose, because of limited support for other requirements.</p> <p>Separately, I've also been working with a Rust UI framework, at different times over a number of months. I talked about problems of AI hallucination a few months ago, because APIs had changed. Interestingly, when I tried more recently, and with a different model, I had much better outcomes. I talked in my last blog post about causes of hallucination. Going back to my blog post on troubleshooting, there are two key differences - the different model and the progression of time.</p> <p>A paid model is obviously going to be better than a free model, and this is something AI consumers need to be aware of. The truth has always been the same, that you get what you pay for. Some customers may be concerned about data sovereignty, but the fact is that the quality of AI from on premises models and the cost of the hardware to run the best is beyond most customers. And customers need to ensure they update the model regularly - more regularly than they would typically update software.</p> <p>The second option was the passage of time. As open source versions gets closer to release, documentation gets updated. If models are updated, that means the content it's using to make its deductions gets updated. This results in better responses, especially if older materials are removed. I've no idea if that's how model training works, but I'm conscious of the effort and cost it would take. The passage of time was also what allowed me to have a better experience using custom agents and sub-agents.</p> <p>So what does all this mean for the future of coding?</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#why-low-code","title":"Why Low Code?","text":"<p>Low code has been a major in-route to software development over the last 40 years. Innovations like Lotus Notes, Visual Basic, Excel formulas, Salesforce have all enabled those who were not computer scientists to build powerful applications. There are other products that offer low code options, for various purposes.</p> <p>But low code has a problem. The documentation tells users where to click, which menu options to choose, what to fill into properties boxes. Under the hood there is code, but documentation and examples don't focus on this. Their focus and what they provide is for the developer and their needs, not on the code that's generated and what the AI integration needs.</p> <p>There's a reason that these products tend to focus on what AI to offer the end user and not the developer. Or they offer AI-assisted functions for the developer. These are the easy wins. But the next steps from there are much more challenging, and I expect AI integration to slow.</p> <p>It's a major undertaking to give AI programs the knowledge needed to convert requirements into the code AI needs to write. And it means a dual effort - developers who want to perform tasks manually need the documentation that focuses on the what the user can see, AI needs the documentation for the code.</p> <p>And then there are potential problems for the IDEs. The IDEs are designed to work one way, taking user input and generate code. If changes are made in the code, will the UI pick up the changes automatically, or is additional development required in the IDE to reflect the changes for the user? And even where there is textual code, this needs a custom AI program that can integrate with the IDE's editors. The hard work that developers of other IDEs have already completed needs to be reproduced in a proprietary IDE. Even where the IDE is based on a more standard IDE but with custom editors, it is not a given that the standard AI extensions in those IDEs will automatically integrate the with custom editors.</p> <p>And I am certain that developers will not expect these problems.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#the-coding-mindset","title":"The Coding Mindset","text":"<p>Let's be clear, I'm not saying low code is dead, for two reasons.</p> <p>Firstly, human civilization has supreme adaptability, but human individuals are often resistant to change. Many developers will be reluctant to move from low code platforms to AI-enabled development in traditional coding languages. Similarly, customers will be reluctant to redevelop their existing systems, so developers will be needed to perform (frankly dead-end) support of those applications.</p> <p>Secondly, the skill in using AI with traditional coding languages is knowing how to architect a solution well, knowing how to explain requirements clearly and effectively, and being able to identify which edge cases are not covered but need to be covered. Not all people have this skill or will be able to master AI-fu. So some will still need low code platforms. The rest will no longer need to start with low code, they will jump-start their development career.</p> <p>The future is clear. A shrinking ecosystem of fewer and fewer customers, admittedly some of whom will be larger organisations. And a shrinking ecosystem of people willing or capable of supporting them, often the less skilful.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#open-source","title":"Open Source","text":"<p>Open source has grown over the last two decades. Even large companies have Open Source Project Offices now and are committed to developing in the open. The benefit of open source frameworks and languages is that not only is the code available for AI, but there are also potentially examples, tests, API docs, and documentation in Markdown. This is particularly important, because all are particularly suited to processing by LLMs. Indeed the format typically used for providing additional information to LLMs is Markdown.</p> <p>But there are also challenges. The levels of completeness differ, as do the levels of current activity on the projects. But more significantly, the open source repositories and the published versions may vary, as may the code and the documentation.</p> <p>Code developed behind firewalls avoids these challenges. But there are other challenges. The code may not be accessible or easily accessible. Tests will not be accessible. Documentation and samples may be limited. The reliance on community content is even higher, to give a suitable abundance of content for LLMs to comprehend.</p> <p>It will be interesting to see how new open source libraries and frameworks evolve. But I think this ia a barrier against the adoption of new languages, whether proprietary or open source. As my colleague Jason Gary blogged, it's more likely that developers will use AI to generate machine code rather than develop a new higher-level language.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#application-migrations","title":"Application Migrations","text":"<p>But what about where applications are stuck on out-dated or legacy platforms and languages? There is a benefit in moving existing developers with the applications, which historically gave an advantage to a language more similar. This has been a play for companies.</p> <p>But when AI makes it easier to learn new languages, the strongest choice depends on different factors. If a language close to the platform is important, then languages like Rust or Go may be preferable. If the client is a browser, Node.js with frameworks like React may be preferred, or the braver developer will go with web components and vanilla JavaScript, HTML and CSS.</p> <p>The strength comes from understanding the application. It comes from being able to interrogate the legacy application and understand the functionality, being able to challenge end users to identify what's actually needed, and being able to ask the right questions to architect the new solution in the right way.</p> <p>For people with the right skills, the options are wider than ever before.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#full-stack-development","title":"Full Stack Development","text":"<p>Historically the main strength of a developer has been knowledge of a language, a framework, their APIs and syntaxes. Full-stack development gained prominence because it didn't need separate back-end and front-end teams, and it arose from both sides.</p> <p>Node.js and frameworks like React championed full-stack development from the client-side approach, focusing on JavaScript as the language of choice, even embedding HTML in JavaScript. On the server side, the Java-based choice was JSF with proprietary extensions like XPages. More recently Rust has tried to address this with Dioxus. And Python had frameworks like Django or Dash.</p> <p>But when the skill is asking the right questions of a knowledgeable AI, why do developers need to limit themselves to a single language for all their development?</p> <p>I don't think full-stack development is disappearing. Some developers will still prefer to use a single language. And the vast amount of available examples for frameworks like React will allow developers to use that single language for certain scenarios.</p> <p>But the problem is that certain languages are best-placed for certain tasks. JavaScript is best placed for in-browser functionality. If you want something close to the platform but accessible, Rust or Go have advantages. If you're happy within a JVM and have the skills, Java is appealing. And if you want to interact with AI, Python is way ahead ahead of everything else.</p> <p>And that's the problem that developers who want to limit themselves to a single language will hit.</p> <p>Because the most flexible developers will choose the best language, libraries in other languages are likely to lag further and further behind. It may be a controversial opinion that the most skilled developers will also choose the best language. But it's definitely factual that some of them will, which will impact the development of those libraries.</p> <p>Equally it's possible that some companies may still have back-end and front-end teams. But I think it's certain that some companies will have teams of individuals that can work in both back-end-only and front-end-only languages.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#the-importance-of-community","title":"The Importance of Community","text":"<p>But let's address the elephant in the room. There is a key requirement for the ability to be flexible with which language you use when. That is community content. Blogs, open source projects, and good quality forum posts.</p> <p>I emphasise \"good quality\" because too often forum posts have inadequate analysis of symptoms, which generate scattergun replies. Older forums don't have accepted answers, but even where answers are accepted, the thread may be insufficient for AI to properly identify why the answer worked. This can result in AI suggesting answers that are wrong, or even being distracted from better answers. With traditional searching by a knowledgeable user, they can quickly dismiss bad threads or bad answers. When your entrypoint is an AI response, it may be harder to discern bad content.</p> <p>Some niche frameworks have become popular with small groups of developers. They may rely on a small handful of advocates who have generated good quality output. However, they will struggle more and more. There will still be outliers used by the person who created that framework and a few others. But their communities will be smaller than they would have been - both the community of consumers and the community of producers.</p> <p>In the age of AI, the importance of a thriving community of producers cannot be understated. It's not sufficient for a vendor to be generating all or even most of the content. Any framework is dependent on a thriving community of consumers for a wealth of content on best practices or integrations. This cannot be sustained by one or two individuals, whether inside or outside the vendor. The community needs to step up, even more so if it's an older technology. Because the older the technology, the harder to identify the good quality, current information that should be used.</p> <p>It becomes much easier to use AI to understand what the code does, and then migrate the application, than to support and enhance it.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2025/12/14/ai-coding-futures/#final-thoughts","title":"Final Thoughts","text":"<p>A colleague summarised recently that the output of an IT programmer was restricted by their knowledge and their speed of typing. Junior developers progressed to senior developers through the knowledge they gained. You had a team of developers mixing architects, code monkeys, QA reviewers, test engineers, automation experts. The future is IT experts who have the skills to marshal a team of IT agents who can help them architect, write code, perform QA, write tests, and build automation. At the heart of this is the ability to develop and master AI-fu.</p> <p>A number of technologies will disappear or become very niche. If they're open source, they may survive even if they're only used by the main developer and a handful of others. If they're proprietary, they will struggle more and more, becoming specialised software, with fewer and fewer customers. They will face the Scylla and Charybdis of being expected to add AI into the development process, but knowing that if they do, it makes it easier to gain the knowledge that allows customers to migrate the applications.</p> <p>At the heart is the need for a community of providers. Those top-heavy on consumers will suffer more and more.</p> <p>Because AI makes choice so much easier than it's ever been before.</p>","tags":["AI","Editorial","GitHub Copilot"]},{"location":"blog/2026/01/12/ai-and-media/","title":"AI, Tailwind, and The Future of Media","text":"<p>Recently there was an announcement that Tailwind was laying off 75% (3 of 4) of its engineering team because of revenue collapse. The initial hint came from a comment on a PR and was followed up with a podcast on X. This has caused a lot of discussion in the IT world. But I want to take a different approach. What does this mean for the media industry and the future of how we find out news?</p> <p>The Tailwind announcement comes from the rise of a number of factors. I'm not an expert, so this is gathered from other sources. One cause seems to be that licenses for monetised content was provided as a one-off payment, which requires either a constant stream of new users to pay for ongoing development or additional revenue streams. The second is that coding agents are able to create components for developers, which reduces the need to pay for pre-built components. The third is lack of traffic, that users get answers to Tailwind questions in their IDE or AI client rather than going to the Tailwind documentation website.</p> <p>It is this third reason that I want to pick up on, and which could have a significant impact on the future of traditional journalism and media empires. But there are also relevant differences that make me think the impact will be less.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#understanding-traffic-and-revenue","title":"Understanding Traffic and Revenue","text":"<p>First, we need to consider why loss of traffic is impacting Tailwind's revenue stream. With Tailwind, as with other traditional websites, revenue comes from advertising. And advertising comes from the benefit of targeting a captive audience.</p> <p>This shouldn't come as a surprise to anyone. It's why we have adverts in social media platforms like Facebook, it's why Twitter increased its focus on adverts when Elon Musk overpaid for the platform, and its why there are sponsored links on search engines.</p> <p>And AI platforms are also starting to talk about adding adverts. OpenAI's ChatGPT have started talking about adding adverts to the platform as of January 2026. And again, this shouldn't come as a surprise to anyone. The platform is not free. Either people have to pay for it, or they get adverts.</p> <p>There will inevitably be pushback, but this has been the case for generations.</p> <p>And this brings us to mainstream media. Most TV news outlets have advertising. It's common in news websites. And that's just a follow-on from printed media - newspapers - which have had adverts since before anyone alive was born. Online media have a bad reputation for including adverts. But unlike TV or printed media, you can pay to remove ads.</p> <p>And advertising or paying to remove adverts is a key revenue stream for mainstream media.</p> <p>The dependency on direct traffic for advertising revenue is very reminiscent of Tailwind's situation.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#ai-and-media","title":"AI and Media","text":"<p>So let me give a couple of real-world examples in the context of media.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#circumventing-paywalls","title":"Circumventing Paywalls","text":"<p>Just this evening, I found an interesting hook to an article on Facebook. As has become commonplace on Facebook, the post was from the mainstream media outlet, provided some information but not all. To find more, I had to click a link in the first comment - again, a standard approach. At this point some websites would bombard me with adverts, lazy-loaded to make it painful for me to read the article, because the page regularly jerked about on my device. This particular mainstream media outlet hid the article behind a paywall, although I could sign up for a free trial.</p> <p>But the AI-savvy person knows there's an alternate route. I closed the website and went to my favourite AI interface. I took the basics of the Facebook post, and asked AI what the news was. I not only got lots of details, I could add follow-up questions to drill down further.</p> <p>Furthermore, I could tell from the logos displayed that AI had searched the mainstream media outlet that had paywalled the article.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#cutting-through-bias","title":"Cutting Through Bias","text":"<p>The other example was from the middle of last year. There was a spate of toxic patriotism in UK. Consumers of mainstream media were fed specific versions of the story, depending on the approach of the outlet. Opinions and \"facts\" were spouted. Ask yourself, how often does mainstream media link to sources outside their own website to allow you to fact-check what they say. Some sites provide fact-checking, but not most.</p> <p>For this particular story, I turned to AI to ask specific questions and drill-down to the antecedents of the trend. Unlike mainstream media, this gathered a wide variety of sources, allowed me to ask follow-up questions, and view first-hand the specific content with evidence of the dating. This demonstrated far-right involvement at the start of the trend, as evidenced by the individuals' own social media accounts.</p> <p>Mainstream media often offers a specific political viewpoint. Human being are naturally lazy, and have often relied on a single media outlet. And whereas social media political attitudes are often based on the people you follow and the algorithm derived from them, mainstream media often have political attitudes that are proudly overt.</p> <p>This is nothing new. Newspapers have had specific political attitudes for longer than anyone has been alive, they rarely change, and never for long. And the \"trusted\" media outlet often transcends generational boundaries - there are individuals who take their news from a specific newspaper or newspaper's website because their parents did. Moreover, as I found out from newspaper rounds when I was a child (who remembers fondly the videogame Paperboy?), most households got their news from a single outlet.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#impact-of-ai-and-media","title":"Impact of AI and Media","text":"<p>So here are two factors:</p> <ol> <li>Using AI offers unique advantages over relying on mainstream media.</li> <li>Mainstream media requires advertising to be sustainable.</li> </ol> <p>If enough people start gathering their news from AI instead, what will the impact be in mainstream media's revenue streams?</p> <p>Will advertisers transfer their advertising to AI platforms instead?</p> <p>But if the impact of AI on mainstream media's revenue stream is similar to the impact of AI on Tailwind's revenue stream, how will this impact journalism?</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#ai-wont-kill-the-media-star","title":"AI Won't Kill The Media Star","text":"<p>The Buggles confidently claimed \"Video Killed The Radio Star\". They were wrong. So wrong in fact that MTV (which showed the video as their first song) died before radio.</p> <p>Similarly, I don't expect AI to completely kill mainstream media, for multiple reasons.</p> <p>Firstly, as already mentioned, human being like confirmation bias, and like to be told what to think by news-streams with a similarly political bias. So many, not just older people, will continue to use a particular mainstream media outlet to give them their news. So even those that are news only will continue to get revenue.</p> <p>Some others diversify by doing more than just news, and this brings revenue from advertising via media that cannot be replaced by AI.</p> <p>Advertising will probably reduce, and we've already seen that with the rise of online media. Local newspapers and news outlets have been aggregated under national groups. And many often curate news from social media platforms or use photos from non-journalists. But there is certainly scope for media outlets to use AI to increase their productivity, not least for same reason laymen will, to aggregate and curate other media reporting.</p> <p>It will also be interesting to see how the balance between AI platforms and mainstream media outlets changes. Will they try to block AI platforms? Will media conglomerates create their own AI \"news expert\" clients? Will AI platforms pass on some advertising costs to mainstream media for access to content? AI platforms will certainly advertise their own platforms on those media outlets, in order to persuade individuals to use their AI platforms for a variety of tasks.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/01/12/ai-and-media/#summary","title":"Summary","text":"<p>AI has surpassed the point of critical mass already. I no longer rely on Google-fu, identifying keywords to search for. I now typically ask AI clients a full question and follow-up questions. And AI is my main approach for research, drilling down into blog posts for confirmation or further details. It's not going away, whether you adopt it heavily or try to avoid it.</p> <p>And its impact will be varied and widespread, some of which we cannot even perceive at this time. The impact on advertising and revenue streams is clear. It's already having an effect, and companies should already be anticipating it. Revenue streams may change, some companies will adapt and some will not survive. And we're really only at the start of it.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/02/01/ai-and-marketing/","title":"AI and Marketing Content","text":"","tags":["AI","Editorial"]},{"location":"blog/2026/02/01/ai-and-marketing/#the-microsoft-revolution","title":"The Microsoft Revolution","text":"<p>About 20 years ago there was a major shift in document processing which had profound repercussions for the AI world, years before it started. With Office 2007, Microsoft made a major change in their document formats. Up until that point, Microsoft had preferred closed, proprietary, customer lock-in of content into silos of information. But during the 2000s the rise of Apache OpenOffice and its open document formats, along with governmental demands for interoperability, put pressure on Microsoft to introduce OOXML. Of course it had added benefits - competition alone rarely encourages change. The new format was up to 75% smaller, improved corruption recovery and improved security.</p> <p>Of course for those willing to adapt, it opened up other options for programmatically creating content, because the files are basically zip files of content. And this meant that documents and spreadsheets and presentation were equally accessible to humand and code.</p> <p>And this is the game-changer that is key to AI. Because in order to effectively ingest content programmatically, it needs to be readable by code systems. This openness makes the content useful, instead of a silo that is of no use outside the system containing it, an irrelevancy to the outside world.</p> <p>This is what python-docx and tools that use it, like docling, leverage for ingesting documents for RAG or passing directly to LLMs.</p> <p>But what about other file types?</p>","tags":["AI","Editorial"]},{"location":"blog/2026/02/01/ai-and-marketing/#the-pdf-problem","title":"The PDF Problem","text":"<p>Although Adobe also embraced openness, relinquished control to the International Organization for Standardization (ISO), turning the PDF format into ISO 32000, there is a big difference compared to the Microsoft formats. PDF format is designed for printing. The format is a digital printing format, not a data structure. Whereas heading in .docx files are semantic structures, in a PDF it is more correct to think of it as an instruction to go to a specific location on the page, use a specific font size and write a set of characters. They store text out of order, tables are a collection of lines and floating text, and custom font encodings and ligatures are used that distort text.</p> <p>The result is a format which is designed for humans to read online or print. But also a format that has inherent barriers to digital extraction.</p> <p>This was famously encountered with the \"vegetative electron microscopy\" blunder, a classic case of layout-induced hallucination caused by PDF parsers reading across columns rather than down them. In a 1959 paper in \"Bacteriological Review\", the word \"vegetative\" in one column was incorrectly merged with \"electron microscopy\" in the next, creating a nonsensical technical term. This \"tortured phrase\" was subsequently ingested into large-scale training datasets, leading AI models to treat it as a legitimate concept and causing it to creep into over twenty peer-reviewed scientific journals as authors relied on AI-generated text.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/02/01/ai-and-marketing/#partial-solution","title":"Partial Solution","text":"<p>There are various general purpose Python libraries for reading PDFs like pypdf, pdfplumber, and PyMuPdf. But the problem is that none are able to read them successfully. Even specialised libraries like docling, Marker, and Unstructured fail on elements. docling, for example, reads headings and tables well, but fails with ligatures.</p>","tags":["AI","Editorial"]},{"location":"blog/2026/02/01/ai-and-marketing/#why-marketing-and-why-they-need-to-change","title":"Why Marketing and Why They Need To Change?","text":"<p>Why do PDFs have ligatures? Typically this is in content generated by marketing departments whose priority is creating content that looks good to humans. But by making content unusable to AI, they're making it less relevant to humans, because AI is incresingly becoming the bridge to digital content.</p> <p>There are technical approaches that can help like PDF/UA which adds ths to the PDF. But this can only help at the point of creating the PDF, and may not solve all problems like image readability and only works if the export to PDF function does not fail the Matterhorn Protocol.</p> <p>A simpler solution may be if marketing departments focus not solely on good looking with complex fonts prioritised for humans, but more on dual purpose content. It will be interesting to see how things change.</p> <p>But change they must.</p>","tags":["AI","Editorial"]},{"location":"professional/","title":"Professional Activity","text":"<p>Although I've been working in IT development for 25 years, most of my professional activity dates from around 2010.</p>"},{"location":"professional/awards/","title":"Awards","text":"<p>HCL Grandmaster 2019-2019 HCL Master 2019 IBM Lifetime Champion, Intec blog post IBM Champion 2018, Intec blog post IBM Champion 2017, Intec blog post IBM Champion 2016, Intec blog post, wikipedia entry IBM Champion 2015, Intec blog post, wikipedia entry IBM Champion 2014 IBM Champion 2013, Intec blog post IBM Champion 2011-2012, Ed Brill blog post, Luis Benitez blog post, Intec blog post</p>"},{"location":"professional/blogs/","title":"External Blogs","text":"<p>Sept 2009 - Oct 2019, Intec Blog - All XPages and Domino technical content Nov 2014 - present, OpenNTF Blog - various posts Jan 2016, Designing your Vaadin based Domino Applications - vaadin.com Blog guest post</p>"},{"location":"professional/books/","title":"Books","text":""},{"location":"professional/books/#author","title":"Author","text":"<p>XPages Extension Library (IBM Press)</p>"},{"location":"professional/books/#technical-editor","title":"Technical Editor","text":"<p>Mastering XPages 2nd Edition (IBM Press)</p>"},{"location":"professional/books/#contributor","title":"Contributor","text":"<p>Opting In: Lessons in Social Business from a Fortune 500 Product Manager (IBM Press)</p>"},{"location":"professional/documentation/","title":"Online Documentation","text":"<p>VoltScript Logging VoltScript Collection VoltScript Json Converter VoltScript Testing Framework Bali Unit - VoltScript Testing adapted for LotusScript developers Watson Work Services API Java SDK CrossWorlds XPages OpenLog Logger XPages to Web App Tutorial XOTS XPages Validation for Radio Buttons, Check Boxes and List Boxes Java and Selections in XPages Dojox Charting in XPages Using Vaadin in IBM Domino on Vaadin Wiki</p>"},{"location":"professional/general/","title":"General Links","text":"<p>GitHub BitBucket SlideShare YouTube Channel</p>"},{"location":"professional/speaking/","title":"Speaking Engagements","text":""},{"location":"professional/speaking/#2024","title":"2024","text":"<p>OpenNTF Webinar, May 2024, VoltScript Deep Dive, recording, slides Engage, Apr 2024, \"OpenNTF Guide to Open Source for HCL Products\" - slides Engage, Apr 2024, \"VoltScript Overview\", slides Engage, Apr 2024, \"Engage Super Pro-Code Mode: Web Apps Without Frameworks\" - slides and code </p>"},{"location":"professional/speaking/#2023","title":"2023","text":"<p>OpenNTF Webinar, May 2023, \"Understanding Collections, Maps and More in Domino Languages\", recording, slides OpenNTF Webinar, April 2023, \"Getting to Know Domino REST API\", recording, slides Engage, Mar 2023, \"Volt MX Go: Introduction to VoltScript and Unit Testing\"</p>"},{"location":"professional/speaking/#2022","title":"2022","text":"<p>OpenNTF Webinar, Sept 2022, \"Domino on Docker (and free alternatives)\" (with Martin De Jong), recording, slides HCL Factory Tour, Sept 2022, \"Introduction to VoltScript\" Engage, May 2022, Domino REST API</p>"},{"location":"professional/speaking/#2021","title":"2021","text":"<p>OpenNTF Webinar, Sept 2021, \"Domino Online Meeting Integration (DOMI)\" (with Rocky Oliver and Devin Olson), recording, slides OpenNTF Webinar, Aug 2021, \"Git and GitHub Explained\" (with Jesse Gallagher), recording, slides OpenNTF Webinar, July 2021, \"HCL Presents Keep, a new API for Domino\" (with Stephan Wissel), recording, slides</p>"},{"location":"professional/speaking/#2020","title":"2020","text":"<p>OpenNTF Webinar, Dec 2020, \"The Volt MX LotusScript Toolkit\", recording, slides Collabsphere, Sept 2020, Rich Text on the Web Engage, Mar 2020, \"Keep It (Not) Secret, Keep It Safe\" - slides Engage, Mar 2020, \"Driven by Events\" - slides Engage, Mar 2020, \"Don't Think Outside The Box\" - slides</p>"},{"location":"professional/speaking/#2019","title":"2019","text":"<p>Intec Webinar, May 2019, \"The Future of Collaboration Solutions Under HCL\" - recording Engage, May 2019, \"Introduction to Node-RED\" (with Fredrik Malmborg, PSK Syd) - slides, source code, blog Engage, May 2019, \"Modernising Your Domino and XPages Applications\" -  slides, source code, blog Engage, May 2019, \"AI: What Is It Good Far?\" - slides, blog</p>"},{"location":"professional/speaking/#2018","title":"2018","text":"<p>Social Connections 14, Oct 2018, \"ICS Integration with Node-RED and Open Source\" (with Stefano Pogliani, IBM) - slides Social Connections 14, Oct 2018, \"You Get What You Give\" (with Luis Suarez, Panagenda) - slides ICON UK, Sept 2018, \"Do You Wanna Build A Chatbot\" - slides Engage, May 2018, \"Tips and Tricks: Domino and JavaScript Development MasterClass\" - slides, source code, blog IBM Think 2018, Mar 2018, \"Tips and Tricks: Domino and JavaScript Development MasterClass\" (with John Jardin, Agilit-e) - slides, source code IBM Think 2018, Mar 2018, \"Developing with Watson Work Services Java SDK\" - slides, demo app IBM Think 2018, Mar 2018, \"Social Zero to Community Hero\"</p>"},{"location":"professional/speaking/#2017","title":"2017","text":"<p>Red Pill Now, Dec 2017, \"Migration vs Modernization\", blog post Social Connections, Oct 2017, \"Using Watson Work Services Java SDK\" - slides, demo app Engage, May 2017, \"GraphQL 101\" - slides IBM Connect, Feb 2017, \"OpenNTF Domino API (ODA): Super-Charging Domino Development\" (with Stephan Wissel, IBM) - slides IBM Connect, Feb 2017, \"GraphQL 101\" - slides</p>"},{"location":"professional/speaking/#2016","title":"2016","text":"<p>TLCC Webinar, Dec 2016, \"Marty, You're Just Not Thinking Fourth Dimensionally\", recording, slide, demo database ICON UK, Sept 2016, \"Find Your Data - Use GraphDB Capabilities in XPages Applications - and Beyond\" - slides Engage, Mar 2016, \"'Marty, You're Just Not Thinking Fourth Dimensionally': Troubleshooting XPages\" - PDF download TLCC Webinar, Apr 2016, \"Getting Started with the OpenNTF Domino API\" - recording IBM Connect, Feb 2016, \"'Marty, You're Just Not Thinking Fourth Dimensionally': Troubleshooting XPages\" - slides, demo database</p>"},{"location":"professional/speaking/#2015","title":"2015","text":"<p>Social Connections, Nov 2015, \"Crossworlds: Unleash the Power of Domino for Connections Development\" - slides ICON UK, Sept 2015, \"To Infinity and Beyond: OpenNTF Domino API and CrossWorlds\" - slides Engage, Mar 2015, \"Jedi and Sith: OpenNTF Domino API and CrossWorlds\" (with Daniele Vistalli, Factor-y) - slides IBM ConnectED, Jan 2015, \"BP106 From XPages Hero To OSGi Guru: Taking The Scary Out Of Building Extension Libraries\" (with Christian Guedemann, Webgate) - slides, demo IBM ConnectED, Jan 2015, \"XPages Performance and Scalability\" (with Tony McGuckin, IBM) - slides</p>"},{"location":"professional/speaking/#2014","title":"2014","text":"<p>ICON UK, Sept 2014, \"What's New and Next in OpenNTF Domino API\" - slides ICON UK, Sept 2014, \"From XPages Hero to OSGi Guru: Taking the Scary out of Building Extension Libraries\" (with John Cooper, JCB) - slides TLCC Webinar, May 2014, \"It's Not Infernal: Dante's None Circles of XPages Heaven\" - recording Engage, Mar 2014, \"OpenNTF Domino API: The Community API\" - slides IBM Connect, Jan 2014, \"Source Control: An End to End Solution\" (with Declan Lynch, Czarnowski) - slides (337 slides) IBM Connect, Jan 2014, \"It\u2019s Not Infernal: Dante\u2019s Nine Circles of XPages Heaven\" - slides, demo database</p>"},{"location":"professional/speaking/#2013","title":"2013","text":"<p>DanNotes, Nov 2013, \"Embracing the power of the notes client\" (with HP Dalen, IBM) - slides DanNotes, Nov 2013, \"Beyond Domino Designer\" - slides DanNotes, Nov 2013, \"OpenNTF Domino API: Making Domino Work The Way You Want\" - slides ISBG (Norwegian IBM user group), Oct 2013, \"The Eureka Moment: The Knowledge You Need to Understand XPages\" ISBG, Oct 2013, \"The OpenNTF Domino API: Making Domino Work the Way You Want\" ICON UK, Sept 2013, \"Mobilise Your Notes/Domino Applications with the IBM Mobile Controls\" (on behalf of TLCC) - slides ICON UK, Sept 2013, \"org.openntf.domino: Making Domino Work The Way You Want\" - slides and demo database TLCC Webinar, May 2013, \"It\u2019s Not Herculean: 12 Tasks Made easier with IBM Domino XPages\" - slides, recording IBM Collaboration Solutions Community Meeting, Apr 2013 IBM Connect, Jan 2013, \"It\u2019s Not Herculean: 12 Tasks Made easier with IBM Domino XPages\" (with Mike McGarel, Czarnowski) - slides, slides and demo database</p>"},{"location":"professional/speaking/#2012","title":"2012","text":"<p>UKLUG, Sept 2012, \"Eureka! The Padawan's Guide to the Dark Side of XPages\" (with Tim Tripcony, GBS) - slides TLCC Webinar, Jul 2012, \"Ask The XPages Experts\" - recording DanNotes, May 2012, \"Ready, Set, Go! How IBM Lotus Domino XPages Became Mobile\" - slides DanNotes, May 2012, \"The Eureka Moment: XPages Under the Covers\" - slides, demo database BLUG, Mar 2012, \"Ready, Set, Go! How IBM Lotus Domino XPages Became\" (with Eamon Muldoon, IBM) - slides and demo database BLUG, Mar 2012, \"The Eureka Moment: The JSF Knowledge You Need to Understand XPages\" - slides, slides and demo database</p>"},{"location":"professional/speaking/#2011","title":"2011","text":"<p>TLCC Ask The XPerts, Dec 2011 The XCast (presenter), 2011 UKLUG, May 2011, \"XPages Developer + Coffee = Code: A Little Bit of Java\" - slides and demo database BLUG, Mar 2011, \"XPages: Enter the Dojo\" - slides and demo database EntwicklerCamp, Feb 2011, \"XPages from a Different 'View'-point\" EntwicklerCamp, Feb 2011, \"XPages Blast\" Lotusphere, Jan 2011, \"XPages: Enter the Dojo\" (with David Leedy, Czarnowski) - slides</p>"},{"location":"professional/speaking/#2010","title":"2010","text":"<p>ILUG, Nov 2010, \"XPages: Enter the Dojo\" This Week In Lotus, Sept 2010 NLLUG, Sept 2010, \"XPages from a Different 'View'-point\" - slides,demo database video of final demo BLUG, Mar 2010, \"XPages from a Different 'View'-point\" - slides, demo database</p>"},{"location":"professional/speaking/#2009","title":"2009","text":"<p>The XCast, Dec 2009</p>"},{"location":"professional/whitepapers/","title":"Whitepapers","text":"<p>October 2018, Modernisation Options for IBM Domino V10 Whitepaper Jan 2016, Migration or Modernisation Whitepaper and Infographic Sept 2015, XPages Developer Roadmap, 2nd Edition Sept 2014, XPages Developer Roadmap Sept 2014, Maximising The Benefits of IBM Domino 9.0.1 With XPages Jan 2013, IBM Notes Social Edition Cheatsheet, (German and Italian Editions also created, thanks to community members) Jan 2013, IBM Lotus Notes 8.5 Cheatsheet (German Edition also created, thanks to community members) Jan 2013, Lotus Notes 8.0 Cheatsheet Mar 2012, Maximising The Benefits of XPages in Domino 8.5.3 May 2011, Maximising The Benefits of XPages in Domino 8.5.2 Jun 2010, Maximising The Benefits of XPages</p>"},{"location":"blog/archive/2026/02/","title":"Feb 2026","text":""},{"location":"blog/archive/2026/01/","title":"Jan 2026","text":""},{"location":"blog/archive/2025/12/","title":"Dec 2025","text":""},{"location":"blog/archive/2025/10/","title":"Oct 2025","text":""},{"location":"blog/archive/2025/09/","title":"Sep 2025","text":""},{"location":"blog/archive/2025/08/","title":"Aug 2025","text":""},{"location":"blog/archive/2025/06/","title":"Jun 2025","text":""},{"location":"blog/archive/2025/05/","title":"May 2025","text":""},{"location":"blog/archive/2025/04/","title":"Apr 2025","text":""},{"location":"blog/archive/2025/03/","title":"Mar 2025","text":""},{"location":"blog/archive/2025/02/","title":"Feb 2025","text":""},{"location":"blog/archive/2025/01/","title":"Jan 2025","text":""},{"location":"blog/archive/2024/12/","title":"Dec 2024","text":""},{"location":"blog/archive/2024/11/","title":"Nov 2024","text":""},{"location":"blog/archive/2024/10/","title":"Oct 2024","text":""},{"location":"blog/archive/2024/09/","title":"Sep 2024","text":""},{"location":"blog/archive/2024/08/","title":"Aug 2024","text":""},{"location":"blog/archive/2024/07/","title":"Jul 2024","text":""},{"location":"blog/archive/2024/05/","title":"May 2024","text":""},{"location":"blog/archive/2024/04/","title":"Apr 2024","text":""},{"location":"blog/archive/2024/03/","title":"Mar 2024","text":""},{"location":"blog/archive/2023/10/","title":"Oct 2023","text":""},{"location":"blog/archive/2023/09/","title":"Sep 2023","text":""},{"location":"blog/archive/2023/07/","title":"Jul 2023","text":""},{"location":"blog/archive/2023/03/","title":"Mar 2023","text":""},{"location":"blog/archive/2023/02/","title":"Feb 2023","text":""},{"location":"blog/archive/2022/12/","title":"Dec 2022","text":""},{"location":"blog/archive/2022/11/","title":"Nov 2022","text":""},{"location":"blog/archive/2022/10/","title":"Oct 2022","text":""},{"location":"blog/archive/2022/09/","title":"Sep 2022","text":""},{"location":"blog/archive/2022/08/","title":"Aug 2022","text":""},{"location":"blog/archive/2022/07/","title":"Jul 2022","text":""},{"location":"blog/archive/2022/06/","title":"Jun 2022","text":""},{"location":"blog/archive/2022/03/","title":"Mar 2022","text":""},{"location":"blog/archive/2022/02/","title":"Feb 2022","text":""},{"location":"blog/archive/2021/12/","title":"Dec 2021","text":""},{"location":"blog/archive/2021/06/","title":"Jun 2021","text":""},{"location":"blog/archive/2021/04/","title":"Apr 2021","text":""},{"location":"blog/archive/2021/03/","title":"Mar 2021","text":""},{"location":"blog/archive/2021/01/","title":"Jan 2021","text":""},{"location":"blog/archive/2020/11/","title":"Nov 2020","text":""},{"location":"blog/archive/2020/07/","title":"Jul 2020","text":""},{"location":"blog/archive/2020/06/","title":"Jun 2020","text":""},{"location":"blog/archive/2020/04/","title":"Apr 2020","text":""},{"location":"blog/archive/2020/03/","title":"Mar 2020","text":""},{"location":"blog/archive/2020/02/","title":"Feb 2020","text":""},{"location":"blog/archive/2020/01/","title":"Jan 2020","text":""},{"location":"blog/archive/2019/12/","title":"Dec 2019","text":""},{"location":"blog/archive/2019/11/","title":"Nov 2019","text":""},{"location":"blog/archive/2019/10/","title":"Oct 2019","text":""},{"location":"blog/archive/2019/09/","title":"Sep 2019","text":""},{"location":"blog/archive/2017/08/","title":"Aug 2017","text":""},{"location":"blog/archive/2017/05/","title":"May 2017","text":""},{"location":"blog/archive/2017/04/","title":"Apr 2017","text":""},{"location":"blog/archive/2017/02/","title":"Feb 2017","text":""},{"location":"blog/archive/2017/01/","title":"Jan 2017","text":""},{"location":"blog/archive/2016/11/","title":"Nov 2016","text":""},{"location":"blog/archive/2016/10/","title":"Oct 2016","text":""},{"location":"blog/archive/2016/09/","title":"Sep 2016","text":""},{"location":"blog/archive/2016/08/","title":"Aug 2016","text":""},{"location":"blog/archive/2016/07/","title":"Jul 2016","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/domino-rest-api/","title":"Domino REST API","text":""},{"location":"blog/category/coding/","title":"Coding","text":""},{"location":"blog/category/rust/","title":"Rust","text":""},{"location":"blog/category/conferences/","title":"Conferences","text":""},{"location":"blog/category/editorial/","title":"Editorial","text":""},{"location":"blog/category/web/","title":"Web","text":""},{"location":"blog/category/xpages/","title":"XPages","text":""},{"location":"blog/category/web-components/","title":"Web Components","text":""},{"location":"blog/category/dev-tools/","title":"Dev Tools","text":""},{"location":"blog/category/voltscript/","title":"VoltScript","text":""},{"location":"blog/category/lotusscript/","title":"LotusScript","text":""},{"location":"blog/category/domino/","title":"Domino","text":""},{"location":"blog/category/support/","title":"Support","text":""},{"location":"blog/category/docker/","title":"Docker","text":""},{"location":"blog/category/java/","title":"Java","text":""},{"location":"blog/category/testing/","title":"Testing","text":""},{"location":"blog/category/vertx/","title":"Vert.x","text":""},{"location":"blog/category/tutorials/","title":"Tutorials","text":""},{"location":"blog/category/databases/","title":"Databases","text":""},{"location":"blog/category/vaadin/","title":"Vaadin","text":""},{"location":"blog/category/servers/","title":"Servers","text":""},{"location":"blog/page/2/","title":"Index","text":""},{"location":"blog/page/3/","title":"Index","text":""},{"location":"blog/page/4/","title":"Index","text":""},{"location":"blog/page/5/","title":"Index","text":""},{"location":"blog/page/6/","title":"Index","text":""},{"location":"blog/page/7/","title":"Index","text":""},{"location":"blog/page/8/","title":"Index","text":""},{"location":"blog/page/9/","title":"Index","text":""},{"location":"blog/page/10/","title":"Index","text":""},{"location":"blog/page/11/","title":"Index","text":""},{"location":"blog/page/12/","title":"Index","text":""},{"location":"blog/category/domino/page/2/","title":"Domino","text":""},{"location":"blog/category/domino/page/3/","title":"Domino","text":""},{"location":"blog/category/editorial/page/2/","title":"Editorial","text":""},{"location":"blog/category/web/page/2/","title":"Web","text":""}]}